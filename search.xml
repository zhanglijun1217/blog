<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[B+树能三层高度承载的数据大小]]></title>
    <url>%2Fblog%2F2022%2F07%2F01%2FB-%E6%A0%91%E8%83%BD%E4%B8%89%E5%B1%82%E9%AB%98%E5%BA%A6%E6%89%BF%E8%BD%BD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[一颗B+树能承载多少数据的计算方式单表数据上亿之后，索引的查询效果可能就没那么明显了。因为B+树的层级可能会变高，定位一条数据可能要经过好几次磁盘IO加载。 以三层高度为例，一颗B+树能承载的数据量是多少？ 这里只考虑聚簇索引，数据页大小16kb，假设一行用户记录大小为1kb，所以叶子节点一个数据页存放记录16条。 对于非叶子节点，一个目录项由主键id和页号组成，主键id是（bigint）8个字节，Innodb中页号指针6个字节，所以一个非叶子节点的数据页有大概 16k / (8+6) = 1170条目录项记录。 那么此时高度为3的树可以存放 1170 1170 16 约等于2000w的数据，所以2000w以内的数据的索引树高度大概是1到3层。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>B+树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些系统调优案例的总结]]></title>
    <url>%2Fblog%2F2021%2F11%2F25%2F%E4%B8%80%E4%BA%9B%E7%B3%BB%E7%BB%9F%E8%B0%83%E4%BC%98%E6%A1%88%E4%BE%8B%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[1. SQL造成大对象JVM频繁full gc哪些场景会频繁full gc？ 内存泄漏（对象没有及时被回收，某些代码问题导致） 死循环一直创建对象 一直创建大对象 主要看下大对象的场景： 可能来源于数据库，查询的结果集太大。 第三方接口传输的对象 消息队列的消息太大 一个排查步骤 是否有吻合时间点的发布？机器的CPU、网络IO、带宽是否正常？如果有先回滚和隔离机器。 观察内存监控，是否有逐渐上升的内存占用，且触发GC之后没办法释放内存（内存泄漏） 如果无内存泄漏(内存在每次fullgc之后能降下来) jmap -histo工具查看存活对象 jmap -dump:format=b,file=file [pid]命令dump内存快照。利用mat工具分析 dump内存占用很小，考虑堆外内存的占用。 dump分析占用最多的对象和相关调用栈，查看可疑代码。 比如：sql查出来的大对象一般伴随着网络IO变大，时间点吻合可以重点欢迎是不是sql的原因。 案例原因 一个in list查找，参数没有判空，导致条件没有带上，查询命中很多记录。 sql一般会复用mybatis模板，某个sql模板如果参数没校验，可能产生大量记录的查询。 2. 内存泄漏案例 内存溢出：程序没有足够的内存分配对象，会发生内存溢出。 内存泄漏：对象没有及时释放，导致内存逐渐增加，就是内存泄漏。内存泄漏一般不会造成程序无法运行，但是会不断的累计造成内存不足，明显现象：触发了GC之后内存占用率不下降，且缓慢上升。 案例及原因 本地内存存放商品数据，如果只存放热点商品，内存占用不会太大。但是如果全量商品加载到内存中，那么内存会不够。 每个缓存记录加了7天的失效时间，保证淘汰掉不是热点访问的商品数据。 经过一次重构之后，过期时间功能失效，没有过期时间淘汰，本地缓存越来越大。 在一定时间之后，报警内存不足。dump内存之后发现缓存了大量的商品数据，造成内存泄漏，且因为本地缓存的引用一直没办法被GC清除。 3. 磁盘IO导致线程阻塞 日常的CPU高的问题可以通过标准步骤： top -H - p pid来查看cpu使用率高的线程id 线程id转换16进制 jstack -l pid | grep ‘线程id16进制’ 但是有时候cpu突刺可能是在一个很短的时间内发生，此时可以采用shell脚本自动执行jstack，比如5s执行一次stack，每次执行完成之后放到不同的日志文件中。只保留2000个日志文件。 12345678910111213141516171819202122232425262728293031#!/bin/bashnum=0log="/tmp/jstack_thread_log/thread_info"cd /tmpif [ ! -d "jstack_thread_log" ]; then mkdir jstack_thread_logfiwhile ((num &lt;= 10000)); do ID=`ps -ef | grep java | grep gaea | grep -v "grep" | awk '&#123;print $2&#125;'` if [ -n "$ID" ]; then jstack $ID &gt;&gt; $&#123;log&#125; fi num=$(( $num + 1 )) mod=$(( $num%100 )) if [ $mod -eq 0 ]; then back=$log$num mv $log $back fi sleep 5done 下一次响应变慢的时候，找到对应时间点的jstack日志文件，里面有很多线程阻塞在logback输出日志的过程，后来精简了log + 配置日志异步输出，则问题得以解决。 4. 死锁案例1. update顺序导致的死锁一个关单定时任务和人工后台关单的死锁案例背景： 定时关单的sql及加锁分析。1update order set status = 'canceled' where created_time &gt; '2020-01-01 08:00:00' and created_time &lt; '2020-01-01 08:00:00' and status = 'UNPAID'; 因为created_time是二级索引，会先给二级索引加锁，再给对应的聚簇索引加锁。（加锁步骤如下所示） 后台关单的sql及加锁分析 1update t_order set status = 'CANCELLED' where id in (2, 3, 5) and status = 'UNPAID' 直接在聚簇索引的记录上加锁： 可以看到定时任务的sql对主键索引加锁顺序是5，4，3，2。而后台的sql对主键加锁顺序是2，3，5。 比如第一个sql对3加锁之后，尝试对2加锁时发现后台取消的sql事务已经对2加锁，而此时第二个sql又尝试对3加锁，两个sql互相等待对方的锁，也就发生了死锁。 解决办法： sql从语句上0保证加锁顺序一致。也就是都按照id排序加锁防止死锁。 或者让批量操作，变为单个执行，减少锁占用的粒度和时间。 取消订单都加一个分布式锁排队执行。 5. 后台导出数据引发的OOM问题描述：公司的后台系统，导出功能之后偶发性的发生OOM。 排查步骤 因为是偶发性的，所以认为是后台系统的内存不足，单方面加大了内存。 但是没有解决问题，只是降低了OOM的频率。加入参数-XX:+HeapDumpOnOutOfMemoryError参数，来触发OOM时自动dump内存。 dump内存之后进行分析，确认了大量string对象，跟其引用都是导出excel处服务。 结合Arthas查看，导出执行时间比较长，但是会在短时间内执行导出很多次，且用户session和参数是一致的。 现象总结就是短时间内多次导出相同的数据，且操作人是一个，产生大量的string对象。未能及时在年轻代清除（比如空间担保机制、比如超过了对象的阈值、比如创建对象速度过快未进行标记就晋升到老年代）。 最后排查是点击导出按钮没有在点击时置灰，导出交互会引导操作人多次点击，每次点击都会去读数据库的几万数据且导出excel，方法执行比较慢，对象无法回收，导致内存溢出。 最后解决：前端导出之后置灰，响应了之后才可以继续点击，且减少了查询订单信息的非必字段来瘦身对象，且分批次查询数据库改为了多线程。之后没有出现OOM。 6. 网站流量暴增后，网站反应出现卡顿问题描述：在测试环境的页面速度很快，但到生产会变慢。推测访问量增大之后，对象创建变多，频繁的GC导致STW。 定位：使用jstat -gc pid指令观察JVM的GC次数，且观察Eden区增长的速度，来观察到触发MinorGC频率特别高，FullGC的触发频率次数和MinorGC几乎一致，甚至更多，这样导致STW业务线程，造成网页出现卡顿。 猜想是触发了老年代的空间担保机制，新生代在minorGC之前发现老年代可用连续空间比新生代所有对象小（或者是历次晋升的平均大小），就会触发空间担保机制，进而触发一次fullGC。 调大内存，老年代内存变大，且新生代的内存也变大，对象触发MinorGC的频次减少。 但是还是会发生卡顿，且卡顿时间比之前更长，发生卡顿频率降低。 继续jstat -gc pid观察，发现GC的次数不多，但是每次GC的时间变大，推测是因为内存调整变大，GC时间被拉长了。 jinfo看到使用的是parallel垃圾回收器的组合，想到特点是多线程进行回收，但是整个回收过程会STW，造成GC期间不能正常使用。 这里替换为parNew + CMS垃圾回收器，CMS在垃圾回收阶段可以和用户线程并发执行，能有效的减少STW时间。 替换之后要设置合理的触发CMSGC的阈值，如果太大，比如在垃圾回收过程因为内存不足再次因为Concurrent mode failure触发full gc，会导致CMS退化为Serial Old单线程回收，整个回收过程都会STW。 总之使用了CMS要注意GC日志中是否有Concurrent mode failure关键字。 7. 未设置元空间大小应用启动很慢MetaSpace会存放静态变量和很多类的信息，在启动时如果不设置元空间大小，则默认21M。 启动时间很长，且GC日志显示频发触发FullGC metaSpace不足 查看启动参数，未设置初始化的metaSpace大小。默认是21m 启动时会加载很多类，大量使用MetaSpace，21m很快被用完触发FullGC，然后MetaSpace动态扩容，然后再full gc再扩容的过程，使得启动变慢。 -XX：MetaspaceSize 参数意思是触发元空间fullgc的阈值。max的参数代表元空间最大值，设置为-1时也受制于直接内存的大小。]]></content>
      <categories>
        <category>系统调优</category>
      </categories>
      <tags>
        <tag>jvm调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql面试题总结]]></title>
    <url>%2Fblog%2F2021%2F10%2F01%2Fmysql%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[描述一下B+树索引的结构？ 什么叫做聚簇索引？聚簇索引是指叶子节点包含完整数据列，即主键索引和其余所有数据列，innodb中以主键索引构建的B+Tree即为聚簇索引，按照主键id排序。其余的索引皆为稀疏索引。 B树和B+树的区别是什么？为什么选用B+树？ 为什么不使用Hash结构来作索引？ Hash不能很好的支持范围查询 hash碰撞问题 hash一般需要一段连续的空间来存储hash表，这样方便维护hash表内部的链表或者数组的移动。 为什么不用二叉树来做索引？ 极端情况会弱化为链表，查找速度很慢。 数据变多，树会变的很高，导致查找时IO次数增多。 为什么不用红黑树来做索引？ 红黑树的高度维护 插入效率不高，需要维护平衡，伴随着左旋/右旋操作 虽然平衡了，但是要树高度也会随着数据量变大而增高。 为什么建议主键用自增id而不使用UUID？ 因为聚簇索引是按照主键进行排序的，如果是UUID则是无序插入，是一个随机过程，可能在维护B+树时频繁造成页分裂和记录移动的场景，增加了不必要的磁盘开销。 自增id占用的空间更小，能承载更多的数据。 什么是索引覆盖？什么是回表？ 查询的字段和条件字段能从索引中直接获取，不需要回表查找聚簇索引。 回表：从二级索引过滤完成之后，需要根据二级索引关联的主键值去聚簇索引中读取完整的数据列。 索引失效的场景有哪些？ 索引列使用函数、运算等操作，不能使用索引。 in、is not null、!=、or等操作命中记录数太多，不如直接聚簇索引查询，可能成本分析导致不能走索引。 联合索引没有使用最左前缀原则。 order by、group by没有使用到所以，用了文件排序或者临时表完成。 索引字段发生了隐式替换。 like的通配符在最前面 like ‘%xxx’ 设计索引的原则有哪些？需要注意哪些问题？ 常见的SQL优化方向有哪些？怎么做SQL优化？ explain type列有哪些值，代表什么含义？ explain extra列有哪些值，代表什么含义？ 索引下推是什么？ 为什么联合索引是最左前缀原则？ 索引树大约有多高？能存多少数据？ 非叶子节点存放的是什么？ 非叶子节点是目录项数据页，其中存放的是索引值和指向叶子节点（或者下层非叶子节点）的页号。 事务的四大特性？ACID: A：原子性 C：一致性 I：隔离性 D：持久性 原子性怎么实现的？ 在写记录的操作会写undo log，记录上隐藏列的roll_pointer会指向上一条版本记录，如果需要回滚可以找到对应的历史版本记录，保证了原子性。 持久性怎么实现的？ 持久性是通过redo log写磁盘来实现的。 对数据修改的物理日志，比如“对表空间号为2的数据页号为50的偏移量xx地址的记录修改数据内容是什么”，在事务过程中会写入磁盘。 redolog写入是二阶段，和binlog保持了一致性。redo log可以设置在事务提交的时候写到OS Cache缓存或者磁盘。 事务的隔离级别有哪些？ RU、RC、RR、串行化 不同隔离级别在并发读写下有哪些问题？ 脏写：事务内修改了一个未提交事务修改的值。 脏读：事务内读到了一个未提交事务修改的值。 不可重复读：事务内多次读的结果不一致。 幻读：读到了符合条件的多条新增记录。 什么是MVCC？ 什么是undo log？ 什么是redo log？ 一条SQL执行的过程是怎样的？ Mysql有那些锁？ 插入一条SQL的过程？ RR隔离级别下可重复读的原理？ RR下的加锁语句分析。 Innodb和Myisam存储引擎的区别有哪些？ Innodb支持事务、行锁。而Myisam不支持事务和行锁。 Innodb数据在聚簇索引上，而Myisam存储引擎索引和数据分离，索引只保存指向数据文件的地址指针。 Buffer Pool是什么？描述一下结构和运行原理？ Mysql count(字段) 和 count(*) 有什么区别？哪个效率高？ count(字段)不统计null值。 效率相差不大，就是字段有二级索引会直接扫描二级索引，效率更高。 什么是file sort，怎么避免？ 不能使用到索引的order by场景会使用文件排序，文件排序可能在内存，也可能在磁盘中排序，效率很低。 避免使用索引排序。 连接查询的原理是什么样子的？ 嵌套循环查询（有索引）和基于块的嵌套循环查询（没有索引，利用join_buffer）。 连接查询分为驱动表和被驱动表，驱动表每行记录会一直和被驱动表进行查询。 连接查询有什么优化吗？什么是基于块的嵌套循环查询？ 对连接的字段加索引 连接查询不能使用索引则调整join_buffer的大小。 GAP锁的作用是什么？ GAP锁之间不相互阻塞，只阻塞插入过程的插入意向锁，避免在RR隔离级别下插入幻影记录。 RR怎么解决一部分的幻读问题？ 快照读场景：事务内多次快照读，只会在第一次快照读时生成对应的ReadView，通过MVCC访问机制，去访问对应的undo log版本链，来解决了不可重复读和幻读问题。 当前读：如果当前读是加锁读，那么是通过加GAP锁来防止间隙处插入符合条件新的记录。Gap锁阻塞插入意向锁，所以是通过锁排队场景来避免当前读的幻读的。 什么是binlog？和redolog的区别是什么？ 死锁是怎么发生的？怎么检测死锁？ 如何避免死锁？]]></content>
      <categories>
        <category>面试题总结</category>
      </categories>
      <tags>
        <tag>mysql面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM面试题总结]]></title>
    <url>%2Fblog%2F2021%2F10%2F01%2FJVM%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[为什么堆区域采用分代？ 年轻代一般放新创建的对象，内存占用小，GC频繁但时间短。 老年代存放生命周期长的对象，内存占用大，GC不频繁时间长。 这样分代按照对象生命周期划分了区域，绝大部分都在年轻代创建之后被回收，实在不行还可以晋升老年代，有一定缓冲和保护内存的作用。 新生代为什么选择复制算法？为什么会有Survivor区域 新生代分为Eden、S0、S1区，S区一个时间只能使用一个。新创建的对象会首先分配在Eden区，如果Eden不够会MinorGC放入S0区，下次MinorGC会清理Eden和S0，将存活对象复制到S1区。 划分Survivor区主要是为了实现复制算法，复制算法的好处是复制之后内存是规整的，可以采用指针碰撞的方式来移动指针分配内存，不会有空间碎片。且复制之后可以一次性清除一个被复制的区域，效率高；缺点是只利用了一般的Survivor区域，利用率低。但是一般对象会在MinorGC之后被回收，存活对象很少，所以Survivor区不会很大。 jdk有哪些类加载器？ 双亲委派模型？ 如何打破双亲委派模型？有哪些实例是打破的？ 类加载的过程，每一步都干了什么？ JVM内存区域模型描述一下。 如何判定一个对象是垃圾对象？ 哪些对象可以作为GCRoots？ 为什么标记不采用引用计数法？ CMS垃圾回收有哪些过程？ 什么操作或者场景下发生FullGC 描述一下java对象四种引用和应用场景。 new一个对象一定堆上分配吗？ 逃逸分析和标量替换是什么？ JIT编译会做哪些事情？ 常见的垃圾回收器及其特点？ CMS垃圾收集器一定不会STW吗？哪些阶段会STW，哪些阶段会并发执行？ 描述一下Java对象的结构？对象头中的结构？ G1垃圾回收器的特点什么？ G1垃圾回收器和CMS的区别是什么？ 常用的JVM参数有哪些？ 常用的JDK调优工具有哪些？ 线上CPU打满怎么排查？ 内存泄漏和内存溢出有什么区别？ 内存泄漏：已经无用的对象还被GCRoots连通引用，被标记为存活无法回收， 内存溢出：当前内存不足以开辟出连续内存用来分配对象，则会内存溢出。 OOM有哪些类型？怎么排查OOM？ OOM：unable to create new native thread。线程栈也是堆内存的一部分，当无法创建机器的native线程会报错OOM。 OOM：MetaSpace。元空间无可用内存会OOM，比如加载了很多类或者动态生成了很多类，导致元空间内存溢出。 OOM：heapSpcae。堆空间内存溢出，比如老年代FullGC之后还是没有足够空间，则会OOM OOM：directMemory。直接空间（堆外内存）不足也会造成OOM。 线上JVM调优经验？ 线上其他问题排查经验？ 线上频繁出现FullGC怎么排查？ 什么是老年代空间担保机制？ promotion failure 在CMS中，什么是Concurrent mode failure，怎么触发的，有什么结果？ 元空间不足会FullGC吗？元空间的参数不设置会怎么样？ 线程的状态有哪些？调用sleep和Locksupport.park、synchronized锁阻塞、lock.lock分别是什么状态？ 常量池在JVM内存的哪个区域？常量池中存放的是什么？ JVM内存模型中，线程私有的区域都有哪些？ 什么是直接内存？怎么应用？怎么对直接内存进行垃圾回收。 OOM之后的Dump文件很小，且分析没有异常，可能是发生什么？ 什么是栈帧？栈帧中存放哪些结构？ 1.7的方法区和1.8的元空间有什么区别？为什么要废弃方法区？常量池和静态变量1.8jdk在哪个区域？]]></content>
      <categories>
        <category>面试题总结</category>
      </categories>
      <tags>
        <tag>jvm面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程面试题总结]]></title>
    <url>%2Fblog%2F2021%2F09%2F22%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Java线程的状态有哪些？ 状态迁移大概是怎样的？wait()、sleep()、LockSupport.lock、synchronized、lock.lock()执行之后对应的锁状态是哪些？ 如何实现线程通讯？ Callable和Runnable的区别？ interrupt()方法怎么中断的线程？如何响应中断？ CPU的三级缓存模型是怎么样的？ 什么是JMM工作模型？为什么要这样去划分工作内存和共享内存？ 并发的三个特性是什么？什么叫做可见性？什么叫做有序性？ 指令重排是什么？ 如何禁止指令重排？ volatile的底层实现原理是什么？ 什么是mesi缓存一致性协议？ synchronized有哪些用法？static方法锁的是什么？ synchronized在jdk高版本有什么优化？ 什么叫轻量级锁？ 什么叫偏向锁？偏向锁的过程？ 什么叫自旋锁？ 上边这几种锁的适用场景？ synchronized的底层实现原理是什么？什么是Monitor对象？ wait()方法和notify()方法为什么一定要在synchronized区域中调用。 锁竞争的过程，竞争失败会怎样，如何被唤醒？ 什么是用户态和内核态？为什么线程上下文切换会是开销大的操作？ 可重入锁ReentrantLock和synchronized的区别？ 如果利用AQS实现的可重入锁？ AQS内部原理？ 怎么实现的公平锁？ 怎么实现的锁超时等待？ 怎么实现的加锁失败在队列中等待？ Conditional怎么在AQS中实现？ AQS还有哪些应用？ JUC这个包常用什么？怎么看待JUC这个包的结构？能分几层？ 有哪些阻塞队列？ CAS是什么？ 以AtomicInteger为例，怎么依赖CAS实现？UnSafe怎么在其中使用的？ CAS的ABA问题是什么？如何解决？ UnSafe还在哪用到了？ 线程池的工作原理？ 线程池核心参数？ 为什么要自己实现线程池参数而不用Executors工具类？ 怎么对线程池进行监控？ submit和execute方法有什么区别，在异常处理方面有什么区别？ FutureTask怎么实现的？如何实现get阻塞等待的？怎么实现cancel的？ 线程池核心线程数量大小选择？ 线程池如何实现线程退出的？ 线程池的优点？ 线程池如何优雅关闭？ 线程死锁怎么排查的？ 生产者消费者模型 CountDownLatch、Semaphore、CyclicBarrier都什么作用？用在什么场景？ CompletableFuture怎么使用？使用有什么注意点吗？ Future和CompletableFuture区别，怎么实现非阻塞的获取结果异步回调的？ ThreadLocal实现原理？ ThreadLocal和Thread的关系？ ThreadLocal使用场景有哪些？ ThreadLocal会内存泄漏吗？ ThreadLocalMap怎么解决hash碰撞的？ ThreadLocalMap怎么删除无用元素的？ ThreadLocalMap内部结构 为什么ThreadLocal的引用是WeakedHashMap？ ThreadLocal怎么跨线程引用？ 了解ThredLocal的扩展吗？比如Netty的FastThreadLocal？ 什么是ForkJoin？什么是工作窃取算法？ Stream的原理是什么？Stream并行流怎么实现的？ HashMap put流程？ 为什么HashMap负载因子是0.75？ HashMap数据结构是什么样的？ 为什么要采用红黑树？ 描述一下红黑树的结构？ 为什么长度是2的整数次幂？ HashMap是线程安全的吗？HashTable呢？ HashMap线程不安全的点在哪？ 扩容的时候为什么会发生死循环？ ConcurrentHashMap怎么实现的线程安全？ ConcurrentHashMap1.7和1.8的区别？ ConcurrentHashMap size()操作怎么计算的？一定会加锁吗？ ArrayList扩容机制是什么？ LinkedList和ArrayList适用于什么场景？ LinkedList是双向链表吗？ 线程安全的List或者Set有哪些？ HashSet怎么实现的？ CopyOnWrite机制适合于什么场景？ LongAdder怎么实现更高的效率？ 了解过无锁队列Disputor吗？原理是什么？]]></content>
      <categories>
        <category>面试题总结</category>
      </categories>
      <tags>
        <tag>并发编程面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[索引数据结构的选择]]></title>
    <url>%2Fblog%2F2021%2F08%2F14%2F%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[二叉树什么是二叉树？ 每个节点至多只有两棵子树，左子树和右子树。左子树和右子树。 逻辑上二叉树有五种基本形态，空二叉树、只有一个根节点的二叉树、只有左子树、只有右子树、完全二叉树（满二叉树） 遍历二叉树有前序遍历、中序遍历、后续遍历。 数据库的索引是要求排好序的数据结构。引入了二叉查找树。 二叉查找树 在二叉树的基础上，加上左子树的节点值总是小于根的节点值，右子树的节点值总是大于跟的节点值，也就是左子树节点值 &lt; 根的节点值 &lt; 右子树节点值。 设计合理的时候可以将一组数据转化为二叉查找树，时间复杂度和二分查找一致。 但是存在设计不合理的场景：当二叉查找树不平衡，可能退化为一个线性的链表，查找时间复杂度高了起来。 二叉搜索树可能不平衡，所以再看下平衡二叉树 平衡二叉树（AVL树） 符合二叉查找树的定义，然后再满足任何节点的两个子树的高度之差的绝对值不超过1。 相对于二叉查找树，平衡二叉树查找效率稳定。 但也有缺点： 查询速度快，但是插入数据时需要维护平衡二叉树，需要1次或多次左旋或者右旋来完成更改节点之后的平衡性。 平衡二叉树随着数据增多，高度会增加，作为索引的数据结构可能查找一个数据可能要好多次IO。 B-tree和B+Tree平衡二叉树存在树过高导致的IO问题，B-Tree用来解决的思想是在每一个节点上存放多个元素，来降低树的高度，以来减少磁盘IO。 B-TreeB-Tree是一个多路搜索树，m阶的B-tree有如下特性： 每个节点最多有m棵子树。 若根节点不是叶子节点，则至少有两个孩子。 每个节点上存有关键字（索引值），且节点的子树按照关键字排序。 每个节点上的关键字都有一个指针，指针指向子树的所有节点索引值都小于当前索引值，都大于上一个索引值。 所有叶子节点都在同一层。 B-Tree中索引和除了索引之外的列是放在一起的，非叶子节点没有冗余索引，同时上层节点中索引关联的指针指向叶子节点，索引节点的左边孩子节点都比索引值小，右边节点都比索引值大。 这样的多路搜索已经降低了树的高度。 但Mysql还是采用了B+Tree。 B+Tree是B-Tree的变种，也是一种多路搜索树，有几点不同： 非叶子节点的子树指针与索引个数相同 非叶子节点只存储索引信息，不存放其余的列。 非叶子节点冗余了索引值。 所有叶子节点之间用双向指针连接，形成双向链表。 叶子节点存放索引之外，还存放了其余列。 为什么用B+Tree而不用B树？ 非叶子节点不存放数据，单个节点能装下更多索引的值，降低了树的高度。 叶子节点双向指针连接，叶子节点中记录项用单向指针连接，提高了区间访问和范围查找的效率。 Hash结构为什么不使用hash结构来存放索引呢？ hash碰撞和扩容问题。 hash数据结构也能快速根据索引列定位数据。hash算法的优点 hash能支持 = in查询，但对于范围查询不好支持。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>索引数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis和zk分布式锁比较]]></title>
    <url>%2Fblog%2F2021%2F07%2F24%2Fredis%E5%92%8Czk%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[比较性能和可用性 Redis加锁性能更高，但会出现锁丢失数据不一致问题；ZK加锁性能没有那么高，但集群通过ZAB协议（写入时原子广播，故障恢复时zxid最大作为Leader节点）保证了数据一致性。 Redis主从/哨兵或者cluster模式下都是AP的，保证了可用性而采用master异步复制数据给slave，存在master加锁成功之后挂了，slave未同步master数据之后再升级为新的主节点，之前的master锁数据就丢失的场景。 Redis也提供了RedLock算法来解决锁丢失问题，写入奇数个master节点，但性能差且对集群部署方式有要求。 ZK集群是CP模式，写leader节点数据的过程中需要同步广播数据到follower节点，保证了数据一致性，不会出现数据不一致的场景。但是如果leader节点挂了，重新选举的过程中zk集群不可使用，牺牲了可用性。 加锁原理 Redis加锁（Redisson）是利用了hash结构存放，key(hash的field)是uuid:threadid，value是重入次数来实现的。默认Redisson是非公平锁，其他被阻塞的客户端订阅锁释放或者自旋重试获取锁。 Zk是创建临时顺序节点来实现分布式锁的，多个客户端会在写入的时候判断写入的序号是否为当前临时节点的第一个，不是的话注册Watcher监听上一个序号的临时节点，wait()实现阻塞，等上一个节点删除之后触发notifyAll()唤醒来重新尝试加锁。 锁续期Redis的Redisson实现分布式锁在重入加锁、重入解锁有锁续期逻辑，且调用默认的lock方法时会注册一个watchdog任务来完成锁续期的逻辑。所以可以支持锁续期；ZK默认不支持锁续期，当然zk是在执行完成之后去释放锁，不存在业务执行超过锁时间的问题。 公平锁 Redisson要实现公平锁还得借助增加list（排队）和zset（加锁超时），复杂度变高。 zk监听上一个序号的临时节点，支持公平锁。先创建临时节点的先加锁。]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZK实现分布式锁]]></title>
    <url>%2Fblog%2F2021%2F07%2F21%2FZK%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[zk实现分布式锁的核心原理zk实现分布式锁的核心原理是利用临时顺序节点和其监听Watcher机制。临时节点有些特点： 生命周期是随着客户端的session周期的，session长时间心跳失败或者客户端关闭session，临时节点会自动删除。 临时目录节点不能创建子目录 基本过程：当客户端抢到锁之后为这个客户端分配一个临时节点，只要锁没有释放就一直持有这个临时节点，当锁释放或者服务意外宕机之后，临时节点会删除，其他客户端可以继续抢占锁（创建临时节点） 未竞争到锁的客户端会阻塞等待，并会开一个监听器监听上一个节点，如果上一个节点释放了锁，那么立即得到通知去上锁。所以加锁的客户端需要感知到上一个客户端节点是谁，也就需要有顺序编号的临时节点，即临时顺序节点。 最后的释放锁就是持有锁的节点删除临时顺序节点，注册监听的其他客户端会收到通知，然后去创建自己的临时顺序节点加锁。 zk实现分布式锁的demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106/** * curator封装的分布式锁api demo * 提供了几种锁 * InterProcessMutex 分布式可重入互斥锁 * InterProcessReadWriteLock 分布式可重入读写锁 * InterProcessSemaphoreMutex 分布式不可重入互斥锁 * InterProcessSemaphoreV2 分布式信号量 */public class CuratorDistributeKLockDemo &#123; static &#123; // 输出日志info级别 LoggerContext loggerContext = (LoggerContext) LoggerFactory.getILoggerFactory(); List&lt;Logger&gt; loggerList = loggerContext.getLoggerList(); loggerList.forEach(logger -&gt; &#123; logger.setLevel(Level.INFO); &#125;); &#125; public static void main(String[] args) throws Exception&#123; // zk地址 如果是集群 多个节点地址逗号隔开 String address = "127.0.0.1:2181"; // 重试策略 连接不上服务端的时候 会重试 重试间隔递增 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); CuratorFramework client = CuratorFrameworkFactory.newClient(address, retryPolicy); // 启动zk客户端 client.start(); try &#123; testInterProcessMutex(client); System.in.read(); &#125; catch(Exception e) &#123; e.printStackTrace(); &#125; finally &#123; client.close(); &#125; &#125; static void testInterProcessMutex(CuratorFramework client) &#123; ExecutorService executorService = Executors.newFixedThreadPool(10); InterProcessMutex lock = new InterProcessMutex(client, "/curator-demo/lock"); for (int i =0 ; i &lt; 3; i++) &#123; executorService.execute(new Task(lock, ("任务" + i))); &#125; &#125; static class Task implements Runnable &#123; private final String taskName; private final InterProcessMutex lock; public Task(InterProcessMutex lock, String taskName) &#123; this.taskName = taskName; this.lock = lock; &#125; @SneakyThrows @Override public void run() &#123; while (true) &#123; lock.acquire(); try &#123; info(taskName + ": 成功获取锁 #1"); // 模拟业务耗时 Thread.sleep(1000); // 尝试锁重入 methodA(); &#125; catch (Exception e) &#123; System.out.println(taskName + ": Happen Exception: " + e.getMessage()); &#125; finally &#123; info(taskName + ": 释放锁 #1\n"); try &#123; lock.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; void methodA() &#123; try&#123; lock.acquire(); info(taskName + ": 成功获取锁 #2"); // 模拟业务耗时 Thread.sleep(1000); &#125; catch (Exception e) &#123; System.out.println(taskName + ": Happen Exception: " + e.getMessage()); &#125; finally &#123; info(taskName + ": 释放锁 #2"); try &#123; lock.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; private static DateTimeFormatter formatter = DateTimeFormatter.ofPattern("HH:mm:ss.SSS"); private static void info(String msg) &#123; String time = formatter.format(LocalTime.now()); String thread = Thread.currentThread().getName(); String log = "["+time+"] "+ " &lt;"+ thread +"&gt; " + msg; System.out.println(log); &#125;&#125; zk分布式重入锁实现加锁客户端加锁入口是：12345// org.apache.curator.framework.recipes.locks.InterProcessMutex#acquire(long, java.util.concurrent.TimeUnit)public boolean acquire(long time, TimeUnit unit) throws Exception &#123; // 直接调用internalLock()方法 return internalLock(time, unit);&#125; 内部的internalLock方法（删减代码版）：1234567891011121314// org.apache.curator.framework.recipes.locks.InterProcessMutex#internalLockprivate boolean internalLock(long time, TimeUnit unit) throws Exception &#123; // 获取当前线程 Thread currentThread = Thread.currentThread(); // 尝试加锁 String lockPath = internals.attemptLock(time, unit, getLockNodeBytes()); // 加锁成功的话就放到threadData里 是一个currentHashMap中，缓存在本地是为了锁重入 if ( lockPath != null ) &#123; LockData newLockData = new LockData(currentThread, lockPath); threadData.put(currentThread, newLockData); return true; &#125; return false;&#125; 尝试加锁调用的internals.attemptLock方法123456789101112131415161718192021222324252627// org.apache.curator.framework.recipes.locks.LockInternals#attemptLock// 下面是删减后的代码String attemptLock(long time, TimeUnit unit, byte[] lockNodeBytes) throws Exception &#123; try &#123; // 创建这个锁 ourPath = driver.createsTheLock(client, path, localLockNodeBytes); // 多个client抢锁时互斥阻塞等待 hasTheLock = internalLockLoop(startMillis, millisToWait, ourPath); &#125; catch ( KeeperException.NoNodeException e ) &#123; //... &#125; return null;&#125;// createsTheLock内部逻辑// org.apache.curator.framework.recipes.locks.StandardLockInternalsDriver#createsTheLock// 下面代码是删减后的代码public String createsTheLock(CuratorFramework client, String path, byte[] lockNodeBytes) throws Exception &#123; return client .create() .creatingParentContainersIfNeeded() .withProtection() // 就是在对应的目录下创建一个临时顺序节点 .withMode(CreateMode.EPHEMERAL_SEQUENTIAL) .forPath(path, lockNodeBytes);&#125; 可以看到加锁就是创建了一个临时顺序节点，加锁成功会构建一个LockData类型的对象，内部维护了线程id、重入次数和锁节点的路径，根据线程id为key缓存在本地的一个ConcurrentHashMap中。 创建的锁临时顺序节点示例： 锁的目录会加一个UUID防止幽灵节点。即创建成功但因为网络原因客户端没拿到成功的结果再次去重试创建对应的节点，会带着UUID去查询，如果存在说明不需要去重试。 整体流程： 锁重入锁的重入次数就是维护在本地的一个map中，value是一个LockData结构。12345678910111213141516private final ConcurrentMap&lt;Thread, LockData&gt; threadData = Maps.newConcurrentMap();// LockData结构 private static class LockData&#123; final Thread owningThread; final String lockPath; // 锁的重入次数 final AtomicInteger lockCount = new AtomicInteger(1); private LockData(Thread owningThread, String lockPath) &#123; this.owningThread = owningThread; this.lockPath = lockPath; &#125;&#125; 可以看到在加锁流程中，以当前线程为key查询LockData对象如果存在，直接是把其lockCount++，以此来记录锁的重入次数。12345678// 根据当前线程获取锁对象，如果获取到了，那肯定是有锁的，这次是锁重入LockData lockData = threadData.get(currentThread);if ( lockData != null ) &#123; // 锁重入的关键：其实就是我们上面说的那个AtomicInteger原子类自增1 lockData.lockCount.incrementAndGet(); // 加锁直接返回成功 return true;&#125; 所以锁重入的流程嵌入加锁流程为： 锁互斥等待当客户端尝试获取锁时，如果发现/lock目录被占用，这里为了满足锁的互斥性，当前客户端要互斥等待Watcher回调通知。 这里互斥等待的逻辑在LockInternals#internalLockLoop方法中12345678910111213141516171819202122232425262728293031// org.apache.curator.framework.recipes.locks.LockInternals#internalLockLoop// 下面代码是删减后的代码private boolean internalLockLoop(long startMillis, Long millisToWait, String ourPath) throws Exception &#123; boolean haveTheLock = false; try &#123; while ((client.getState() == CuratorFrameworkState.STARTED) &amp;&amp; !haveTheLock) &#123; // 获取path下对应临时顺序节点，并按编号从小到大排序。底层采取的java.util.Comparator#compare来排序的 List&lt;String&gt; children = getSortedChildren(); // 获取当前线程创建的临时顺序节点名称 String sequenceNodeName = ourPath.substring(basePath.length() + 1); // 这个方法底层就是判断当前节点编号是不是children里的第一个，是的话就能抢锁，不是的话就计算出上一个节点序号是谁，然后下面监听这个节点。（因为按照编号排序了，所以可以得出上一个节点是谁） PredicateResults predicateResults = driver.getsTheLock(client, children, sequenceNodeName, maxLeases); // 如果当前客户端就是持有锁的客户端，直接返回true if (predicateResults.getsTheLock() ) &#123; haveTheLock = true; &#125; else &#123; // 如果没抢到锁，则监听上一个节点 String previousSequencePath = basePath + "/" + predicateResults.getPathToWatch(); synchronized(this) &#123; try &#123; // 监听器，watcher下面分析 client.getData().usingWatcher(watcher).forPath(previousSequencePath); // 重点在这了，wait()，等待。也就是说没抢到锁的话就开启监听器然后wait()等待。 wait(); &#125; catch ( KeeperException.NoNodeException e ) &#123;&#125; &#125; &#125; &#125; &#125; return haveTheLock;&#125; 逻辑为： 查询所有的临时顺序节点（全部抢锁的客户端），按照编号从小到大排序； 判断当前客户端的编号是不是第一个，是的话代表加锁成功。不是的话要计算上一个节点序号，注册Watcher监听此节点。（解锁之后删除此节点会收到Watcher的通知回调） 调用wait()方法实现阻塞。 注册的Watcher通知回调逻辑：（其实就是notifyAll来实现）123456789101112131415161718// org.apache.curator.framework.recipes.locks.LockInternals#watcherprivate final Watcher watcher = new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 调postSafeNotify方法 client.postSafeNotify(LockInternals.this); &#125;&#125;;// org.apache.curator.framework.CuratorFramework#postSafeNotifydefault CompletableFuture&lt;Void&gt; postSafeNotify(Object monitorHolder) &#123; return this.runSafe(() -&gt; &#123; synchronized(monitorHolder) &#123; // 重点在这里，notifyAll。通知所有等待（wait）的节点。 monitorHolder.notifyAll(); &#125; &#125;);&#125;s 锁释放锁释放调用的是release方法： 123456789101112131415161718192021// org.apache.curator.framework.recipes.locks.InterProcessMutex#release// 下面代码是删减后的代码public void release() throws Exception &#123; // 获取当前线程 Thread currentThread = Thread.currentThread(); // 获取当前线程的锁对象，从ConcurrentHashMap里获取 LockData lockData = threadData.get(currentThread); // 锁重入次数-1，然后看看是不是大于0，如果大于0那代表有锁重入，直接-1，不删除锁节点，因为没释放完全。 int newLockCount = lockData.lockCount.decrementAndGet(); if ( newLockCount &gt; 0 ) &#123; return; &#125; try &#123; // 如果锁重入次数为0了，那就释放锁 internals.releaseLock(lockData.lockPath); &#125; finally &#123; // 释放完后从ConcurrentHashMap里移除 threadData.remove(currentThread); &#125;&#125; 流程为： 根据线程对象从锁缓存中获取锁对象，对其中的锁重入 次数-1 如果大于0 说明是重入的解锁逻辑，直接返回。 如果锁重入次数为0，直接去删除对应的临时顺序节点即可。（删除之后ZK会发送之前阻塞在这个路径节点下Watcher通知，通知逻辑及时notifyall wait在同一锁对象的client线程，让其再去按照序号去竞争锁，可以看到保证了公平性） 缓存中remove此锁对象。 zk实现分布式锁小结 一些问题为什么去设计排队，让被阻塞的客户端去只监听上一个序号的目录节点呢？最基本的zk分布式锁实现：不考虑临时顺序节点，只去创建一个临时节点/lock。 未竞争到锁的客户端去监听外层的/lock节点的删除事件。 这带来两个问题： 非公平。很显然没有顺序，再次抢占锁的客户端是非公平的。 羊群效应：比如1000个client被锁节点阻塞，都注册Watcher到/lock节点，当持有锁的节点释放锁时候，会并发去抢占锁，给ZK服务端带来羊群效应。 基于这两个问题，利用ZK的临时顺序节点的功能，让被阻塞的节点只监听上一个序号的节点，去注册Watcher，实现了公平排队获取，也避免了羊群效应。]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>zk实现分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redisson锁续期]]></title>
    <url>%2Fblog%2F2021%2F07%2F16%2FRedisson%E9%94%81%E7%BB%AD%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[watchDog核心原理定时检测业务是否执行完毕，没结束的话观察这个key是否经历的锁超时时间的的三分之一，如果经历了，那么进行到期时间的重新设置，防止业务没有执行完就被释放锁造成的并发场景。 这其中还有一些实现细节，比如每个加锁的方法都去起一个线程去执行锁的续期吗？（Redisson这里不是直接的异步线程，而是借助了Netty的时间轮）；再比如所有的加锁场景都会注册触发watchdog的锁续期吗？（这里如果传入了超时时间leaseTime，内部不会注册watchDog续期任务） WatchDog注册的条件不是所有的lock方法都会去触发watchDog的自动续期功能。12345678910111213141516171819202122232425private &lt;T&gt; RFuture&lt;Long&gt; tryAcquireAsync(long leaseTime, TimeUnit unit, final long threadId) &#123; if (leaseTime != -1) &#123; // 如果传入的leaseTime不为-1 （即用户设置了leaseTime） // 去直接调用加锁逻辑 return tryLockInnerAsync(leaseTime, unit, threadId, RedisCommands.EVAL_LONG); &#125; RFuture&lt;Long&gt; ttlRemainingFuture = tryLockInnerAsync(commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.addListener(new FutureListener&lt;Long&gt;() &#123; @Override public void operationComplete(Future&lt;Long&gt; future) throws Exception &#123; if (!future.isSuccess()) &#123; return; &#125; Long ttlRemaining = future.getNow(); // lock acquired if (ttlRemaining == null) &#123; // 大前提是leaseTime设置了-1 // 如果返回了null 说明加锁成功 这里去续期 scheduleExpirationRenewal(threadId); &#125; &#125; &#125;); return ttlRemainingFuture; &#125; 可以看到当调用lock(leaseTime)加锁api时，内部不会触发watchDog任务的建立，这里在使用的时候要注意，只有lock()方法才去做了watchDog的续期任务创建，默认超时释放时间是30s。 watchDog任务删除在执行unlock的时候，会去删除注册的watchDog续期任务。1234567891011121314151617181920212223@Overridepublic void unlock() &#123; Boolean opStatus = get(unlockInnerAsync(Thread.currentThread().getId())); if (opStatus == null) &#123; throw new IllegalMonitorStateException("attempt to unlock lock, not locked by current thread by node id: " + id + " thread-id: " + Thread.currentThread().getId()); &#125; if (opStatus) &#123; // 解锁成功 取消watchdog任务 cancelExpirationRenewal(); &#125;&#125;// cancelExpirationRenewal方法 void cancelExpirationRenewal() &#123; // map中移除 Timeout task = expirationRenewalMap.remove(getEntryName()); if (task != null) &#123; // task去取消 内部是加入到时间轮的取消队列中 task.cancel(); &#125;&#125; WatchDog的逻辑看下scheduleExpirationRenewal方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private void scheduleExpirationRenewal(final long threadId) &#123; // watchDog任务的缓存 if (expirationRenewalMap.containsKey(getEntryName())) &#123; // 缓存中存在 直接返回 等待调度即可。 return; &#125; // 利用时间轮去创建一个任务 内部会有一个线程扫描并执行续期操作。 Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() &#123; @Override public void run(Timeout timeout) throws Exception &#123; // 执行lua脚本去设置新的续期时间 RFuture&lt;Boolean&gt; future = commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return 1; " + "end; " + "return 0;", Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId)); future.addListener(new FutureListener&lt;Boolean&gt;() &#123; @Override public void operationComplete(Future&lt;Boolean&gt; future) throws Exception &#123; // map中移除，只要续期任务被触发一次 在返回中就先删除缓存中的任务 expirationRenewalMap.remove(getEntryName()); if (!future.isSuccess()) &#123; log.error("Can't update lock " + getName() + " expiration", future.cause()); return; &#125; if (future.getNow()) &#123; // reschedule itself // 在返回结果再次递归调用自己去续期 scheduleExpirationRenewal(threadId); &#125; &#125; &#125;); &#125; // 任务执行的时间间隔 &#125;, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); if (expirationRenewalMap.putIfAbsent(getEntryName(), task) != null) &#123; // 缓存中存在 任务取消 说明已经注册过续期任务了 task.cancel(); &#125; &#125; 在执行完续期操作之后，在返回值的回调中又去调用scheduleExpirationRenewal方法注册续期任务，来触发下一次的续期。 续期的lua脚本逻辑： 判断key、uuid:tid这个hash结构的锁是否存在，即当前线程是否持有锁 如果不存在，返回0。可以直接取消任务（这里应该认为unlock了，取消了续期任务） 如果存在，去设置新的超时时间，相当于去续期。 整个任务注册的delay执行时间是三分之一 internalLockLeaseTime，按照默认配置是10s之后的任务。 WatchDog任务的执行运用了Netty中的时间轮。和Dubbo中运用的时间轮大同小异。这里不是每加锁都会起一个线程去执行watchDog的逻辑，而是集中式的在时间轮中去管理这个定时任务。 总结]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>watchdog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo服务调用过程]]></title>
    <url>%2Fblog%2F2021%2F07%2F16%2Fdubbo%E6%9C%8D%E5%8A%A1%E8%B0%83%E7%94%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[调用过程消费端消费端的发送请求模型如下： 调用过程总结客户端发送请求 调用一个dubbo接口，此时调用的是在服务引入过程生成的动态代理对象，内部代理逻辑就是构造RpcInvocation请求参数，然后调用Invoker.invoke方法。 Invoker会通过ClusterInvoker，依次调用服务目录Directory的list、服务路由的router、负载均衡的 select选出一个具体的Invoker，本身也会根据Cluster的配置实现对应的集群容错的处理。 按照顺序执行客户端的invokerFilter链，然后走到一个AsyncToSyncInvoker，内部会持有DubboInvoker.invoke方法，内部会通过NettyClient发起真正的远程调用。在其中返回的AsyncRpcResult中会持有发起调用时生成的DefaultFuture，且每个Future都绑定了一个id，在AsyncToSyncInvoker中就是调用了其get()方法来实现的Netty异步网络调用但阻塞了当前调用的业务线程。 服务端接收请求 NettyServer收到请求之后，根据具体的协议反序列化为对象，然后按照派发策略将请求消息封装为ChannelEventRunnable发送给线程池中（DubboServerHandler）执行，IO线程返回继续处理读/写请求。 Dubbo服务线程会判断消息类型最后执行到在DubboProtocol内部实现的ExchangeHandlerAdapter，将请求调用到期reply方法处理。 Dubbo服务线程根据serviceKey从服务导出时存入的exporterMap中找到对应的DubboExporter，经过一系列的服务端的InvokerFilter链之后，最终通过反射调用到真正服务实现类的方法。最后通过DubboServerHandler向客户端发送结果Response。 客户端接收返回请求 和服务端接收请求一样，经历反序列化之后，按照派发策略派发给DubboClientHandler线程执行。 根据之前绑定DefaultFuture的id从本地Future的缓存Map中找到DefaultFuture，通过DefaultFuture.received(response)来唤醒阻塞在AsyncToSyncInvoker的client线程，将最后的结果返回到调用处即可。 客户端/服务端接收数据（请求、结果写回）之后的线程派发模型]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo调用过程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redisson实现分布式锁]]></title>
    <url>%2Fblog%2F2021%2F07%2F14%2FRedisson%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[Redisson实现分布式锁的基本原理锁实例代码1234567891011121314151617181920212223242526272829303132333435363738394041static int count = 0; public static void main(String[] args) throws InterruptedException &#123; Config config = new Config(); // 单机地址 config.useSingleServer().setAddress("redis://127.0.0.1:6379"); // 获取redisson的client RedissonClient redissonClient = Redisson.create(config); RLock lock = redissonClient.getLock("distribute-lock-key"); for (int i=0; i &lt; 100; i++) &#123; new Thread(() -&gt; &#123; try &#123;// lock.tryLock(2000, 1000, TimeUnit.MILLISECONDS); // 写入：redis是hash结构： // key：distribute-lock-key // field:随机数:线程id // value：重入次数 lock.lock(1000, TimeUnit.MILLISECONDS); count++; System.out.println("线程" + Thread.currentThread().getName() + "获取到了锁 当前count:" + count); // 模拟业务线程的执行 Thread.sleep(200); try &#123; // 尝试重入 value是重入次数 lock.lock(1000, TimeUnit.MILLISECONDS); Thread.sleep(300); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;).start(); &#125; new CountDownLatch(1).await(); &#125; lock操作加锁关键代码：12345678910111213141516171819&lt;T&gt; RFuture&lt;T&gt; tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand&lt;T&gt; command) &#123; internalLockLeaseTime = unit.toMillis(leaseTime); // 运用lua脚本完成加锁逻辑 return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command, "if (redis.call('exists', KEYS[1]) == 0) then " + "redis.call('hset', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " + "redis.call('hincrby', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + "return redis.call('pttl', KEYS[1]);", // 传入lua脚本的参数 Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId));&#125; 逻辑是运用了redis的lua脚本去完成加锁逻辑。可以分开看： 第一段12345"if (redis.call('exists', KEYS[1]) == 0) then " + "redis.call('hset', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; 这里KEYS[1]就是锁的key，即redissonClient.getLock(“distribute-lock-key”)中的distribute-lock-key。 AVG[2]是field的值，Redisson使用的是redis的hash结构。这里是 uuid:threadId。因为存在多台机器，所以会加上一个uuid。 AVG[1]是设置key的过期时间。 这段逻辑流程是： 判断distribute-lock-key 在redis中是否存在，结果为0是不存在、 不存在调用hset key uuid:threadId 1 设置key、uuid:threadId是field、value是1。 设置过期时间 返回null代表成功。 第二段12345"if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " + "redis.call('hincrby', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; 如果存在key、field hash值 此时就是重入场景，把对应的value加一即可。 命令是hincrby 再次设置下过期时间，这里相当于完成了一次锁续期。pexpire 返回null表示成功。 第三段1redis.call('pttl', KEYS[1]); 这里的场景是redis中存在key这把锁，但持有锁的不是当前线程，即锁被别的线程持有。这时候调用 pttl命令返回key的过期时间。 这里知道了加锁场景下，如果返回了null说明加锁成功，返回过期时间说明锁被占用。 拿到结果之后，可以根据ttl进行重试加锁。12345678910111213141516171819202122232425262728293031323334353637383940 @Override public void lockInterruptibly(long leaseTime, TimeUnit unit) throws InterruptedException &#123; long threadId = Thread.currentThread().getId(); // 内部调用了lua加锁逻辑 Long ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired if (ttl == null) &#123; // 为null 加锁成功返回 return; &#125; // 如果ttl不是null 订阅对应key等待锁删除时的通知 // 释放锁的时候会订阅key的客户端可以收到通知 RFuture&lt;RedissonLockEntry&gt; future = subscribe(threadId); commandExecutor.syncSubscription(future); try &#123; while (true) &#123; // 再次尝试获取 ttl = tryAcquire(leaseTime, unit, threadId); // lock acquired if (ttl == null) &#123; // 获取到跳出循环 break; &#125; // waiting for message if (ttl &gt;= 0) &#123; // ttl大于0 去利用semaphore的tryAcquire去阻塞获取 内部是Semaphore(0) getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); &#125; else &#123; // 小于0去调用 Semaphore的acquire()获取 getEntry(threadId).getLatch().acquire(); &#125; &#125; &#125; finally &#123; unsubscribe(future, threadId); &#125;// get(lockAsync(leaseTime, unit)); &#125; Redisson加锁流程总结： unlock解锁流程1234567891011121314151617rotected RFuture&lt;Boolean&gt; unlockInnerAsync(long threadId) &#123; return evalWriteAsync(getRawName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, "if (redis.call('hexists', KEYS[1], ARGV[3]) == 0) then " + "return nil;" + "end; " + "local counter = redis.call('hincrby', KEYS[1], ARGV[3], -1); " + "if (counter &gt; 0) then " + "redis.call('pexpire', KEYS[1], ARGV[2]); " + "return 0; " + "else " + "redis.call('del', KEYS[1]); " + "redis.call('publish', KEYS[2], ARGV[1]); " + "return 1; " + "end; " + "return nil;", Arrays.asList(getRawName(), getChannelName()), LockPubSub.UNLOCK_MESSAGE, internalLockLeaseTime, getLockName(threadId));&#125; 这段lua脚本的逻辑： 如果hexists命令调用之后发现key、field的hash结构不存在，（lock(leaseTime)没有watchDog机制续约锁），那就是锁超时过期释放了，无需释放直接返回。 如果当前线程持有锁（也就是key、field的hash结构存在） 给锁的的value-1。hincrby key uuid:tid -1命令。 返回null 判断减1之后的值，如果还大于0，说明是锁重入场景的解锁。这里去锁续期一下（默认30s）。返回0 减1之后为0，去删除key，然后发布删除key通知。（redis的发布订阅模式 pub/sub） 返回1代表解锁成功。]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>Redisson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis实现分布式锁]]></title>
    <url>%2Fblog%2F2021%2F07%2F12%2Fredis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[关键点有四个关键点： 原子性 过期时间 锁续期 正确释放锁 原子性按照分布式锁的实现比如用两个命令：12redis.set(key,value);redis.expire(key, time, unit); 但是这个不是原子性的。比如设置了key和value，但是没有设置失效时间，则会出现死锁的问题。 过期时间这个没啥可说的，分布式锁防止死锁的做法。 锁续期当业务执行时间超过超时时间，则可能出现其他客户端获取锁从而并发执行，失去了分布式锁的意义。这时候就是锁续期。 开辟另外一个线程，专门用于锁续期，加锁的时候就起个线程进行死循环续期，核心流程就是判断锁的时间过了三分之一则就重新续期为上锁时间。 比如，加锁时间是3s，执行1s没有释放锁之后，会为这个key的锁重新设置超时时间为3s。 正确释放锁释放锁可能有问题： 可能释放别人的锁。其实还是上边提到的问题，比如锁设置了3s的超时时间，执行了4s，但是这时别的客户端或者线程获取了锁，所以可能删除了其他客户端的锁。 而且别的客户端可能上来就是搞破坏，可能有代码实现的问题，未加锁就解锁。 这两个问题总结起来就是要正确的释放锁。 利用锁续期机制，防止业务没执行完成就释放锁的情况 释放的时候要判断是否为自己加的锁，避免释放别人的锁。 关于锁续期的伪代码：123456789101112131415161718192021222324252627282930313233343536373839// 标识当前锁是否在运行中private volatile boolean isRunning;// 抢锁成功if (RESULT_OK.equals(client.setNxPx(key, value, ttl))) &#123; // 启动续期线程 renewalTask = new RenewTask(new IRenewalHandler() &#123; @Override public void callBack() throws LockException &#123; // 刷新值 client.expire(key, ttl &lt;= 0 ? 10 : ttl); &#125; &#125;, ttl); renewalTask.start();&#125;// 省略释放锁的代码 里面也要维护isRunning// 续期线程的逻辑@Overridepublic void run() &#123; while (isRunning) &#123; try &#123; // 1、续租，刷新值 call.callBack(); LOGGER.info(&quot;续租成功!&quot;); // 2、三分之一过期时间续租 TimeUnit.SECONDS.sleep(this.ttl * 1000 / 3); &#125; catch (InterruptedException e) &#123; close(); &#125; catch (LockException e) &#123; close(); &#125; &#125;&#125;public void close() &#123; isRunning = false;&#125; 另一个是要在删除时判断出是否为此次加锁，因为是分布式的，线程id可能重复，所以不能单单用线程id作为value，这里建议是用当前业务上下文中的userId或者其他随机数。但这里也要注意一下线程安全问题，因为在解锁（删除锁）时如果是先判断再删除，则有原子性问题：1234// 代码有原子性问题if (userId).equals(redis.get(lockKey)) &#123; redis.delete(key);&#125; 此原子性问题可能是：比如在get之后，del之前锁超时自动释放，那么可能在执行del的时候还是删除了刚刚加锁的其他客户端。当然如果保证了锁续期，那么这里不会出现此原子性问题，但是还是建议这里判断删除是用lua脚本提供原子性：123456// 如果get的值等于传进来的值，就给它delif redis.call(&quot;get&quot;,KEYS[1])==ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end Redis的部署方式 单机单机模式下面就是一台redis为分布式锁的存储，优点是方便易部署，缺点是存在单机故障。 哨兵（Sentinel）有单点故障，会搞几个slave从节点做备份，redis很好支持了sentinel模式。但是这里涉及master和slave的数据一致性问题。锁写到master，没有同步到slave，slave选举为master之后slave中没有锁，相当于此时其他客户端也能加锁。 集群集群只是做了slot分片，降低了一定概率，锁还是只写到一个master上，和哨兵一样存在数据一致性的问题。 红锁（RedLock）红锁的思路是：搞几个独立的master节点，比如5个，然后挨个加锁，只要超过一半以上（这里是3个）就代表加锁成功，这样一台master成功，还有其他的master节点，不耽误。虽然解决了上面集群模式的问题，但是性能影响很大，且集群维护也需要部署成本。 Redis实现分布式锁的优缺点优点 只引入了redis，大多数公司都在用redis，社区活跃，问题容错性高。 redis相对来说还是很简单的中间件，性能比较高，符合分布式锁的高性能。 多个客户端支持，redisson客户端用法封装（公平锁、可重入锁、读写锁）。缺点 存在集群模型下的各种问题，不能百分百安全， 即使是红锁，也是需要牺牲性能（每次写过半节点，多次redis IO通讯），需要进行权衡。]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql实现分布式锁]]></title>
    <url>%2Fblog%2F2021%2F07%2F11%2Fmysql%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[实现原理 利用唯一索引来实现，只要抢占锁就插入了一记录，互斥性体现在其他客户端插入重复数据会报唯一key的异常。 具体的步骤： 客户端1先来加锁，insert一条记录 客户端2来加锁，插入一条数据返回重复key的异常，这时候就进行重试加锁。 持有锁的线程执行完逻辑释放锁，即删除数据库中的记录即可。其他客户端可以进行抢锁了。 但是也有一些问题： 分布式锁的表怎么设计，一个业务一张表？ 抢占锁要怎么去做重试 释放锁如果安全的释放锁。 表设计和唯一key 为了每个业务不去都加这个锁的表，则设计一个能兼容多个业务的锁表，抽象锁的业务id为resource_id；且标识是哪个业务，有project_name、method_name，同时在数据库层可以将唯一索引抽象为lock_id，生成规则可能是project_name下划线method_name下划线resource_id；另外为了实现可重入，要记录一个字段entry_count作为重入次数，同时保留加锁的host_ip和thread_id。 比如表DDL：12345678910DROP TABLE IF EXISTS `common_lock`;CREATE TABLE `common_lock` ( `id` int NOT NULL, `lock_key` varchar(100) NOT NULL, `thread_id` int NOT NULL, `entry_count` int NOT NULL, `host_ip` varchar(30) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `lock_key` (`lock_key`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 释放锁上面提到释放锁就是delete这条记录，但是因为有了重入次数，所以释放锁可能是重入次数-1的过程。 防止死锁如果加锁之后服务端发生宕机未释放锁，则可能造成死锁。当然这里可以记录下超时时间，然后定时任务去扫描超时未释放的数据，然后将其删除。但这样还是出现一个问题，就是定时任务不能判断出是否业务逻辑超时，如果业务超时时间内未完成，定时任务删除了锁记录，也可能出现问题，mysql这里不太好解决这个问题。（redis解决可以有watchDog机制和续期的线程）。 MySQL实现分布式锁的优缺点：优点 实现方式简单，加锁是insert记录，重入是重入次数+1，删除锁是update重入次数或者删除记录。 不依赖其他中间件，数据库的高可用保障性强。 缺点 性能很低，支持并发不高。最简单的不考虑重入场景，也需要和数据库进行两次io操作。 线程不安全，为了防止死锁引入定时任务扫描，没办法解决续约的场景。 还是依赖了MySQL，没有百分之百高可用的中间件。 没办法支持读写锁或者公平锁。]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>mysql实现分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁基础]]></title>
    <url>%2Fblog%2F2021%2F07%2F11%2F%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[为什么需要分布式锁 锁能保证多线程环境下对同一份资源竞争的数据安全性 分布式锁是 用于保证集群内多台机器的多线程并发线程安全的一种手段。 分布式锁的特征 高性能：不能影响主要的业务，只是为了安全的一种手段。 可重入：可重入的条件就是持有锁的线程就是当前请求的线程，重入几次就释放几次。 防止死锁：不能永远加锁。比如有超时时间和自动超时。 互斥性：锁就是要保证这一点。 高性能要求了不能每次都从磁盘上拿数据，锁要存放在内存中（当然会带来高可用的问题），所以业内比较成熟的是redis和zk来做分布式锁。 可重入的实现需要记录当前threadId的重入次数，获取锁时如果锁存在就判断是不是当前线程，如果是的话只需要给重入数+1。不是当前线程相当于没有获取到锁。 防止死锁：锁加超时时间，避免死锁。 分布式锁的实现 Mysql redis zk 分布式锁的应用 防止缓存击穿。如果击穿瞬间不加锁，则可能打垮数据库，在缓存击穿查询数据库的时候加分布式锁，查出来放入缓存，之后的大批请求就会命中缓存，这里能防缓存击穿。 保证接口幂等。防止重复提交，添加分布式锁排队请求，第二次在获取到锁之后加入判断幂等的逻辑。 防止超卖等场景 任务调度。如果没有分布式调度工具，需要保证集群中执行任务的就只有一台。]]></content>
      <categories>
        <category>分布式锁</category>
      </categories>
      <tags>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zk集群及ZAB]]></title>
    <url>%2Fblog%2F2021%2F07%2F08%2Fzk%E9%9B%86%E7%BE%A4%E5%8F%8AZAB%2F</url>
    <content type="text"><![CDATA[集群架构 client节点：从业务角度来看，这是分布式应用中的一个节点，通过长连接和zkServer端建立连接，定时发送心跳。从集群角度来看，是集群的客户端，可以连接集群中的一个节点，进行node添加、删除、更新数据、注册Watcher机制等。 leader节点：zk的主节点，负责zk集群的写操作，保证集群内部事务处理的顺序性。同时也负责Follower和Observer节点的数据同步。 follower节点：ZK集群的从节点，可以接受Client的读请求返回数据，并不处理写请求，而是转发到Leader节点去写数据。follower节点也要参与集群内leader节点的选举。 observer节点：特殊节点，只处理读请求，但不参与集群选举。作用是提高zk的吞吐量，因为一直增加follower节点会带来副作用，leader写数据要半数follower节点都响应ack之后才能commit，follower节点太多会影响写吞吐量。因为Observer节点不参加选举，且能响应读请求，所以可以增加读吞吐量。 ZK的消息广播流程对于写请求，如果Client连接的是Follower节点或者Observer节点，那么会转发给Leader节点进行数据写入和消息广播的流程。Leader节点处理写请求的核心流程： Leader节点收到写请求之后，为写请求赋予一个全局唯一的zxid（64位自增id），通过zxid的大小可以实现写操作的顺序一致性。 Leader会通过先进先出队列（每个Follower节点创建一个队列，保证发送的顺序），将带有zxid消息的作为一个proposal(提案)分发给所有follower节点。 当Follower节点收到proposal提案消息之后，将proposal消息写到本地事务日志，写成功后返回给Leader节点 ACK消息。 当Leader节点收到过半的Follower节点的ACK消息之后，向所有的follower节点去发送Commit消息，并执行本地提交。 Follower节点收到Commit命令之后，去执行本地提交，集群写操作完成。 Leader节点的重新选举（崩溃恢复）写数据的流程中，如果Leader宕机，整个ZK集群可能出现两个状态： Leader节点收到半数Follower节点的ack消息，向各个Follower节点广播Commit命令，在各个Follower节点收到Commit命令之前宕机，剩下的服务器没办法执行本地写commit操作，也就是这次写操作数据还没有commit。 Leader节点在生成proposal之后宕机了，其他Follower没有收到proposal或者未超过半数收到proposal消息，这次写入数据是失败的。 重新选举Leader，ZK集群有两个原则： 对于原Leader提交的proposal（到了执行commit阶段），新的Leader能广播并提交，这样可以选择拥有最大zxid的节点作为新的Leader。 对于原Leader未广播或者少数节点被广播的proposal消息，新的Leader能通知原Leader和已经同步的Follower删除此proposal消息，保证集群数据一致性。 zk的集群选举采用了ZAB协议，一个示例来解释选举新Leader的过程： 启动时的leader选举每个节点刚启动时处于Looking状态，然后进行选举流程。 每个server发出一个投票，初始状态都对自己投票，投票信息包括epoch、zxid、myid（zk进程号），比如server1代表的票是(myid，zxid）是(1,0）、server2是(2,0)，然后发送给集群中参与选举的其他机器。 接收各个服务器的投票，集群中每个服务器收到投票之后，去比较和自己的投票信息： 优先比较epoch，是同一纪元的选票才有效。 再去比较zxid，在启动时都为0。集群选举的时候优先比较 如果zxid相同，最后比较myid。 比较完成之后，按照比较规则修改自己的投票，并且广播投票信息给其他服务器。 超过半数节点统计出相同的票，那么对应节点就是新的leader节点 各个服务器响应leader选举成功消息，改变自己服务器状态为follower节点。 ZAB协议zab协议是在2pc的基础上zk保证数据一致性的协议。（Zookeeper Atomic Broadcast），包含两个最基本的模式： 消息广播（保证写入数据的一致性） 崩溃恢复（Leader挂了之后的集群恢复） Zk就在这两个模式之间进行切换。当Leader可用时，就进行消息广播模式；当Leader不可用时进入崩溃恢复模式。]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>ZAB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZK客户端curator的基本使用]]></title>
    <url>%2Fblog%2F2021%2F07%2F03%2FZK%E5%AE%A2%E6%88%B7%E7%AB%AFcurator%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Curator的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200/** * curator zk客户端api操作demo */public class CuratorApiDemo &#123; public static void main(String[] args) throws Exception&#123; // zk地址 如果是集群 多个节点地址逗号隔开 String address = "127.0.0.1:2181"; // 重试策略 连接不上服务端的时候 会重试 重试间隔递增 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3); CuratorFramework client = CuratorFrameworkFactory.newClient(address, retryPolicy); // 启动zk客户端 client.start();// createZNode(client);// testBackGroundCallback(client);// simpleWatcher(client); watcherWithCache(client); new CountDownLatch(1).await(); &#125; /** * curator 提供了三种cache包装了watcher机制 内部封装了收到watcher通知之后再次进行watcher事件注册的逻辑 对编程很友好 * @param client * @throws Exception */ static void watcherWithCache(CuratorFramework client) throws Exception&#123; // 创建NodeCache，监听的是"/user"这个节点 NodeCache nodeCache = new NodeCache(client, "/user"); // start()方法有个boolean类型的参数，默认是false。如果设置为true， // 那么NodeCache在第一次启动的时候就会立刻从ZooKeeper上读取对应节点的 // 数据内容，并保存在Cache中。 nodeCache.start(true); if (nodeCache.getCurrentData() != null) &#123; System.out.println("NodeCache节点初始化数据为：" + new String(nodeCache.getCurrentData().getData())); &#125; else &#123; System.out.println("NodeCache节点数据为空"); &#125; // 添加监听器 nodeCache.getListenable().addListener(() -&gt; &#123; String data = new String(nodeCache.getCurrentData().getData()); System.out.println("NodeCache节点路径：" + nodeCache.getCurrentData().getPath() + "，节点数据为：" + data); &#125;); // 创建PathChildrenCache实例，监听的是"user"这个节点 PathChildrenCache childrenCache = new PathChildrenCache(client, "/user", true); // StartMode指定的初始化的模式 // NORMAL:普通异步初始化 // BUILD_INITIAL_CACHE:同步初始化 // POST_INITIALIZED_EVENT:异步初始化，初始化之后会触发事件 childrenCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE); // childrenCache.start(PathChildrenCache.StartMode.POST_INITIALIZED_EVENT); // childrenCache.start(PathChildrenCache.StartMode.NORMAL); List&lt;ChildData&gt; children = childrenCache.getCurrentData(); System.out.println("获取子节点列表："); // 如果是BUILD_INITIAL_CACHE可以获取这个数据，如果不是就不行 children.forEach(childData -&gt; &#123; System.out.println(new String(childData.getData())); &#125;); childrenCache.getListenable().addListener(((client1, event) -&gt; &#123; System.out.println(LocalDateTime.now() + " " + event.getType()); if (event.getType().equals(PathChildrenCacheEvent.Type.INITIALIZED)) &#123; System.out.println("PathChildrenCache:子节点初始化成功..."); &#125; else if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_ADDED)) &#123; String path = event.getData().getPath(); System.out.println("PathChildrenCache添加子节点:" + event.getData().getPath()); if (event.getData()!= null &amp;&amp; event.getData() != null) &#123; System.out.println("PathChildrenCache子节点数据:" + new String(event.getData().getData())); &#125; &#125; else if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_REMOVED)) &#123; System.out.println("PathChildrenCache删除子节点:" + event.getData().getPath()); &#125; else if (event.getType().equals(PathChildrenCacheEvent.Type.CHILD_UPDATED)) &#123; System.out.println("PathChildrenCache修改子节点路径:" + event.getData().getPath()); System.out.println("PathChildrenCache修改子节点数据:" + new String(event.getData().getData())); &#125; &#125;)); // 创建TreeCache实例监听"user"节点 TreeCache cache = TreeCache.newBuilder(client, "/user").setCacheData(false).build(); cache.getListenable().addListener((c, event) -&gt; &#123; if (event.getData() != null) &#123; System.out.println("TreeCache,type=" + event.getType() + " path=" + event.getData().getPath()); &#125; else &#123; System.out.println("TreeCache,type=" + event.getType()); &#125; &#125;); cache.start(); &#125; /** * 简单的watcher监听机制 * @param client * @throws Exception */ static void simpleWatcher(CuratorFramework client) throws Exception&#123; client.create().withMode(CreateMode.PERSISTENT).forPath("/test-children-watch"); client.getChildren().usingWatcher(new CuratorWatcher() &#123; // 为getChildren目录节点的子节点去创建一个watcher 注意 watcher只会被触发一次 @Override public void process(WatchedEvent watchedEvent) throws Exception &#123; // type NodeChildrenChanged System.out.println("收到监听回调事件, type: " + watchedEvent.getType()); System.out.println("回调事件的path: " + watchedEvent.getPath()); &#125; &#125;).forPath("/test-children-watch"); // 去添加children 来触发watcher回调 client.create().withMode(CreateMode.EPHEMERAL).forPath("/test-children-watch/child0"); // watcher是一次性的 不再去注册 不会再收到通知 client.create().withMode(CreateMode.EPHEMERAL).forPath("/test-children-watch/child1"); &#125; static void testBackGroundCallback(CuratorFramework client) throws Exception&#123; // backGround是异步处理机制 搭配CuratorListener监听器使用 // 添加一个CuratorListener 来处理backGround后台处理之后的回调 client.getCuratorListenable().addListener(new CuratorListener() &#123; @Override public void eventReceived(CuratorFramework curatorFramework, CuratorEvent curatorEvent) throws Exception &#123; // 针对不通过的事件去处理 String path = curatorEvent.getPath(); System.out.println("当前线程名称：" + Thread.currentThread().getName()); switch (curatorEvent.getType()) &#123; case CREATE: System.out.println("节点创建，path: "+ path); break; case EXISTS: System.out.println("检查节点是否存在。path：" + path); break; case DELETE: System.out.println("delete节点。path: " + path); break; case GET_DATA: System.out.println("执行getdata path:" + path + " 数据：" + new String(curatorEvent.getData())); break; case SET_DATA: System.out.println("执行setData path: " + path); break; case CHILDREN: System.out.println("执行children path : "+ path); break; default: &#125; &#125; &#125;); // inBackGround操作 都是在上边的回调处理 client.create().withMode(CreateMode.PERSISTENT).inBackground().forPath("/curator-background-api", "test".getBytes(StandardCharsets.UTF_8)); client.checkExists().inBackground().forPath("/curator-background-api"); client.getData().inBackground().forPath("/curator-background-api"); client.setData().inBackground().forPath("/curator-background-api", "test2".getBytes(StandardCharsets.UTF_8)); &#125; /** * 创建节点 * @param client */ static void createZNode(CuratorFramework client) &#123; try &#123; String path = client.create().withMode(CreateMode.PERSISTENT).forPath("/curator-demo", "test".getBytes(StandardCharsets.UTF_8)); System.out.println("创建的持久化节点path是：" + path); // 检查一个节点是否存在 返回节点的stat信息 Stat stat = client.checkExists().forPath("/curator-demo"); Stat stat1 = client.checkExists().forPath("/curator-demo1"); System.out.println("节点信息" + stat); System.out.println("不存在的节点 checkExists方法返回值" + stat1); // 查询节点存储的内容 byte[] bytes = client.getData().forPath("/curator-demo"); System.out.println("节点内容：" + new String(bytes)); // 设置节点存储的内容 client.setData().forPath("/curator-demo", "testChanged".getBytes(StandardCharsets.UTF_8)); // 在子目录下创建多个临时顺序节点 for (int i =0; i &lt; 5 ; i++) &#123; client.create().withMode(CreateMode.EPHEMERAL_SEQUENTIAL).forPath("/curator-demo/child-"); &#125; // 获取所有子节点 List&lt;String&gt; strings = client.getChildren().forPath("/curator-demo"); strings.stream().forEach(System.out::println); // 删除指定的节点 级联删除 client.delete().deletingChildrenIfNeeded().forPath("/curator-demo"); &#125; catch (Exception e) &#123; &#125; &#125;&#125;]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>curator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZK简介和节点数据特性]]></title>
    <url>%2Fblog%2F2021%2F07%2F01%2FZK%E7%AE%80%E4%BB%8B%E5%92%8C%E8%8A%82%E7%82%B9%E6%95%B0%E6%8D%AE%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[ZK相关概念zk是一个分布式协调框，可以用在分布式系统中的一些数据管理问题，比如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 ZX的核心功能主要就两个：文件系统数据结构 和 监听通知机制。 1. 文件系统数据结构 每个子目录项都被称作为znode（目录节点）。主要有几种类型的znode节点： Persistent 持久化目录节点：客户端和zk断开连接之后，节点依旧存在，手动不删除永远存在。 Persistent_Sequential 持久化顺序编号目录节点：客户端与zk断开连接之后，节点仍然存在，一个有序节点被分配一个唯一的单调递增的整数。 Ephemeral 临时目录节点：客户端会话超时或者和zk断开连接之后，节点被删除。 Ephemeral——Sequential 临时顺序编号目录节点：客户端和zk断开连接之后，该节点被删除，只是zk给该节点名称进行顺序编号。 Container节点（3.5.3版本新增）容器节点：如果Container节点下面没有子节点，则Container节点在未来被Zk自动清除掉，定时任务默认60s检查一次。 TTL节点：需要配置开启，自动过期的功能不稳定。 2. 监听通知机制客户端注册监听它关心的任意节点，或者目录节点及递归子目录节点。ZK使用监听和通知模式来避免客户端轮询，当节点发生更改或者被删除，监听节点的客户端都能收到通知。 无论是对目录变化的监听还是对数据变化的监听，都是一次性的，收到一次事件之后，再次发生变更是不会触发这个客户端的。 主动推送，避免轮询 一次性。再次监听需要再次注册Watcher 顺序性：如果更新了多个Watcher，那么Watcher通知顺序和更新顺序一致。 zk配置文件中的一些配置 tickTime：客户端和服务端维持心跳的间隔，单位是毫秒 initLimit：容忍的最大心跳失活次数。 syncLimit：集群中follower服务器和leader服务器之间的请求、应答最多容忍心跳数量。 dataDir：存放数据的目录。 ZK相关命令create创建znode命令create path data。 不加参数默认是持久化节点。 -e：临时节点 -s：顺序节点 -c：容器节点 -t：给节点加过期时间，需要通过参数启动。 注意临时节点不能有子节点目录。 get查看节点数据get path -s：查看数据的同时查看节点的state信息 -w：针对当前节点数据建立一个watch监听。 ls查看目录 -R：递归看子节点 -w：建立一个watch监听，监听目录节点变更 -s：查看目录信息的同时查看state信息。 state信息可以查看节点的state信息。 cZxid：创建znode的事务id(Zxid的值) mZxid：最后修改znode的事务id。 pZxid：最后添加或删除子节点的事务id（子节点列表发生变化才会发生改变） ctime：znode的创建时间 mtime：znode的最近修改时间 dataVersion：znode的当前数据版本。 cversion：znode的子节点结果集版本。（子节点增加、删除都会影响这个版本） aclVersion：对此znode的acl版本 ephemeralOwner：znode是临时节点时，表示znode所有者的sessionId。如果znode是持久化节点不是临时节点，此值为null。 dataLength：znode数据字段的长度。 顺序节点的创建create -s /path即可创建顺序节点。 zk中的事件类型事件监听机制可以监听的事件有： None：连接建立事件 NodeCreated：节点创建 NodeDeleted：节点删除 NodeDataChanged：节点数据变化 NodeChildrenChanged：子节点列表变化 DataWatchRemoved：节点监听被移除 ChildWatchRemoved：子节点监听被移除 注意：一次操作可能触发多个事件：下边注册了对节点的递归目录节点监听，然后删除一个子节点触发了节点删除和子节点列表变化两个事件。 zk的内存数据和持久化ZK在内存中的数据 ZK的事务日志zk会把客户端的事务操作（创建节点、删除节点、变更数据等）记录在事务日志中，对应的目录是在配置文件中用dataLogDir指定，如果没有使用dataDir。 这里能看到如果每个命令都去写对应的文件，那么会频繁造成磁盘IO操作，ZK这里的事务日志不断的追加为文件开辟新的磁盘块。为了提高效率，ZK在创建事务日志文件的时候会进行文件空间的预分配。创建文件的时候，就向操作系统申请一块大一点的磁盘块。 事务日志是写多个文件的，用log.&lt;当时最大事务ID&gt;作为文件名称 ZK的数据快照因为事务日志恢复速度慢，占用空间大，所以这里是用快照一起做数据日志持久化 的。 快照是某一时间的全量内存数据快照，恢复起来很快。而事务日志数据更全。一般数据恢复时先使用快照恢复某时刻的全量数据，再通过事务日志恢复更全的增量 数据。]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis持久化方式]]></title>
    <url>%2Fblog%2F2021%2F07%2F01%2Fredis%E6%8C%81%E4%B9%85%E5%8C%96%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Redis持久化RDB快照redis内存数据库快照保存到dump.rdb二进制文件中。 RDB快照文件体积小，因为进行过压缩，是一个二进制文件，且RDB文件直接恢复即可，恢复速度很快。 在redis.conf配置文件中有关于rdb持久化方式的频率策略： 这里比如 save 60 10000代表的是60s内至少有10000个键被改动时，会自动生成并保存一个RDB快照文件。 可以手动执行RDB快照文件的生成，执行save命令或者bgsave命令即可。 save和bgsave命令简单来说，save是主线程去写RDB内存快照文件的，而bgsave是fork一个子进程，用子进程去写RDB快照文件，不阻塞主线程去执行客户端发来的命令。（当然bgsave fork子进程的时候是阻塞的）。redis在触发生成RDB时是bgsave的方式。 bgsave的写时复制机制bgsave是利用COW（copy on write）这个机制来fork一个子进程去执行生成RDB持久化文件，redis本身还可以继续执行命令。子进程运行之后，会读取redis进程的内存数据，写入RDB文件。这时如果主线程还执行写数据命令，那么也会对修改的数据复制一份副本，bgsave子进程会将这个副本数据写入RDB文件。 RDB缺点 虽然恢复速度很快，但是生成快照文件间隔之间redis挂了，那么这时没持久化到RDB文件的数据就会丢失。也就是RDB快照可能丢数据。 AOF持久化 （append only file)AOF持久化机制，将修改的每一条指令记录进文件appendonly.aof中（先写入到buffer，再定时去fsync刷入磁盘）。当数据恢复时，会按照AOF文件中的写数据命令执行，来重建数据。 这种AOF文件因为存放原生写命令，所以文件会比较大，且恢复时要执行Redis写命令，所以恢复速度比较慢。（相对于RDB快照恢复）当然，这种持久化方式数据安全性更高，因为可以设置fsync的频率，默认是每秒去将aof文件fsync到磁盘中，数据安全性比RDB更高。 AOF重写AOF文件中可能有比较多的没用指令，比如自增五次，那么可以重写为set key 5即可，redis会定期根据内存最新数据进行aof重写，来压缩下AOF文件。 AOF重写是和bgsave类似，是fork一个子进程去执行AOF文件的重写，不会阻塞其他客户端请求redis。 RDB和AOF的比较 Redis4.0之后的混合持久化方式Redis 4.0之后提供了混合持久化方式。12-- 开启redis 混合持久化方式# aof‐use‐rdb‐preamble yes 如果开启了混合持久化，那AOF在重写的时候，不再单纯去压缩无用的命令，会对此刻内存做RDB快照处理，并将此过程中新的写的命令追加到AOF文件，来生成新的AOF文件，覆盖原来的AOF文件。 这样redis在恢复时，可以先根据RDB快照文件去恢复（速度快、体积小），然后再重放后面增量的写命令（数据安全），效率和安全都得到了保障。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis持久化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo异步调用原理]]></title>
    <url>%2Fblog%2F2021%2F06%2F01%2Fdubbo%E5%BC%82%E6%AD%A5%E8%B0%83%E7%94%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[dubbo支持异步调用，在2.7版本之后引入CompletableFuture来优化了异步调用。当然在2.6版本也支持异步调用。基本原理是一样的，只不过2.7版本支持了设置CompletableFuture来回调，细节上有些差别。 异步调用基本使用2.6版本dubbo在2.6版本中的使用就是在调用完dubbo接口（dubbo接口配置了异步属性）之后，从RpcContext中获取Future，调用get()方法等待结果返回唤醒。 12345678910111213//1ReferenceConfig&lt;GreetingService&gt; referenceConfig = new ReferenceConfig&lt;GreetingService&gt;();...//2\. 设置为异步referenceConfig.setAsync(true);//3\. 直接返回 nullGreetingService greetingService = referenceConfig.get();System.out.println(greetingService.sayHello("world"));//4.等待结果java.util.concurrent.Future&lt;String&gt; future = RpcContext.getContext().getFuture();System.out.println(future.get()); 这里看到2.6版本还是用Future来实现的，调用线程阻塞式的等待，且需要手动从RpcContext中拿到调用时的Future对象。也无法聚合多个Future的结果。 2.7版本dubbo2.7版本的dubbo直接在接口上声明一个返回CompletableFuture的默认方法即可。 声明异步接口 12345678910public interface DemoService &#123; String sayHello(String name); // 声明异步接口 default CompletableFuture&lt;String&gt; sayHelloAsync(String name) &#123; return CompletableFuture.completedFuture(sayHello(name)); &#125;&#125; 直接调用异步方法即可实现异步调用 12345678910public interface DemoService &#123; String sayHello(String name); // 声明异步接口 default CompletableFuture&lt;String&gt; sayHelloAsync(String name) &#123; return CompletableFuture.completedFuture(sayHello(name)); &#125;&#125; 看到在2.7版本之后，只需要接口声明异步接口，然后调用即可完成异步调用。当然也可以从RpcContext中取出CompletableFuture对象。且因为CompletableFuture特性，可以聚合多个结果。 异步调用原理 异步调用的过程： 步骤 1 当服务消费端发起 RPC 调用时候使用的用户线程，用户线程首先使用步骤 2 创建了一个 Future 对象,然后步骤 3 会把请求转换为 IO 线程来执行,步骤 3 为异步过程，所以会马上返回，然后用户线程使用步骤 4 把其创建的 Future 对象设置到 RpcContext 中，其后用户线程就返回了。 然后步骤 5 用户线程可以在某个时间点从 RpcContext 中获取设置的 Futrue 对象，并且使用步骤 6 来等待调用结果。 步骤 7 当服务提供方返回结果后，调用方线程模型中的线程池中线程则会把结果使用步骤 8 写入到 Future,这时候用户线程就可以得到远程调用结果了。 2.6 和 2.7 版本实现异步的细节不同2.6版本12345678910111213141516171819202122232425262728293031323334353637protected Result doInvoke(final Invocation invocation) throws Throwable &#123; RpcInvocation inv = (RpcInvocation) invocation; final String methodName = RpcUtils.getMethodName(invocation); inv.setAttachment(Constants.PATH_KEY, getUrl().getPath()); inv.setAttachment(Constants.VERSION_KEY, version); ExchangeClient currentClient; if (clients.length == 1) &#123; currentClient = clients[0]; &#125; else &#123; currentClient = clients[index.getAndIncrement() % clients.length]; &#125; try &#123; boolean isAsync = RpcUtils.isAsync(getUrl(), invocation); boolean isOneway = RpcUtils.isOneway(getUrl(), invocation); int timeout = getUrl().getMethodParameter(methodName, Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT); if (isOneway) &#123; boolean isSent = getUrl().getMethodParameter(methodName, Constants.SENT_KEY, false); currentClient.send(inv, isSent); RpcContext.getContext().setFuture(null); return new RpcResult(); &#125; else if (isAsync) &#123; // 异步调用 ResponseFuture future = currentClient.request(inv, timeout); // 内部返回的future对象 设置到上下文中 RpcContext.getContext().setFuture(new FutureAdapter&lt;Object&gt;(future)); return new RpcResult(); &#125; else &#123; RpcContext.getContext().setFuture(null); // 同步调用 通过request返回的Future对象的get()方法实现阻塞发起dubbo请求的用户线程 return (Result) currentClient.request(inv, timeout).get(); &#125; &#125; catch (TimeoutException e) &#123; throw new RpcException(RpcException.TIMEOUT_EXCEPTION, "Invoke remote method timeout. method: " + invocation.getMethodName() + ", provider: " + getUrl() + ", cause: " + e.getMessage(), e); &#125; catch (RemotingException e) &#123; throw new RpcException(RpcException.NETWORK_EXCEPTION, "Failed to invoke remote method: " + invocation.getMethodName() + ", provider: " + getUrl() + ", cause: " + e.getMessage(), e); &#125; 可以从代码看到： 异步调用通过在上下文中设置进去Future对象，然后调用线程不会被阻塞返回，当需要异步调用的结果时需要get()从Future中取出结果。 而同步调用是通过返回的Future直接get()阻塞实现，注意这里不是永久阻塞，内部是一个包装了timeout时间的get()阻塞。 2.7版本2.7版本将客户端发起调用时创建的DefaultFuture继承了CompletableFuture，来更好的实现异步调用。 123456/** * DefaultFuture. dubbo2.7版本继承了CompletableFuture */public class DefaultFuture extends CompletableFuture&lt;Object&gt; &#123;// 省略代码&#125; 且在HeaderExchangeChannel的request()方法中发送完请求之后会将这个DefaultFuture封装在返回值AsyncRpcResult中，即都会向上传递这个CompletableFuture。 123456789101112131415161718192021@Overridepublic ResponseFuture request(Object request, int timeout) throws RemotingException &#123; if (closed) &#123; throw new RemotingException(this.getLocalAddress(), null, "Failed to send request " + request + ", cause: The channel " + this + " is closed!"); &#125; // create request. Request req = new Request(); req.setVersion("2.0.0"); req.setTwoWay(true); req.setData(request); // 封装DefaultFuture 返回。 DefaultFuture future = new DefaultFuture(channel, req, timeout); try &#123; // 调用底层Netty进行请求的发送 channel.send(req); &#125; catch (RemotingException e) &#123; future.cancel(); throw e; &#125; return future;&#125; 2.7版本的同步调用会用一个异步转同步的AsyncToSyncInvoker包装DubboInvoker，用来实现同步调用阻塞调用线程等待。如果调用模式是异步调用则不会被转为同步，也就是会把CompletableFuture返回给上层的Invoker，即所以异步接口定义的返回值。 1234567891011121314151617// 调用内部Invoker的invoke方法 获取结果 // 注意DubboInvoker会在发送请求时 创建一个DefaultFuture Result asyncResult = invoker.invoke(invocation); try &#123; if (InvokeMode.SYNC == ((RpcInvocation) invocation).getInvokeMode()) &#123; // 同步模式 才去异步转同步 调用future.get() 阻塞结果 异步不需要 /** * NOTICE! * must call &#123;@link java.util.concurrent.CompletableFuture#get(long, TimeUnit)&#125; because * &#123;@link java.util.concurrent.CompletableFuture#get()&#125; was proved to have serious performance drop. */ asyncResult.get(Integer.MAX_VALUE, TimeUnit.MILLISECONDS); &#125; // 省略异常代码 return asyncResult;&#125; 2.7版本在DubboInvoker这里的实现就很简单了，不再区分是否为异步调用，同步调用会直接在AsyncToSyncInvoker中阻塞等待，两种调用方式也都会把CompletableFuture写入到RpcContext中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Overrideprotected Result doInvoke(final Invocation invocation) throws Throwable &#123; RpcInvocation inv = (RpcInvocation) invocation; final String methodName = RpcUtils.getMethodName(invocation); inv.setAttachment(PATH_KEY, getUrl().getPath()); inv.setAttachment(VERSION_KEY, version); ExchangeClient currentClient; if (clients.length == 1) &#123; currentClient = clients[0]; &#125; else &#123; currentClient = clients[index.getAndIncrement() % clients.length]; &#125; try &#123; // 是否为oneWay调用？ boolean isOneway = RpcUtils.isOneway(getUrl(), invocation); // 计算配置的timeOut int timeout = calculateTimeout(invocation, methodName); if (isOneway) &#123; // 如果是OneWay调用 boolean isSent = getUrl().getMethodParameter(methodName, Constants.SENT_KEY, false); currentClient.send(inv, isSent); // 直接返回一个包装结果为null的AsyncRpcResult return AsyncRpcResult.newDefaultAsyncResult(invocation); &#125; else &#123; // 非OneWay场景 同步调用或者异步调用 ExecutorService executor = getCallbackExecutor(getUrl(), inv); CompletableFuture&lt;AppResponse&gt; appResponseFuture = // 发送请求 返回CompletableFuture // 内部会返回一个DefaultFuture（DubboInvoker --&gt; HeaderExchangeChannel） 这里Future内包装为AppResponse currentClient.request(inv, timeout, executor).thenApply(obj -&gt; (AppResponse) obj); // save for 2.6.x compatibility, for example, TraceFilter in Zipkin uses com.alibaba.xxx.FutureAdapter // futureContext写入 CompletableFuture FutureContext.getContext().setCompatibleFuture(appResponseFuture); // 封装一个AsyncRpcResult 在AsyncToSyncInvoker中 会调用其get方法阻塞等待 AsyncRpcResult result = new AsyncRpcResult(appResponseFuture, inv); result.setExecutor(executor); return result; &#125; &#125; catch (TimeoutException e) &#123; throw new RpcException(RpcException.TIMEOUT_EXCEPTION, "Invoke remote method timeout. method: " + invocation.getMethodName() + ", provider: " + getUrl() + ", cause: " + e.getMessage(), e); &#125; catch (RemotingException e) &#123; throw new RpcException(RpcException.NETWORK_EXCEPTION, "Failed to invoke remote method: " + invocation.getMethodName() + ", provider: " + getUrl() + ", cause: " + e.getMessage(), e); &#125;&#125;// 继承的父类AbstractInvoker的invoke方法@Overridepublic Result invoke(Invocation inv) throws RpcException &#123; // 省略代码... AsyncRpcResult asyncResult; try &#123; // 调用子类的doInvoke方法 比如DubboInvoker 返回AsyncRpcResult asyncResult = (AsyncRpcResult) doInvoke(invocation); &#125; catch (InvocationTargetException e) &#123; // biz exception // 省略异常处理代码.. &#125; // RpcContext中设置调用返回的futureRpcContext中设置调用返回的future 用于异步请求的结果返回 // dubbo2.7版本之后是一个CompletableFuture 客户端可以从RpcContext中取出执行CompletableFuture一系列操作。 RpcContext.getContext().setFuture(new FutureAdapter(asyncResult.getResponseFuture())); return asyncResult;&#125;]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo异步调用原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo负载均衡算法]]></title>
    <url>%2Fblog%2F2021%2F05%2F15%2Fdubbo%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[dubbo负载均衡 LoadBalance（负载均衡）的职责是将网络请求或者其他形式的负载“均摊”到不同的服务节点上，从而避免服务集群中部分节点压力过大、资源紧张，而另一部分节点比较空闲的情况。 dubbo提供了5种负载均衡实现， 一致性hash的负载均衡算法 ConsistentHashLoadBalance 基于权重随机算法 RandomLoadBalance 基于最小活跃调用数算法 LeastActiveLoadBalance 基于加权轮询算法的 RoundRobinLoadBalance 基于最短响应时间的算法 ShortestResponseLoadBalance AbstractLoadBalance12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public abstract class AbstractLoadBalance implements LoadBalance &#123; static int calculateWarmupWeight(int uptime, int warmup, int weight) &#123; // 计算权重，随着服务运行时间uptime增大，权重ww的值会慢慢接近配置值weight int ww = (int) ( uptime / ((float) warmup / weight)); return ww &lt; 1 ? 1 : (Math.min(ww, weight)); &#125; @Override public &lt;T&gt; Invoker&lt;T&gt; select(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; if (CollectionUtils.isEmpty(invokers)) &#123; return null; &#125; if (invokers.size() == 1) &#123; return invokers.get(0); &#125; // 委托给子类实现 return doSelect(invokers, url, invocation); &#125; protected abstract &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation); /** * AbstractLoadBalance提供的getWeight方法 用于计算Provider节点权重 * @param invoker the invoker * @param invocation the invocation of this invoker * @return weight */ int getWeight(Invoker&lt;?&gt; invoker, Invocation invocation) &#123; int weight; // provider节点的url URL url = invoker.getUrl(); // Multiple registry scenario, load balance among multiple registries. if (REGISTRY_SERVICE_REFERENCE_PATH.equals(url.getServiceInterface())) &#123; // 如果是RegistryService接口的话 直接获取参数上的权重 weight = url.getParameter(REGISTRY_KEY + "." + WEIGHT_KEY, DEFAULT_WEIGHT); &#125; else &#123; // 从url中获取 权重 weight = url.getMethodParameter(invocation.getMethodName(), WEIGHT_KEY, DEFAULT_WEIGHT); if (weight &gt; 0) &#123; long timestamp = invoker.getUrl().getParameter(TIMESTAMP_KEY, 0L); if (timestamp &gt; 0L) &#123; long uptime = System.currentTimeMillis() - timestamp; if (uptime &lt; 0) &#123; return 1; &#125; int warmup = invoker.getUrl().getParameter(WARMUP_KEY, DEFAULT_WARMUP); if (uptime &gt; 0 &amp;&amp; uptime &lt; warmup) &#123; // 对刚启动的provider节点 进行降权 weight = calculateWarmupWeight((int)uptime, warmup, weight); &#125; &#125; &#125; &#125; return Math.max(weight, 0); &#125;&#125; select方法委托给具体的子类实现。 提供了getWeight获取provider节点权重的方法，内部对刚启动的provider节点做了预热处理，即权重在时间内逐步接近配置的权重值。 ConsistenHashLoadBalance 一致性hash负载均衡算法一致性Hash算法简介一致性 Hash 负载均衡可以让参数相同的请求每次都路由到相同的服务节点上，这种负载均衡策略可以在某些 Provider 节点下线的时候，让这些节点上的流量平摊到其他 Provider 上，不会引起流量的剧烈波动。 与直接对节点数取模不同，为了避免因一个 Provider 节点宕机，而导致大量请求的处理节点发生变化的情况，我们可以考虑使用一致性 Hash 算法。一致性 Hash 算法的原理也是取模算法，与 Hash 取模的不同之处在于：Hash 取模是对 Provider 节点数量取模，而一致性 Hash 算法是对 2^32 取模。其对Provider地址和请求参数对2^32取模。12hash(Provider地址) % 2^32hash(请求参数) % 2^32 一致性hash可能存在流量倾斜的问题，解决方案就是为真实节点设置虚拟槽位，来通过虚拟槽位让节点在逻辑上分布的更加均匀。 dubbo中的一致性hash实现dubbo为每个dubbo接口（serviceKey做区分）的方法，缓存了一个一致性Hash选择器 ConsistentHashSelector，主要负载均衡去选出具体的Provider节点的过程就是去调用Selector的select方法。12// key：serviceKey+methodName value：ConsistentHashSelector 这个缓存来为方法调用选择一致性hash选择器 private final ConcurrentMap&lt;String, ConsistentHashSelector&lt;?&gt;&gt; selectors = new ConcurrentHashMap&lt;String, ConsistentHashSelector&lt;?&gt;&gt;(); 可以看到一致性hash负载均衡算法的doSelect的实现：12345678910111213141516171819@Override protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; String methodName = RpcUtils.getMethodName(invocation); // key是 service.methodName String key = invokers.get(0).getUrl().getServiceKey() + "." + methodName; // using the hashcode of list to compute the hash only pay attention to the elements in the list // 这是为了在invokers列表发生变化时都会重新生成ConsistentHashSelector对象 int invokersHashCode = invokers.hashCode(); // 根据key从缓存中取 ConsistentHashSelector&lt;T&gt; selector = (ConsistentHashSelector&lt;T&gt;) selectors.get(key); if (selector == null || selector.identityHashCode != invokersHashCode) &#123; // selector不存在 则去创建一个 selectors.put(key, new ConsistentHashSelector&lt;T&gt;(invokers, methodName, invokersHashCode)); selector = (ConsistentHashSelector&lt;T&gt;) selectors.get(key); &#125; // 通过selector 选择一个Invoker对象 return selector.select(invocation); &#125; 选择器ConsistentHashSelector的初始化构造函数和最终select负载均衡逻辑：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 private static final class ConsistentHashSelector&lt;T&gt; &#123; // 用于记录虚拟 Invoker 对象的 Hash 环。这里使用 TreeMap 实现 Hash 环，并将虚拟的 Invoker 对象分布在 Hash 环上。 // 虚拟槽是为了节点在分配流量时更加均匀 private final TreeMap&lt;Long, Invoker&lt;T&gt;&gt; virtualInvokers; // 虚拟Invoker个数 private final int replicaNumber; // Invoker集合的hashcode值 private final int identityHashCode; // 需要参与hash计算的参数索引 private final int[] argumentIndex; ConsistentHashSelector(List&lt;Invoker&lt;T&gt;&gt; invokers, String methodName, int identityHashCode) &#123; // 初始化virtualInvokers字段，也就是虚拟Hash槽 this.virtualInvokers = new TreeMap&lt;Long, Invoker&lt;T&gt;&gt;(); // 记录Invoker集合的hashCode，用该hashCode值来判断Provider列表是否发生了变化 this.identityHashCode = identityHashCode; URL url = invokers.get(0).getUrl(); // 获取配置的虚拟槽的个数 默认160个 this.replicaNumber = url.getMethodParameter(methodName, HASH_NODES, 160); // 获取参与Hash计算的参数下标值，默认对第一个参数进行Hash运算 String[] index = COMMA_SPLIT_PATTERN.split(url.getMethodParameter(methodName, HASH_ARGUMENTS, "0")); argumentIndex = new int[index.length]; for (int i = 0; i &lt; index.length; i++) &#123; argumentIndex[i] = Integer.parseInt(index[i]); &#125; // 构建虚拟Hash槽，默认replicaNumber=160，相当于在Hash槽上放160个槽位 for (Invoker&lt;T&gt; invoker : invokers) &#123; // 循环当前的provider对应的invokers节点 String address = invoker.getUrl().getAddress(); // 外层轮询40次，内层轮询4次，共40*4=160次，也就是同一节点虚拟出160个槽位 for (int i = 0; i &lt; replicaNumber / 4; i++) &#123; byte[] digest = md5(address + i); for (int h = 0; h &lt; 4; h++) &#123; long m = hash(digest, h); // 构建Hash槽 virtualInvokers.put(m, invoker); &#125; &#125; &#125; &#125; public Invoker&lt;T&gt; select(Invocation invocation) &#123; // 将参与一致性Hash的参数拼接到一起 String key = toKey(invocation.getArguments()); // 计算key的Hash值 byte[] digest = md5(key); // 从hash环中去匹配Invoker对象 return selectForKey(hash(digest, 0)); &#125; private Invoker&lt;T&gt; selectForKey(long hash) &#123; // 从virtualInvokers集合（TreeMap是按照Key排序的）中查找第一个节点值大于或等于传入Hash值的Invoker对象 Map.Entry&lt;Long, Invoker&lt;T&gt;&gt; entry = virtualInvokers.ceilingEntry(hash); if (entry == null) &#123; // 如果Hash值大于Hash环中的所有Invoker，则回到Hash环的开头，返回第一个Invoker对象 entry = virtualInvokers.firstEntry(); &#125; return entry.getValue(); &#125;&#125; RandomLoadBalance 加权随机负载均衡算法加权随机算法RandomLoadBalance是dubbo默认的负载均衡算法。加权随机算法简单高效，可以通过一个例子来说明加权随机算法的核心思想。 dubbo中的加权随机的实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Select one invoker between a list using a random criteria * @param invokers List of possible invokers * @param url URL * @param invocation Invocation * @param &lt;T&gt; * @return The selected invoker */ @Override protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; // Number of invokers int length = invokers.size(); // Every invoker has the same weight? boolean sameWeight = true; // the weight of every invokers Invokers对应的权重数组 int[] weights = new int[length]; // the first invoker's weight 计算第一个节点的权重 getWright是在AbstractLoadBalance中提供的公共能力 int firstWeight = getWeight(invokers.get(0), invocation); weights[0] = firstWeight; // The sum of weights 记录总权重 int totalWeight = firstWeight; for (int i = 1; i &lt; length; i++) &#123; // 获取每个节点的权重 int weight = getWeight(invokers.get(i), invocation); // save for later use weights[i] = weight; // Sum totalWeight += weight; if (sameWeight &amp;&amp; weight != firstWeight) &#123; // 标记是否为全部一样的权重 sameWeight = false; &#125; &#125; // 各个Invoker权重值不相等时，计算随机数落在哪个区间上 if (totalWeight &gt; 0 &amp;&amp; !sameWeight) &#123; // If (not every invoker has the same weight &amp; at least one invoker's weight&gt;0), select randomly based on totalWeight. // 随机获取一个[0, totalWeight)区间的内的数字 int offset = ThreadLocalRandom.current().nextInt(totalWeight); // Return a invoker based on the random value. // 循环让offset数减去Invoker的权重值，当offset小于0时，返回相应的Invoker for (int i = 0; i &lt; length; i++) &#123; offset -= weights[i]; if (offset &lt; 0) &#123; return invokers.get(i); &#125; &#125; &#125; // If all invokers have the same weight value or totalWeight=0, return evenly. // 如果总权重为0 或者 权重都相等 则随机返回一个 return invokers.get(ThreadLocalRandom.current().nextInt(length)); &#125; 其核心逻辑有三个关键点： 计算每个Invoker对应的权重及总权重值。 每个Invoker权重值不相等时，计算随机数应该落在哪个Invoker区间中，返回对应的Invoker对象。计算的逻辑其实就是生成[0, 总权重)区间内的随机数，然后循环每个节点的长度，第一个减去长度小于0的区间对应的节点即为按权重随机选择出的节点。 如果各个Invoker权重相等，随机返回一个Invoker即可。方法也是生成一个随机数，从Invoker集合中按照随机数作为下标获取。 LeastActiveLoadBalance 最小活跃调用数负载均衡算法最小活跃数负载均衡算法认为当前活跃请求数越小的Provider节点，剩余的处理能力越多，利用率越高。那么这个Provider会在单位时间内承载更多的请求，优先将这些请求分配给该Provider节点。 LeastActiveLoadBalance需要配合ActiveLimitFilter使用，ActiveLimitFilter会记录每个接口方法的活跃请求数，在进行负载均衡的时候，会根据活跃请求数最少的Invoker集合里挑选Invoker。 当然最小活跃调用数对应的Invoker可能有多个，在dubbo的实现中还像加权随机那样，去根据权重选出有一样最小活跃调用数的Invoker来调用。 dubbo实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; // Number of invokers int length = invokers.size(); // The least active value of all invokers // 最小的活跃请求数 int leastActive = -1; // The number of invokers having the same least active value (leastActive) // 记录活跃请求数最小的Invoker集合的个数 int leastCount = 0; // The index of invokers having the same least active value (leastActive) // 记录活跃请求数最小的Invoker在invokers数组中的下标位置 int[] leastIndexes = new int[length]; // the weight of every invokers // 记录活跃请求数最小的Invoker集合中，每个Invoker的权重值 int[] weights = new int[length]; // The sum of the warmup weights of all the least active invokers int totalWeight = 0; // 总权重 // The weight of the first least active invoker int firstWeight = 0; // Every least active invoker has the same weight value? boolean sameWeight = true; // 是否每个最小活跃数的Invoker有相同的权重 // Filter out all the least active invokers for (int i = 0; i &lt; length; i++) &#123; Invoker&lt;T&gt; invoker = invokers.get(i); // Get the active number of the invoker 获取活跃数 int active = RpcStatus.getStatus(invoker.getUrl(), invocation.getMethodName()).getActive(); // Get the weight of the invoker's configuration. The default value is 100. // 获取当前Invoker节点的权重 int afterWarmup = getWeight(invoker, invocation); // save for later use weights[i] = afterWarmup; // If it is the first invoker or the active number of the invoker is less than the current least active number if (leastActive == -1 || active &lt; leastActive) &#123; // 当前的Invoker是第一个活跃请求数最小的Invoker，则记录如下信息 // Reset the active number of the current invoker to the least active number // 重新记录最小活跃数 leastActive = active; // Reset the number of least active invokers // 重新记录活跃请求数最小的Invoker集合个数 leastCount = 1; // Put the first least active invoker first in leastIndexes // 重新记录最小活跃数Invoker索引下标 leastIndexes[0] = i; // Reset totalWeight 重新记录总权重 totalWeight = afterWarmup; // Record the weight the first least active invoker 记录第一个最小活跃数的权重 firstWeight = afterWarmup; // Each invoke has the same weight (only one invoker here) sameWeight = true; // If current invoker's active value equals with leaseActive, then accumulating. &#125; else if (active == leastActive) &#123; // 又找到一个最小活跃请求数的Invoker // Record the index of the least active invoker in leastIndexes order 记录该Invoker的下标 leastIndexes[leastCount++] = i; // Accumulate the total weight of the least active invoker 更新总权重 totalWeight += afterWarmup; // If every invoker has the same weight? if (sameWeight &amp;&amp; afterWarmup != firstWeight) &#123; // 更新权重值是否相等 sameWeight = false; &#125; &#125; &#125; // Choose an invoker from all the least active invokers if (leastCount == 1) &#123; // 只有一个最小活跃数 直接返回 // If we got exactly one invoker having the least active value, return this invoker directly. return invokers.get(leastIndexes[0]); &#125; if (!sameWeight &amp;&amp; totalWeight &gt; 0) &#123; // 下面按照RandomLoadBalance的逻辑，从活跃请求数最小的Invoker集合中，随机选择一个Invoker对象返回 // 即最小活跃数Invoker集合中如果权重不一致 那么按照加权随机算法去选出一个Invoker 具体@see RandomLoadBalance // If (not every invoker has the same weight &amp; at least one invoker's weight&gt;0), select randomly based on // totalWeight. int offsetWeight = ThreadLocalRandom.current().nextInt(totalWeight); // Return a invoker based on the random value. for (int i = 0; i &lt; leastCount; i++) &#123; int leastIndex = leastIndexes[i]; offsetWeight -= weights[leastIndex]; if (offsetWeight &lt; 0) &#123; return invokers.get(leastIndex); &#125; &#125; &#125; // If all invokers have the same weight value or totalWeight=0, return evenly. // 和加权随机一样 如果都有一样的权重 则从最小随机数的Invoker列表中随机选一个 return invokers.get(leastIndexes[ThreadLocalRandom.current().nextInt(leastCount)]); &#125; RoundRobinLoadBalance 加权轮询负载均衡算法轮询指的是将请求轮流分配给每个 Provider。例如，有 A、B、C 三个 Provider 节点，按照普通轮询的方式，我们会将第一个请求分配给 Provider A，将第二个请求分配给 Provider B，第三个请求分配给 Provider C，第四个请求再次分配给 Provider A……如此循环往复。 轮询是一种无状态负载均衡算法，实现简单，适用于集群中所有 Provider 节点性能相近的场景。但现实情况中就很难保证这一点了，因为很容易出现集群中性能最好和最差的 Provider 节点处理同样流量的情况，这就可能导致性能差的 Provider 节点各方面资源非常紧张，甚至无法及时响应了，但是性能好的 Provider 节点的各方面资源使用还较为空闲。这时我们可以通过加权轮询的方式，降低分配到性能较差的 Provider 节点的流量。 加权之后，分配给每个 Provider 节点的流量比会接近或等于它们的权重比。例如，Provider 节点 A、B、C 权重比为 5:1:1，那么在 7 次请求中，节点 A 将收到 5 次请求，节点 B 会收到 1 次请求，节点 C 则会收到 1 次请求。 Dubbo加权轮询负载均衡算法的实现算法总的描述：每个Provider节点（Invoker）创建两个权重，一个是配置的weight，在整个过程中不会变化，另一个是currentWeight，在负载均衡的过程中动态调整，初始值为0。 当请求进来的时候，RoundRobinLoadBalance会遍历Invoker列表，并用响应的currentWeight加上其配置的权重。遍历完成之后，再找到最大的currentWeight，将其减去权重总和，然后返回相应的Invoker对象。 新的请求来了之后会重复上面的操作。 原理示意图： 一个示例： 源码实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 和一致性hash套路一样 缓存serviceKey_methodName 作为key value也是map key是Invoker对象 value是WeightedRoundRobin对象（记录了Invoker节点的配置权重和在轮询过程中的动态当前权重） private ConcurrentMap&lt;String, ConcurrentMap&lt;String, WeightedRoundRobin&gt;&gt; methodWeightMap = new ConcurrentHashMap&lt;String, ConcurrentMap&lt;String, WeightedRoundRobin&gt;&gt;();protected &lt;T&gt; Invoker&lt;T&gt; doSelect(List&lt;Invoker&lt;T&gt;&gt; invokers, URL url, Invocation invocation) &#123; String key = invokers.get(0).getUrl().getServiceKey() + "." + invocation.getMethodName(); // 为当前服务方法在缓存中初始化 Invoker -&gt; WeightedRoundRobin 映射 存在直接获取 ConcurrentMap&lt;String, WeightedRoundRobin&gt; map = methodWeightMap.computeIfAbsent(key, k -&gt; new ConcurrentHashMap&lt;&gt;()); int totalWeight = 0; // 最大的current值 long maxCurrent = Long.MIN_VALUE; long now = System.currentTimeMillis(); Invoker&lt;T&gt; selectedInvoker = null; WeightedRoundRobin selectedWRR = null; for (Invoker&lt;T&gt; invoker : invokers) &#123; // 获取invoker的身份String 作为map的key String identifyString = invoker.getUrl().toIdentityString(); // 获取权重 （配置值） int weight = getWeight(invoker, invocation); WeightedRoundRobin weightedRoundRobin = map.computeIfAbsent(identifyString, k -&gt; &#123; // 检测当前Invoker是否有相应的WeightedRoundRobin对象，没有则进行创建 WeightedRoundRobin wrr = new WeightedRoundRobin(); wrr.setWeight(weight); return wrr; &#125;); if (weight != weightedRoundRobin.getWeight()) &#123; //weight changed 配置的权重发生变化也会重新设置 weightedRoundRobin.setWeight(weight); &#125; // current+配置的权重 long cur = weightedRoundRobin.increaseCurrent(); // 设置lastUpdate字段 weightedRoundRobin.setLastUpdate(now); // 找具有最大currentWeight的Invoker，以及Invoker对应的WeightedRoundRobin if (cur &gt; maxCurrent) &#123; maxCurrent = cur; selectedInvoker = invoker; selectedWRR = weightedRoundRobin; &#125; totalWeight += weight; // 计算权重总和 &#125; if (invokers.size() != map.size()) &#123; map.entrySet().removeIf(item -&gt; now - item.getValue().getLastUpdate() &gt; RECYCLE_PERIOD); &#125; if (selectedInvoker != null) &#123; // 用选中的最大权重的Invoker的currentWeight减去totalWeight selectedWRR.sel(totalWeight); // 返回选中的Invoker对象 return selectedInvoker; &#125; // should not happen here // 一个兜底 应该不会走到这 return invokers.get(0); &#125;]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>负载均衡算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis缓存一致性相关问题]]></title>
    <url>%2Fblog%2F2021%2F05%2F01%2Fredis%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[https://mp.weixin.qq.com/s/4W7vmICGx6a_WX701zxgPQ 缓存利用率希望缓存中是都是热点数据，而不是冷数据占用缓存中的内存。具体做法体现在访问缓存的交互上： 写请求写数据库 读请求先读缓存，如果缓存不存在，从数据库中读取并且重建缓存。 写入缓存数据库中的数据，设置失效时间。 这样缓存中不经常访问的数据，随着时间推移都会逐渐过期淘汰掉，最终缓存中保留的都是经常访问的热数据。 当然上述过程存在数据一致性问题。 缓存一致性问题缓存和数据库绝对一致是不可能实现的，只能尽可能的保证缓存和数据库的最终一致性。这里要更新数据库同时去维护缓存。 第二步失败带来的问题这里先去考虑异常场景，即第二步操作发生异常的场景。 1.先更新缓存，后更新数据库缓存更新成功，数据库更新失败，此时缓存中是最新值，数据库是旧值。后续请求在缓存未失效的场景会读到缓存中最新的值，但是如果缓存失效，会从数据库中读到旧值，那么对业务影响。 2.先更新数据库，再更新缓存更新缓存失败，数据库中是新值，数据库是旧值，那么此时后续请求读到的是旧值，对业务有影响。 读写并发带来的问题再来考虑并发带来的问题。 1. 双写不一致情况 2. 读写并发不一致问题 这里线程1库存更新之后删除缓存,线程3读取到为空读数据库，再线程2写数据库之后删更新缓存，之后线程3才去更新缓存。线程2是最新的数据，但是缓存中不一致。 更新缓存换为删除缓存从缓存利用率的角度考虑，如果在更新数据时都去更新缓存，也是不太合适的。因为每次数据发生变更之后，都无脑更新缓存，这里缓存在之后不一定就会被读到，所以导致缓存中存放了可能不被访问的冷数据，浪费缓存资源。 且重建缓存可能是一个耗时和复杂的过程，因为可能经历一系列计算才能写入缓存。造成机器的浪费。所以更新缓存应该换为删除缓存。让之后的读请求去重建缓存。 更新在写写并发下也有上面覆盖的问题。 所以写数据流程应该选择删除缓存，而不是直接更新缓存。 删除缓存中存在的问题1. 先删除缓存，后更新数据库 线程A是要去更新数据X=2 (原X=1) 线程A先删除缓存 线程B读缓存不存在，读数据库（x=1) 线程A写入数据库(X=2) 线程B重建缓存写入旧值。（X=1) 这里还是存在不一致的场景。因为先删除缓存之后，之后更新数据库的时间内缓存都是没有这个值的，所以其他线程可能因为策略去读到了数据库旧值去重建缓存。 2. 先更新数据库，后删除缓存 缓存中不存在数据，线程A读数据库旧值（X=1） 线程B更新数据库（X=2） 线程B删除缓存 线程A旧值重建缓存（X=1) 这里理论上是有不一致的问题存在的，但看下这里必须是更新数据库 + 删除缓存的时间比读书库+写缓存的时间短。这个条件发生概率还是比较低的。 所以在顺序上要采用这种方案，更新数据库比删除缓存慢，要把删除缓存放到后边执行，降低缓存中数据不存在的时间，也就降低了缓存不一致的概率。 当然这里不是不存在不一致，只不过调整了删除缓存在后，降低概率。 缓存不一致处理可以看到缓存不一致的主要原因是： 第二步缓存删除失败。 并发操作导致写入了旧数据。 保证第二步成功的方式要想保证第二步成功，可以去在应用内重试，但是重试应该解耦，也不应该在应用内执行，所以通常会引入MQ去删除缓存， 消息队列保证可靠性：写入队列的消息，成功消费之前不会丢失。 消息队列保证成功投递：消费失败MQ会有重新投递的策略。 而在项目中引入发送MQ又显得成本比较高。这里可以把MQ转移为订阅数据库binlog来完成。也就是订阅binlog变更，再操作缓存。这样对于更新数据侧： 无需自己写入MQ，binlog产生也在Mysql事务中保证 订阅方可以通过canal收到MQ投递的消息来操作删除缓存。 所以对于缓存一致性，我们可以采取先更新数据库，再删除缓存方案，对于删除缓存，可以配合消息队列和订阅binlog日志方式来做保证重试成功。 采用Canal方式还有个好处是一个公共服务，可以解耦业务操作，不需要去为每个更新数据的地方写删除缓存或者主动发MQ的操作。 延时双删除策略。 如果采用的是先删除缓存，再删除数据库，在删除缓存之后其他读线程可能读到旧值来写入到缓存中。这个是前面也讨论过的问题； 还有个场景是数据库做了主从同步和读写分离。如果主从同步完成时间大于删除缓存之后新的读请求的时间，那么读请求会读到旧的值（从库）然后写入缓存。 这两个问题其实就是写入旧值到缓存，那么可以针对这个场景去做一个延时双删除。 问题1：可以采用更换删除缓存的顺序，或者在更新完缓存之后，去延迟一定时间去再次删除缓存。 问题2：也一样，需要生成延时消息来完成对缓存的第二次删除。 这里想一下，其实在极端情况下，这个延时如果不够还是会出现不一致的场景，这里无法避免。且这个时间是一个经验值，在极端情况下可能还是不够用，还是要去从降低主从同步这方面来入手去降低这个问题的发生概率。 缓存不一致的兜底设置合理的缓存失效时间是最后的兜底方案，缓存按照规定时间失效会重建缓存，所以针对写数据频率比较高的缓存key也可以尝试去设置较短的失效时间。 或者对于可见不会更改的数据，可以在流量大的场景下设置长一点的失效时间，避免并发问题出现的概率，等流量小的时候再重建缓存。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis缓存一致性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis底层存储数据结构]]></title>
    <url>%2Fblog%2F2021%2F04%2F21%2Fredis%E5%BA%95%E5%B1%82%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[RedisKV设计原理Redis底层数据结构Redis底层有SDS（简单动态字符串）、链表（list)、字典（ht-hashtable）、跳跃表（SkipList）、整数集合（intset）、压缩链表（ziplist）、quicklist等数据结构。Redis使用这些底层数据结构实现key、value的存储。 Redis数据库的key和value是怎么存储的整个Redis中所有的key和value组成了一个全局字典，存储在RedisDb数据结构中。 RedisDb：结构包含一个字典dict，还包含一些过期键值对等dict字典值。 dict：dict是字典结构，保存键值的抽象数据结构，包含两个hash表，供平时和rehash时候使用。hash表使用链地址法来解决键冲突，对hash表进行扩容或者缩容的时候，为了服务的可用性，rehash的过程不是一次性完成的，而是渐进式的。 dictht：即字典的dict的hashtable结构，包含多个dictEntry节点的数组。 dicEntry：真正包装key、value的抽象存储结构，包含key(key都是string类型的)和value结构。value即可以通过RedisObject包装为不同的基础数据类型。 Redis中的字典是怎么扩容的。（什么是redis的渐进式rehash）存放dictEntry的数组在容量不足时，肯定需要扩容，上看可以看到dict中持有两个dictht 哈希表，即ht[0]和ht[1]。在扩容的时候，把ht[0]里的值rehash到ht[1]，这是一个渐进式hash的过程。 渐进式hash：是rehash过程并不是一次性、集中式的完成的，而是分多次、渐进式的完成的。 全部rehash完成之后，h[1]就取代h[0]存储字典中的元素。 Redis中的value存储结构value支持很多数据类型。比如String、list、hash、set、sorted set等。在redis中value（dictEntry中的val指针）是使用RedisObject表示的。 123456typedef struct redisObejct&#123; unsigned type; // value对象类型 unsigned encoding; // 编码方式 lru:LRU_BITS； // 淘汰时用 void *ptr; // 真正底层数据结构指针&#125; Redis底层数据结构常用的value基础数据类型和底层数据结构的映射关系： Redis中的字符串redis中所有的key都是用字符串存储的，而且redis没有直接使用c语言中的字符串。而是使用自己实现的简单动态字符串(sds)。 1234567891011struct sdshdr &#123; // 记录buf数组中已经使用的数量 // 等于sdc所保存字符串的长度 unsigned int len; // 记录buf数组中未使用字节的数量 unsigned int free; // 字符数组 用于保存字符串 char buf[];&#125; Redis为什么不使用c语言字符串进行存储C语言中的字符串使用了长度为N+1的字符数组来存储表示长度为N的字符串，并且字符数组最后一个元素总是\0。 存在的问题： 获取字符串的效率复杂度高：C不保存字符串数组的长度，要获取长度要遍历数组。 不能杜绝 缓冲区溢出/内存泄漏的问题：C语言一个问题可能造成缓冲区溢出的问题，从而导致内存泄漏。 C只能保存文本数据：C中的字符串不是二进制安全的，redis某些场景也要存储音视频文件，如果遇到字符 ‘\0’会截断不安全。 Redis的sds怎么解决的 在sds中增加了数组长度len，获取长度直接获取。时间复杂度O(1)。 自动扩展空间：SDS对字符串修改时，根据free能检查是否满足修改所需的空间，不够的话SDS会自动扩容，避免了C语言存在的字符串溢出的问题。 预分配内存有效降低内存分配次数，避免频繁扩容分配内存。SDS在扩容时是成倍的扩容的，这种预分配的机制避免了字符数组频繁扩容。 二进制安全：C语言字符串只能保存ASCII码，对于图片、视频等是二进制不安全的。SDS除了兼容了C语言的字符串，同时对写入、读取不做过滤、分割，是二进制安全的。 Redis中的双端链表（linkedlist）redis的数组结构的value可以通过list结构来实现。（list底层数据结构还有ziplist压缩列表实现） Redis中的list是一个双向无环链表结构，每个链表的节点用listNode结构，每个节点有前驱节点和后置节点方便遍历。 listNode：list中的一个节点，包含value（值）和前驱和后置节点。 list：list结构，包含head和tail节点，是维护节点。持有了链表头尾的指针。 Redis中的ziplist压缩列表是为了节约内存而开发的线性顺序数据结构，可以包含多个节点，每个节点可以保留一个字节数组或者整数值。 ziplist是一个为Redis专门提供的底层数据结构之一，本身可以有序也可以无序。当作为list和hash的底层实现时，节点之间没有顺序；当作为zset的底层实现时，节点之间会按照大小顺序排列。 Redis中的quicklistRedis早期版本list列表是通过压缩列表和普通的linkedlist来实现的，当元素少的时候用linkedlist，元素多的时候用ziplist。 而链表的附加空间比较高（prev和next指针），且链表内存中不连续，会有内存碎片化的问题。 在Redis 3.2版本之后对list结构的数据进行了改造，使用quicklist代替了ziplist和linkedlist。结合了空间和时间效率的考量。 quicklist其实也是由ziplist和linkedlist组成： quickList：本质还是一个linkedlist的维护节点，有头尾节点，连接的节点都是quicklistNode quickListNode：有前驱和后继节点，但真正的内容不再是list中的一个值，而是一个ziplist。 这样两种数据结构的优点都能使用到。 Redis中的字典table结构Redis中的hashtable就是dictht结构，和整体Redis kv数据存储是一个数据结构。当value是hash结构即代表value也是类型是dictht，内部每个dictEntry结构存储。 当hash结构的value元素比较少时，底层用ziplist存储（ziplist是有序的 dictht是无序的），元素数量比较大的时候会由字典来实现。由元素个数和元素大小的阈值来控制是否转变为字典结构。 Redis中的skiplist跳表Redis中实现有序集合，底层选用的是hashtable和跳表作为数据结构。 和hash基础数据结构一样，当数据量小的时候，zset也可以用ziplist数据结构实现，当数据量大之后采用hashtabel+skiplist。 当zset满足： 元素个数小于128 所有成员的长度都小于64字节这两个条件时候，会采用ziplist作为底层存储结构。 当不满足任一条件时，会采用dict(hashtable) + skiplist来实现：1234567/* zset结构体 */typedef struct zset &#123; // 字典，维护元素值和分值的映射关系 dict *dict; // 按分值对元素值排序，支持O(logN)数量级的查找操作 zskiplist *zsl;&#125; zset; hashtable：支持zrank、zscore等直接根据key查找的命令。但hash结构无序，不能很好支持快速排序。 skiplist：支持zrange、zrevrange等范围查找。跳表可以简单理解为简单的多层级排序链表，搜索的时间复杂度是O(LogN)。 skipList在Redis中的应用：在链表增加、删除节点O(1)的时间复杂度基础上，能加入多层级的索引节点，来实现近似二分查找的时间复杂度O(LogN)数据结构。 为什么选用skiplist，而不去选择其他结构 有序和无序：skipList和各种平衡树（AVL、红黑树）都是有序排列的，而hash不是有序的。在hash表上只能做单key的查找，不适合范围查询。 范围查找：在进行范围查找的时候，树结构比跳表的操作更复杂。虽然平衡树中序遍历能找到指定范围的小指，但需要再次中序遍历才能查找到范围的大值。而跳表只需要按照上层索引节点跳到第一层（原始链表）再次遍历即可。 插入和删除：平衡树可能涉及到树的转换，子树的调整，实现复杂。而skiplist多层级都是链表，只需要维护对应的指针即可。 skipList的维护过程 跳表的时间复杂度和优缺点跳表是空间换时间的一种链表基础上优化的数据结构。 链表查找的时间复杂度是O(N)，借鉴数据库索引的思想，提取出关键节点（索引），先在关键节点上查找，然后下层链表继续查找。 跳表的时间复杂度是O(logN)。空间复杂度是O(N)。在新增和删除的节点的过程中，因为要维护上层索引，且排序，时间复杂度也是logN。 优点：在数据量大且读多写少的场景下，能让链表查找的时间复杂度降低，Redis底层的链表是双向链表，很好的支持了zset的范围查询。 缺点：需要更多的空间，每次插入删除都要维护上层索引，写多的场景下效率存在折中。 Redis中的整数集合intset用于保存整数值的抽象数据结构，不会出现重复元素。 Redis中的set value默认是用值为null的字典ht实现的。（和java中HashMap、HashSet很像），当值元素都是整数时，使用此intset结构来存储value。 12345678910typedef struct intset &#123; //编码方式 uint32_t encoding; // 数组长度 uint32_t length; // 保存元素的数组 int8_t contents[]&#125;]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis底层数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis底层存储结构]]></title>
    <url>%2Fblog%2F2021%2F04%2F03%2Fredis%E5%BA%95%E5%B1%82%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis底层存储结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo超时机制]]></title>
    <url>%2Fblog%2F2021%2F04%2F02%2Fdubbo%E8%B6%85%E6%97%B6%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[预备知识 针对服务端超时，dubb之后在Filter中进行超时的日志输出，但不会阻碍执行。 针对客户端超时，会创建DefaultFuture的时候创建一个基于时间轮的定时任务，在客户端扫描，如果超过了超时时间，则构建一个超时异常让Future返回。 客户端如何知道请求失败？ RPC采用Netty作为底层通讯框架，非阻塞通信方式更高效，非阻塞的特性导致发送数据和接收数据是一个异步的过程，当存在服务端异常、网络问题时（比如超时）客户端是收不到响应的，如何判断一次RPC调用是失败的？ 误区一：Dubbo是同步调用的吗？ dubbo在通讯层面因为采用Netty是异步的，呈现给调用者错觉是内部做了阻塞等待，实现了异步转同步。 误区二：channel.writeAndFlush返回一个channelFuture，channelFuture.isSuccess能判断请求成功了。 writeAndFlush只能表示网络缓存区写入成功，不代表发送成功。 那么Dubbo客户端感知到请求失败得自己去造了。 客户端的超时感知 客户端发起一个RPC请求时，会设置一个超时时间，发起调用的时候会开启一个定时器： 如果收到正常响应，删除这个定时器。 定时器倒计时完毕，会被认为超时，在客户端构造一个失败的返回。 Dubbo中的超时判定逻辑： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public static DefaultFuture newFuture(Channel channel, Request request, int timeout, ExecutorService executor) &#123; final DefaultFuture future = new DefaultFuture(channel, request, timeout); future.setExecutor(executor); // ThreadlessExecutor needs to hold the waiting future in case of circuit return. if (executor instanceof ThreadlessExecutor) &#123; ((ThreadlessExecutor) executor).setWaitingFuture(future); &#125; // 创建一个Future时候 进行timeout check 内部是创建一个基于时间轮的定时check任务 timeoutCheck(future); return future;&#125;private static void timeoutCheck(DefaultFuture future) &#123; // 超时时间的检查任务 TimeoutCheckTask task = new TimeoutCheckTask(future.getId()); future.timeoutCheckTask = TIME_OUT_TIMER.newTimeout(task, future.getTimeout(), TimeUnit.MILLISECONDS);&#125;private static class TimeoutCheckTask implements TimerTask &#123; private final Long requestID; TimeoutCheckTask(Long requestID) &#123; this.requestID = requestID; &#125; // 超时检查的逻辑 @Override public void run(Timeout timeout) &#123; // 根据requestId从Future缓存中获取future DefaultFuture future = DefaultFuture.getFuture(requestID); if (future == null || future.isDone()) &#123; // 如果future正常返回 则说明没有超时 return; &#125; // 否则去响应一个超时 if (future.getExecutor() != null) &#123; future.getExecutor().execute(() -&gt; notifyTimeout(future)); &#125; else &#123; notifyTimeout(future); &#125; &#125; private void notifyTimeout(DefaultFuture future) &#123; // create exception response. // 客户端在超时之后创建一个超时的返回 Response timeoutResponse = new Response(future.getId()); // set timeout status. // 根据future的isSent确定状态是客户端响应超时 还是 服务端响应超时 timeoutResponse.setStatus(future.isSent() ? Response.SERVER_TIMEOUT : Response.CLIENT_TIMEOUT); timeoutResponse.setErrorMessage(future.getTimeoutMessage(true)); // handle response. // 响应超时 DefaultFuture.received(future.getChannel(), timeoutResponse, true); &#125;&#125; 超时即调用失败，一次 RPC 调用的失败，必须以客户端收到失败响应为准。 这里关键信息在dubbo发送请求相关代码中。 123456789发起RPC调用 --&gt; 客户端代理对象 --&gt; 服务目录/负载均衡选择一个可执行的Invoker --&gt;Invoker客户端Filter链 --&gt;AsyncToSyncInvoker(路由请求) --&gt; DubboInvoker(委托NettyClient发送请求) --&gt; HeaderExchangeChannel.request(发送请求 创建一个DefaltFuture) --&gt; DefaultFuture创建 时会注册一个定时任务检测超时 --&gt; channel.send 真正去发送请求 服务端的超时判断 在服务端，超时是通过Filter机制来完成的。具体代码是在TimeoutFilter中： 1234567891011121314151617181920212223242526272829303132333435@Activate(group = CommonConstants.PROVIDER)public class TimeoutFilter implements Filter, Filter.Listener &#123; private static final Logger logger = LoggerFactory.getLogger(TimeoutFilter.class); @Override public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; return invoker.invoke(invocation); &#125; // 执行完Invoker.invoke之后的回调 @Override public void onResponse(Result appResponse, Invoker&lt;?&gt; invoker, Invocation invocation) &#123; // 拿到写在上下文中的timeout统计 Object obj = RpcContext.getContext().get(TIME_COUNTDOWN_KEY); if (obj != null) &#123; TimeoutCountDown countDown = (TimeoutCountDown) obj; if (countDown.isExpired()) &#123; // 如果超时 结果调用clear()方法清除 ((AppResponse) appResponse).clear(); // clear response in case of timeout. if (logger.isWarnEnabled()) &#123; // 打印warn日志 logger.warn("invoke timed out. method: " + invocation.getMethodName() + " arguments: " + Arrays.toString(invocation.getArguments()) + " , url is " + invoker.getUrl() + ", invoke elapsed " + countDown.elapsedMillis() + " ms."); &#125; &#125; &#125; &#125; @Override public void onError(Throwable t, Invoker&lt;?&gt; invoker, Invocation invocation) &#123; &#125;&#125; Filter.Listener接口中的onResponse方法可以认为是在执行完此filter中的invoke方法之后，获取到的Result提供了回调机制，能触发正常返回和异常的两个回调函数。（代码在ProtocolFilterWrapper中） 可以看到这里就是在调用正常返回之后触发了一个检查，如果服务端执行超时，则会打印一个warn日志。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo超时</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis的单线程和io多路复用]]></title>
    <url>%2Fblog%2F2021%2F04%2F01%2Fredis%E7%9A%84%E5%8D%95%E7%BA%BF%E7%A8%8B%E5%92%8Cio%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8%2F</url>
    <content type="text"><![CDATA[redis是全部只有一个线程吗？Redis是单线程，主要是指Redis的网络IO和键值读写是一个线程完成的，这也是Redis对外提供键值存储服务的主要流程。但Redis的其他功能，比如持久化、异步删除、集群数据同步等，是由额外的线程执行的（Redis 4.0增加功能）。 所以从整体看，Redis并不只有一个线程完成了所有工作，而是主要的网络IO读写、键值服务都是一个线程完成的。比如bigkey的删除，一定是采用unlink去进行异步的删除，而不是直接del，会阻塞其他客户端的命令执行。 Redis为什么采用单线程？ 主要是一个基于数据结构的内存操作服务，如果用多线程需要考虑对数据结构并发操作的线程安全问题，比如对临界资源进行排队，反而增加了复杂度和效率。 对于Redis来说，瓶颈不在CPU，而在内存和网络带宽，采用单线程去处理读写命令即可。设计更简单。 redis单线程为什么这么快？ 基于内存进行操作，效率高于磁盘 处理命令是单线程，避免频繁的上下文切换和临界资源的锁竞争。 底层使用优化过的数据结构，比如hash表和跳表等，读取的时间复杂度都很低。 采用了非阻塞的IO多路复用模型，可以非阻塞的同时处理多个客户端的socket请求。 IO多路复用模型阻塞IO模型与阻塞点比如对于一个get请求，服务端处理的步骤如下： 监听客户端请求（bind/listen） 和客户端建立连接（accept) 从socket中读取数据（recv），java中就是IO的read操作。 解析客户端的请求(parse) 执行get操作 返回给客户端结果，向socket中写回数据（send）。 其中阻塞点在于accept建立连接和recv读取数据（联想Java中的BIO），比如服务端如果阻塞在read数据，就无法响应其他客户端的连接请求，是阻塞的。 非阻塞模式socket网络模型本身支持非阻塞模式。 针对监听套接字，可以设置非阻塞模式：当服务端调用accept()但一直未有连接到达时，Redis线程可以返回处理其他操作，不需要一直阻塞，有一套机制在监听套接字有连接时通知到Redis。 同样的针对已连接套接字，Redis调用recv()之后，没有数据到达进行读操作，也可以先返回非阻塞，利用相同机制当有数据读时通知到Redis 基于多路复用的高性能IO模型IO多路复用是指的一个线程处理多个IO流，在linux中是通过select/poll/epoll三个机制实现的。 简单来说，IO多路复用就是内核监测文件的读写事件，再通知Redis完成对应的回调操作，保证Redis的非阻塞IO能顺利完成。 多路：多个socket连接复用：复用一个线程来处理多个socket连接请求事件、 在Redis的单线程模型中，IO多路复用是在内核中，同时存在多个监听套接字和已连接套接字，内核会一直监听这些监听套接字和已连接套接字，如果有对应的请求到达，才会交给Redis的服务端线程处理，Redis服务端的单线程通过文件事件处理队列去排队，文件事件处理器去分派处理，达到多个客户端的socket请求（多路），单线程处理（复用）的高性能线程模型。 Redis IO模型中，利用内核的epoll机制，让内核去监听客户端的socket连接，Redis并不会阻塞在某个特定的accept或者read请求上，当epoll监听到对应的事件请求，会放入就绪队列中，通过基于事件的回调来通知到Redis来处理对应的请求。 Redis使用基于epoll实现的网络通信模型，可以避免Redis自身的线程去轮询FD来监听是否有请求事件发生，而是交给了内核，由epoll来完成监听。Redis基于事件回调自身只从事件队列中拿出事件来处理，及时响应客户端的请求，达到IO多路复用的效果。 Redis的IO多路复用的工作模型： Redis 6.0 中的多线程模型是怎么样的？Redis6.0的多线程只是说多线程来处理事件的读写和协议解析，真正执行对命令的还是单线程机制。默认是不开启的，这样的多线程模型为了提高IO效率，解决在网络IO上的瓶颈。 在 Redis 6.0 中新增了多线程的功能来提高 I/O 的读写性能，他的主要实现思路是将主线程的 IO 读写任务拆分给一组独立的线程去执行，这样就可以使多个 socket 的读写可以并行化了，采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求（尽量减少网络IO的时间消耗），将最耗时的Socket的读取、请求解析、写入单独外包出去，剩下的命令执行仍然由主线程串行执行并和内存的数据交互。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>reidsIO多路复用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis连接池的实现]]></title>
    <url>%2Fblog%2F2021%2F03%2F22%2Fmybatis%E8%BF%9E%E6%8E%A5%E6%B1%A0%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言最近看了Mybatis自带的数据源连接池的实现，在对比其他数据源的复杂实现，mybatis比较轻量简单的实现了数据库连接池化管理。这里写下来记录下。]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
        <tag>连接池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地消息表的一种实现方式]]></title>
    <url>%2Fblog%2F2021%2F03%2F04%2F%E6%9C%AC%E5%9C%B0%E6%B6%88%E6%81%AF%E8%A1%A8%E7%9A%84%E4%B8%80%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>分布式事务</category>
      </categories>
      <tags>
        <tag>本地消息表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现LRU算法]]></title>
    <url>%2Fblog%2F2021%2F01%2F17%2F%E5%AE%9E%E7%8E%B0LRU%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[LRU算法概述LRU：Least Recently Read （最近最少访问），是一种常见的缓存淘汰算法，当容量不够时淘汰最近最少访问的元素。 使用示例1234567891011121314151617181920212223242526/* 缓存容量为 2 */LRUCache cache = new LRUCache(2);// 你可以把 cache 理解成一个队列// 假设左边是队头，右边是队尾// 最近使用的排在队头，久未使用的排在队尾// 圆括号表示键值对 (key, val)cache.put(1, 1);// cache = [(1, 1)]cache.put(2, 2);// cache = [(2, 2), (1, 1)]cache.get(1); // 返回 1// cache = [(1, 1), (2, 2)]// 解释：因为最近访问了键 1，所以提前至队头// 返回键 1 对应的值 1cache.put(3, 3);// cache = [(3, 3), (1, 1)]// 解释：缓存容量已满，需要删除内容空出位置// 优先删除久未使用的数据，也就是队尾的数据// 然后把新的数据插入队头cache.get(2); // 返回 -1 (未找到)// cache = [(3, 3), (1, 1)]// 解释：cache 中不存在键为 2 的数据cache.put(1, 4); // cache = [(1, 4), (3, 3)]// 解释：键 1 已存在，把原始值 1 覆盖为 4// 不要忘了也要将键值对提前到队头 一些特点 查找效率高。尽量O(1)的查找效率 插入、删除要求效率高，容量满的时候要淘汰最近最少访问的元素。 元素之间是有顺序的，因为区分最近使用和久未使用的数据。 实现LRU核心数据结构 hash表：快速查找，但是各个节点的数据不是有顺序的。 链表：插入删除快，有序但是查找速度不快。 最终结合两种数据结构。哈希链表 这里的两个问题： 为什么链表是双向链表？因为在容量满足了之后需要对链表尾部的节点删除，这时候要断开上一个节点和其的链，这就需要找到上一个节点。 为什么链表中也是key和value，不能直接放value吗？因为在删除尾部节点的时候需要同时删除hash表中的元素，这时候需要根据key来删除，索引链表里也要冗余key。 java代码实现使用HashMap和LinkedList来实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class LRUCache &#123; class EasyNode &#123; Integer key; Integer value; EasyNode(Integer key, Integer value) &#123; this.key = key; this.value = value; &#125; &#125; Map&lt;Integer, EasyNode&gt; map; LinkedList&lt;EasyNode&gt; cache; int cap; public LRUCache(int capacity) &#123; map = new HashMap&lt;&gt;(capacity); cache = new LinkedList&lt;&gt;(); this.cap = capacity; &#125; public int get(int key) &#123; // 如果key在map中不存在 返回-1 if (!map.containsKey(key)) &#123; return -1; &#125; // 存在 则维护LRU中双向链表的逻辑 EasyNode node = map.get(key); // 删除再添加到队尾 cache.remove(node); cache.addLast(node); return node.value; &#125; public void put(int key, int value) &#123; // map中存在 则map更新 且将该节点放入队尾 EasyNode node = new EasyNode(key, value); if (map.containsKey(key)) &#123; cache.remove(map.get(key)); cache.addLast(node); map.put(key, node); &#125; else&#123; // 判断是否需要淘汰 if (cache.size() == cap) &#123; // 进行缓存的淘汰 EasyNode removeNode = cache.removeFirst(); map.remove(removeNode.key); &#125; // 再添加到map和队尾 map.put(key, node); cache.addLast(node); &#125; &#125;&#125;/** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */ 使用HashMap和自定义的双向链表实现 自定义双向链表的节点和双向链表1234567891011121314151617181920212223/** * 双向链表节点 */@Setter@Getterpublic class DeListNode &lt;Key, Value&gt;&#123; private Key key; private Value value; // 前驱节点 private DeListNode&lt;Key, Value&gt; prev; // 后继节点 private DeListNode&lt;Key, Value&gt; next; public DeListNode(Key key, Value value) &#123; this.key = key; this.value = value; &#125; public String toString() &#123; return String.format(" (key:%s，value:%s) ", key, value); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 简单实现双端链表 */public class DeLinkedList&lt;Key, Value&gt; &#123; // 头和尾节点 private DeListNode&lt;Key, Value&gt; head, tail; // 链表元素个数 private int size; public int size() &#123;return size;&#125; public DeLinkedList() &#123; // 初始化一个 头尾巴节点 两个节点不会在真正使用中用到 head = new DeListNode&lt;&gt;(null, null); tail = new DeListNode&lt;&gt;(null, null); // 首尾连接 head.setNext(tail); tail.setPrev(head); &#125; // 给尾巴加入新的节点 public void addLast(DeListNode&lt;Key, Value&gt; node) &#123; // 虚拟的尾巴 DeListNode&lt;Key, Value&gt; dummyTail = tail; // 真实的尾节点 DeListNode&lt;Key, Value&gt; trueTail = dummyTail.getPrev(); // 加入的节点和真实尾巴节点链接 trueTail.setNext(node); node.setPrev(trueTail); // 再链接上虚拟尾巴节点 node.setNext(dummyTail); dummyTail.setPrev(node); size ++; &#125; public String toString() &#123; if (head.getNext() == tail) &#123; return "[]"; &#125; else &#123; DeListNode current = head.getNext(); StringBuffer sb = new StringBuffer(); sb.append(" 旧 ["); while (!current.equals(tail)) &#123; sb.append(current.toString()); current = current.getNext(); &#125; sb.append("] 新"); return sb.toString(); &#125; &#125; public void remove(DeListNode&lt;Key, Value&gt; node) &#123; // 假设node 一定存在 node.getPrev().setNext(node.getNext()); node.getNext().setPrev(node.getPrev()); size -- ; &#125; // 队头移除元素 public DeListNode&lt;Key, Value&gt; removeFirst() &#123; // 没有元素不操作 if (head.getNext() == tail) &#123; return null; &#125; DeListNode&lt;Key, Value&gt; trueHead = head.getNext(); head.setNext(trueHead.getNext()); trueHead.getNext().setPrev(head); size--; return trueHead; &#125;&#125; 实现LRUCache12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class 实现LRU &#123; /** * 简单双端队列 * @param &lt;Key&gt; * @param &lt;Value&gt; */ public static class LRUCache &lt;Key, Value&gt;&#123; // 容量 private int cap; // hash表 private HashMap&lt;Key, DeListNode&lt;Key, Value&gt;&gt; map; // 双向链表缓存 private DeLinkedList&lt;Key, Value&gt; cache; // 初始化函数 public LRUCache(int cap) &#123; this.cap = cap; map = new HashMap&lt;&gt;(cap); cache = new DeLinkedList&lt;&gt;(); &#125; /** * 根据key获取value * 逻辑： * 1. 如果缓存中不存在 则返回null * 2. 如果存在，将命中的元素移动到队头（LRU的逻辑） 返回元素 * @param key * @return */ public synchronized Value get(Key key) &#123; if (!map.containsKey(key)) &#123; return null; &#125; else &#123; // 移动元素到队尾 采用先删除再插入的方式 DeListNode&lt;Key, Value&gt; keyValueDeListNode = map.get(key); cache.remove(keyValueDeListNode); cache.addLast(keyValueDeListNode); return keyValueDeListNode.getValue(); &#125; &#125; /** * 添加一个元素到LRU缓存中 * 逻辑： * 1. LRUCache中是否存在key 如果存在替换对应的node 即map.put 然后维护双向链表（插入再删除） * 2. 如果不存在 需要根基cap判断是否需要淘汰队头元素。 队列尾部添加节点 map中添加然后删除淘汰的key * * @param key * @param value */ public synchronized void put(Key key, Value value) &#123; // 构造新节点 DeListNode&lt;Key, Value&gt; node = new DeListNode&lt;&gt;(key, value); if (map.containsKey(key))&#123; // 存在key 则去替换map中的节点 且维护双端队列（删除再添加） cache.remove(map.get(key)); cache.addLast(node); map.put(key, node); &#125; else &#123; // 不存在要去判断是否触发淘汰 if (cap == cache.size()) &#123; // 双端队列队头淘汰（LRU逻辑） DeListNode&lt;Key, Value&gt; first = cache.removeFirst(); // map中remove map.remove(first.getKey()); &#125; // 直接添加到尾部即可 cache.addLast(node); // map中添加 map.put(key, node); &#125; &#125; public void printf() &#123; System.out.println("cachhe 队列" + cache.toString()); &#125; &#125; public static void main(String[] args) &#123; // 初始化为2的lruCache LRUCache&lt;Integer, Integer&gt; lruCache = new LRUCache&lt;&gt;(2); lruCache.put(1, 1); lruCache.put(2, 2); // 新 [(2,2), (1,1)] 旧 lruCache.printf(); Integer integer = lruCache.get(1); // 新 [(1,1), (2,2)] 旧 lruCache.printf(); lruCache.put(2,3); // 新 [(2,3), (1,1)] 旧 lruCache.printf(); // 再添加 淘汰的是1 lruCache.put(4, 4); // 新 [(4,4), (2,3)] 旧 lruCache.printf(); &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>LRU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis-spring-boot-starter自动装配实现]]></title>
    <url>%2Fblog%2F2020%2F12%2F18%2Fmybatis-spring-boot-starter%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Mybatis-spring-boot-starter的入口 会在springboot启动的时候加载autoconfigure模块定义的自动装配类：MybatisAutoConfiguration MybatisAutoConfiguration中装配Mybatis的逻辑SqlSessionFactory的构造12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Bean @ConditionalOnMissingBean public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception &#123; // 通过SqlSessionFactoryBean来实现 SqlSesionFactory的初始化 SqlSessionFactoryBean factory = new SqlSessionFactoryBean(); factory.setDataSource(dataSource); factory.setVfs(SpringBootVFS.class); if (StringUtils.hasText(this.properties.getConfigLocation())) &#123; factory.setConfigLocation(this.resourceLoader.getResource(this.properties.getConfigLocation())); &#125; // 给factory设置Mybatis的全局配置对象Configuration applyConfiguration(factory); if (this.properties.getConfigurationProperties() != null) &#123; factory.setConfigurationProperties(this.properties.getConfigurationProperties()); &#125; if (!ObjectUtils.isEmpty(this.interceptors)) &#123; factory.setPlugins(this.interceptors); &#125; if (this.databaseIdProvider != null) &#123; factory.setDatabaseIdProvider(this.databaseIdProvider); &#125; if (StringUtils.hasLength(this.properties.getTypeAliasesPackage())) &#123; factory.setTypeAliasesPackage(this.properties.getTypeAliasesPackage()); &#125; if (this.properties.getTypeAliasesSuperType() != null) &#123; factory.setTypeAliasesSuperType(this.properties.getTypeAliasesSuperType()); &#125; if (StringUtils.hasLength(this.properties.getTypeHandlersPackage())) &#123; factory.setTypeHandlersPackage(this.properties.getTypeHandlersPackage()); &#125; if (!ObjectUtils.isEmpty(this.typeHandlers)) &#123; factory.setTypeHandlers(this.typeHandlers); &#125; if (!ObjectUtils.isEmpty(this.properties.resolveMapperLocations())) &#123; factory.setMapperLocations(this.properties.resolveMapperLocations()); &#125; Set&lt;String&gt; factoryPropertyNames = Stream .of(new BeanWrapperImpl(SqlSessionFactoryBean.class).getPropertyDescriptors()).map(PropertyDescriptor::getName) .collect(Collectors.toSet()); Class&lt;? extends LanguageDriver&gt; defaultLanguageDriver = this.properties.getDefaultScriptingLanguageDriver(); if (factoryPropertyNames.contains("scriptingLanguageDrivers") &amp;&amp; !ObjectUtils.isEmpty(this.languageDrivers)) &#123; // Need to mybatis-spring 2.0.2+ factory.setScriptingLanguageDrivers(this.languageDrivers); if (defaultLanguageDriver == null &amp;&amp; this.languageDrivers.length == 1) &#123; defaultLanguageDriver = this.languageDrivers[0].getClass(); &#125; &#125; if (factoryPropertyNames.contains("defaultScriptingLanguageDriver")) &#123; // Need to mybatis-spring 2.0.2+ factory.setDefaultScriptingLanguageDriver(defaultLanguageDriver); &#125; return factory.getObject(); &#125; SqlSessionFactory的实例化是通过SqlSessionFactoryBean.getObject()实现的，该类会被注入DataSource对象（负责管理数据库连接池，Session指的是一次会话，而这个会话是在DataSource提供的Connection上进行的）。 在factory.getObject()中，最核心还是调用了buildSqlSessionFactory 去创建了sqlSessionFactory实例。12345678910111213141516171819202122232425262728293031protected SqlSessionFactory buildSqlSessionFactory() throws IOException &#123; Configuration configuration; XMLConfigBuilder xmlConfigBuilder = null; if (this.configLocation != null) &#123; // 创建XMLConfigBuilder对象，读取指定的配置文件 xmlConfigBuilder = new XMLConfigBuilder(this.configLocation.getInputStream(), null, this.configurationProperties); configuration = xmlConfigBuilder.getConfiguration(); &#125; else &#123; // 其他方式初始化Configuration全局配置对象 &#125; // 初始化MyBatis的相关配置和对象，其中包括： // 扫描typeAliasesPackage配置指定的包，并为其中的类注册别名 // 注册plugins集合中指定的插件 // 扫描typeHandlersPackage指定的包，并注册其中的TypeHandler // 配置缓存、配置数据源、设置Environment等一系列操作 // ...... 省略配置代码 if (this.transactionFactory == null) &#123; // 默认使用的事务工厂类 this.transactionFactory = new SpringManagedTransactionFactory(); &#125; // 根据mapperLocations配置，加载Mapper.xml映射配置文件以及对应的Mapper接口 for (Resource mapperLocation : this.mapperLocations) &#123; XMLMapperBuilder xmlMapperBuilder = new XMLMapperBuilder(...); xmlMapperBuilder.parse(); &#125; // 最后根据前面创建的Configuration全局配置对象创建SqlSessionFactory对象 return this.sqlSessionFactoryBuilder.build(configuration);&#125; SqlSessionFactory.getObject()方法里会根据我们mybatis相关配置（比如上面的mybatis.mapper-locations配置）找到并解析我们的mapper文件，解析出sql与dao方法里的映射、ResultMap与具体实体类的映射等，并放到SqlSessionFactory的Configuration中缓存下来，在后续调用过程中会通过这些信息来匹配jdbc操作。这个过程中会生成Mapper的代理类，执行sql时会走代理拦截方法执行，则不需要实现类就可以直接调用到具体的sql。 SqlSessionTemplate如果使用手写java代码的方式去操作数据库，则使用的为sqlSessionTemplate，SqlSessionTemplate 是线程安全的，可以在多个线程之间共享使用。SqlSessionTemplate 内部持有一个 SqlSession 的代理对象（sqlSessionProxy 字段），这个代理对象是通过 JDK 动态代理方式生成的；使用的 InvocationHandler 接口是 SqlSessionInterceptor，其 invoke() 方法会拦截 SqlSession 的全部方法，并检测当前事务是否由 Spring 管理。 在MybatisAutoConfiguration自动配置中也去生成了SqlSessionTemplate这个bean对象。 1234567891011@Bean @ConditionalOnMissingBean public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) &#123; ExecutorType executorType = this.properties.getExecutorType(); if (executorType != null) &#123; // 返回SqlSessionTemplate bean 用于手写数据库的CRUD操作。 return new SqlSessionTemplate(sqlSessionFactory, executorType); &#125; else &#123; return new SqlSessionTemplate(sqlSessionFactory); &#125; &#125; MapperFactoryBean和MapperScannerConfigurer在MybatisAutoConfiguration中注册了AutoConfiguredMapperScannerRegistrar，其实ImportBeanDefinitionRegistrar的实现类，Spring中可以通过 @Import 注解导入的 ImportBeanDefinitionRegistrar 实现类往 BeanDefinitionRegistry 注册 BeanDefinition。这里其实就是去注册了MapperScannerConfigurer 这个BeanDefinitionRegistryPostProcessor的实现。 BeanDefinitionRegistryPostProcessor是一个特殊的接口，继承了BeanFactoryPostProcessor接口，提供了注册bean定义的能力，这里MapperScannerConfigurer扫描所有的@Mapper注解的Maapper类，在扩展点方法中去注册bean定义都为MapperFactoryBean接口，这些Bean在实例化时会因为是FactoryBean，而调用getObject方法寻找Mapper接口的动态代理缓存。 12345678910111213141516171819202122232425262728public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) &#123; if (this.processPropertyPlaceHolders) &#123; processPropertyPlaceHolders(); &#125; ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry); scanner.setAddToConfig(this.addToConfig); scanner.setAnnotationClass(this.annotationClass); scanner.setMarkerInterface(this.markerInterface); scanner.setSqlSessionFactory(this.sqlSessionFactory); scanner.setSqlSessionTemplate(this.sqlSessionTemplate); scanner.setSqlSessionFactoryBeanName(this.sqlSessionFactoryBeanName); scanner.setSqlSessionTemplateBeanName(this.sqlSessionTemplateBeanName); scanner.setResourceLoader(this.applicationContext); scanner.setBeanNameGenerator(this.nameGenerator); // 设置扫描的Bean都是MapperFactoryBean接口的子类 scanner.setMapperFactoryBeanClass(this.mapperFactoryBeanClass); if (StringUtils.hasText(lazyInitialization)) &#123; scanner.setLazyInitialization(Boolean.valueOf(lazyInitialization)); &#125; if (StringUtils.hasText(defaultScope)) &#123; scanner.setDefaultScope(defaultScope); &#125; scanner.registerFilters(); // scan扫描@Mapper注解的接口 注册bean定义。 scanner.scan( StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS)); &#125;s MapperFactoryBean的getObject()方法是从Configuration配置对象中寻找对应Mapper接口的代理类缓存，没有的话会创建。 这个SpringBean在注入的时候就是对应的代理类了。内部会转发到sqlSession、SqlExecutor去执行sql。 12345@Override public T getObject() throws Exception &#123; // getSqlSession.getMapper 会从全局唯一的配置对象中寻找接口类对应的Mapper代理 return getSqlSession().getMapper(this.mapperInterface); &#125;s]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
        <tag>spring boot starter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[库存设计的场景原则]]></title>
    <url>%2Fblog%2F2020%2F11%2F22%2F%E5%BA%93%E5%AD%98%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%9C%BA%E6%99%AF%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[基本原则对于秒杀商品的库存设计应该遵守的几个原则： 渠道隔离：秒杀活动中使用的库存应当按照渠道进行隔离，这样既能保证不对正常售卖渠道产生影响，又有利于精细化运作库存管理。比如，根据用户群体或平台支持的力度不同，我们可能需要在不同渠道透出不同的库存数量，并且它们与正常售卖渠道分离，这种场景下就需要渠道隔离； 防止超卖：业务来说，超卖可能意味着资损；对技术来说，超卖意味着架构的失败。试想，原价999元商品的秒杀价为599，库存100件却卖出了10000件，那么我们就会面临严重的客诉或资损； 防止重复扣减：与超卖相对的是没有卖出去，其同样不可小觑。比如，10000件的库存仅有10人成单，库存明明还在却显示已经售罄，活动未到达预期，前期准备和推广的资金投入都打了水漂，而由系统设计缺陷造成的重复扣减 就会导致这种糟糕的情况发生；也就是同时也防止少卖。 高性能：在前面的文章我们谈到了如何通过缓存提高秒杀架构中的”读“性能，殊不知”写“性能也是秒杀架构的重要指标之一。举例来说，10000比订单，每秒写入300单和每秒写入3000单在用户体验上有着显著的差异。 常用库存扣减方案基于数据库的库存扣减方案顾名思义，基于数据库的库存扣减方案指的是利用数据库的特性在数据库层面完成库存扣减。这种方式实现起来比较简单，对于并发量低或库存低的场景，推荐使用这种方案。所谓库存低，指的是当库存很少时，我们只要把大流量拦截在应用之外或数据库之外即可，确保数据库的并发量处于低位。否则，在高并发写入的场景下，这种方案是不合适的 基于缓存的库存扣减方案既然数据库方案无法满足高并发写入场景，那么缓存是否可以呢？答案是看情况。对于Redis这种缓存，虽然速度较快，但是存在数据丢失的可能，这是我们无法接受的。所以，当我们决定使用基于缓存的扣减方案时，我们就必须考虑如何保证缓存不丢失的稳定，比如使用LevelDB等。另外，相较于数据库方案，缓存方案需要更高的实现复杂度。 分库分表库存扣减方案解决数据库高并发的写入问题，除了使用缓存方案外，还可以采用分库分表的库存扣减方案，将库存分散到不同的库中。比如，单台数据库每秒能支撑300的库存更新，那么10台数据库即可支撑3000的并发写入。 当然，相较于前两种方案，虽然分库分表的优势明显，但具有更高的复杂性和实现成本。 常用库存扣减方式 下单扣库存：优势在于简单，链路短，性能好，缺点在于容易被恶意下单。活动刚开始，可能即被恶意下单清空库存； 支付扣库存：优势在于可以控制恶意下单，最后得到库存的都是有效订单。当然，其缺点也较为明显，无法控制下单人数，用户需要在支付时再次确认库存； 下单预扣库存，超时取消：相较于前两种方式，这种方式较为折中且有效，对于正常下单的用户来说抢单即是得到，对于恶意下单的来说，占据的库存会超时自动释放。]]></content>
      <categories>
        <category>系统设计</category>
      </categories>
      <tags>
        <tag>库存扣减</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RR隔离级别下的gap和插入意向锁死锁分析]]></title>
    <url>%2Fblog%2F2020%2F09%2F18%2FRR%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E4%B8%8B%E7%9A%84gap%E5%92%8C%E6%8F%92%E5%85%A5%E6%84%8F%E5%90%91%E9%94%81%E6%AD%BB%E9%94%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[死锁日志的查看https://segmentfault.com/a/1190000018730103 sql1show engine innodb status; 记录锁，间隙锁，Next-key 锁和插入意向锁。这四种锁对应的死锁如下：记录锁（LOCK_REC_NOT_GAP）: lock_mode X locks rec but not gap间隙锁（LOCK_GAP）: lock_mode X locks gap before recNext-key 锁（LOCK_ORNIDARY）: lock_mode X插入意向锁（LOCK_INSERT_INTENTION）: lock_mode X locks gap before rec insert intention 死锁案例https://my.oschina.net/hebaodan/blog/1835966https://my.oschina.net/hebaodan/blog/3033276 gap锁 + 并发insert死锁表及Sql 分析 锁冲突矩阵 并发delete + insert唯一键冲突死锁表及场景sql： 分析 死锁日志1234567891011121314151617181920212223242526272829303132333435363738394041*** (1) TRANSACTION:TRANSACTION 11074, ACTIVE 11 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s)MySQL thread id 10, OS thread handle 123145442168832, query id 160 localhost 127.0.0.1 root updating// T2的SQLdelete from t4 where a = 1*** (1) WAITING FOR THIS LOCK TO BE GRANTED: // 等待的锁RECORD LOCKS space id 68 page no 3 n bits 72 index PRIMARY of table// 可以看到这里是阻塞等待X行锁`aliyun`.`t4` trx id 11074 lock_mode X locks rec but not gap waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 32 0: len 4; hex 80000001; asc ;; 1: len 6; hex 000000002b41; asc +A;; 2: len 7; hex 2e000001dc13c4; asc . ;;*** (2) TRANSACTION:TRANSACTION 11073, ACTIVE 40 sec insertingmysql tables in use 1, locked 13 lock struct(s), heap size 1136, 2 row lock(s), undo log entries 1MySQL thread id 9, OS thread handle 123145442725888, query id 161 localhost 127.0.0.1 root update// 事务T1的SQLinsert into t4 values(1)*** (2) HOLDS THE LOCK(S):// 持有的锁RECORD LOCKS space id 68 page no 3 n bits 72 index PRIMARY of table`aliyun`.`t4` trx id 11073 lock_mode X locks rec but not gapRecord lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 32 0: len 4; hex 80000001; asc ;; 1: len 6; hex 000000002b41; asc +A;; 2: len 7; hex 2e000001dc13c4; asc . ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED: // 等待的锁RECORD LOCKS space id 68 page no 3 n bits 72 index PRIMARY of table// 这里看到事务T1 S-Next锁 等待行锁`aliyun`.`t4` trx id 11073 lock mode S waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 3; compact format; info bits 32 0: len 4; hex 80000001; asc ;; 1: len 6; hex 000000002b41; asc +A;; 2: len 7; hex 2e000001dc13c4; asc . ;;*** WE ROLL BACK TRANSACTION (1) sql加锁分析https://www.aneasystone.com/archives/2017/12/solving-dead-locks-three.html (这是一个系列) 基本加锁规则 常见语句的加锁 select 快照读，在RC和RR下不加锁 select … lock in share mode为当前读，加S锁 select … for update为当前读，加X锁 dml语句，当前读，加X锁 ddl语句，加标记所，隐式提交，不能回滚。 表锁 表锁（S和X锁） 意向锁（IS锁和IX锁） 自增锁（一般看不见 只有在innodb_autoinc_lock_mode=0或者bulk inserts时才可能有） 行锁 记录锁（S和X锁） 间隙锁（S和X锁） Next-Key锁（S和X锁） 插入意向锁 行锁分析 行锁是加在索引上的，最终都会落在聚簇索引上。 加行锁是一行行加的 锁冲突 不同隔离级别下的锁 select快照读在Serializable隔离级别下为当前读，加S锁。 RC下没有间隙锁和Next-Key锁（唯一索引情况下有特殊，purge线程+记录有锁唯一索引，会加S Next-Key锁） 对于insert操作如果没有这两个问题（1）为了防止幻读，如果记录之间加有 GAP 锁，此时不能 INSERT；（插入意向锁解决）（2）如果 INSERT 的记录和已有记录造成唯一键冲突，此时不能 INSERT；（对已存在唯一记录加S Next锁） insert一开始只有隐式锁，除非隐式锁转换为显式锁： InnoDb 在插入记录时，是不加锁的。如果事务 A 插入记录且未提交。如果这时事务 B 尝试对这条记录加锁，事务 B 会先去判断记录上保存的事务 id 是否活跃，如果活跃的话，那么就帮助事务 A 去建立一个锁对象，然后自身进入等待事务 A 状态，这就是所谓的隐式锁转换为显式锁。还比如对于辅助索引也是隐式锁。 insert 对唯一索引的加锁逻辑： 1.先做UK冲突检测，如果存在目标行，先对目标行加S Next Key Lock（该记录在等待期间被其他事务删除，此锁被同时删除） 2.如果1成功，对对应行加插入意向锁 3.如果2成功，插入记录，并对记录加X + 行锁（有可能是隐式锁）]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>死锁</tag>
        <tag>GAP锁</tag>
        <tag>插入意向锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo集群容错Cluster]]></title>
    <url>%2Fblog%2F2020%2F07%2F16%2Fdubbo%E9%9B%86%E7%BE%A4%E5%AE%B9%E9%94%99Cluster%2F</url>
    <content type="text"><![CDATA[Cluster接口Cluster接口提供了集群容错的功能。在 Dubbo 中，通过 Cluster 这个接口把一组可供调用的 Provider 信息组合成为一个统一的 Invoker 供调用方进行调用。经过 Router 过滤、LoadBalance 选址之后，选中一个具体 Provider 进行调用，如果调用失败，则会按照集群的容错策略进行容错处理。 Cluster接口的工作流程Cluster接口工作流程分为两步： 在Consumer进行服务引用的时候，会创建对应Cluster实现类的集群容错策略对应的ClusterInvoker。也就是说在Cluster接口实现中，都会创建对应的Invoker对象，这些都继承自AbstractClusterInvoker抽象类。 调用时使用ClusterInvoker实例，内部会实现集群容错的逻辑，且会依赖Directory、Router、LoadBalance等组件得到最终要调用的Invoker对象。 也就是说，因为Cluster在服务引用的过程中将多个Invoker伪装为带有集群容错的ClusterInvoker实现，所以在调用的时候可以在对应集群容错逻辑下，再对Invokers进行服务目录、服务路由过滤、负载均衡选址选出真正调用的Invoker发起远程调用逻辑。 常见的几种集群容错的方式 Dubbo中的AbstractClusterInvoker了解了 Cluster Invoker 的继承关系之后，我们首先来看 AbstractClusterInvoker，它有两点核心功能：一个是实现的 Invoker 接口，对 Invoker.invoke() 方法进行通用的抽象实现；另一个是实现通用的负载均衡算法。 在 AbstractClusterInvoker.invoke() 方法中，会通过 Directory 获取 Invoker 列表，然后通过 SPI 初始化 LoadBalance，最后调用 doInvoke() 方法执行子类的逻辑。在 Directory.list() 方法返回 Invoker 集合之前，已经使用 Router 进行了一次筛选。 123456789101112131415161718192021@Override public Result invoke(final Invocation invocation) throws RpcException &#123; // 检查当前Invoker是否已销毁 checkWhetherDestroyed(); // binding attachments into invocation. // 将RpcContext中的attachment添加到Invocation中 Map&lt;String, Object&gt; contextAttachments = RpcContext.getContext().getObjectAttachments(); if (contextAttachments != null &amp;&amp; contextAttachments.size() != 0) &#123; ((RpcInvocation) invocation).addObjectAttachments(contextAttachments); &#125; // 通过Directory获取Invoker对象列表 RegistryDirectory内部会调用Router完成服务路由的功能 List&lt;Invoker&lt;T&gt;&gt; invokers = list(invocation); // 先去服务目录选择 // 再通过SPI加载对应的负载均衡实现 LoadBalance loadbalance = initLoadBalance(invokers, invocation);// 再负载均衡 RpcUtils.attachInvocationIdIfAsync(getUrl(), invocation); // 委托给具体的集群容错子类 实现调用 return doInvoke(invocation, invokers, loadbalance); &#125; 在子类实现的doInvoke方法中，调用了在抽象类中提供的select()方法，来完成负载均衡。这里没有去直接委托给LoadBalance去做负载均衡，而是在 select() 方法中会根据配置决定是否开启粘滞连接特性，如果开启了，则需要将上次使用的 Invoker 缓存起来，只要 Provider 节点可用就直接调用，不会再进行负载均衡。这里知道即可。1234567891011121314151617181920212223242526272829303132protected Invoker&lt;T&gt; select(LoadBalance loadbalance, Invocation invocation, List&lt;Invoker&lt;T&gt;&gt; invokers, List&lt;Invoker&lt;T&gt;&gt; selected) throws RpcException &#123; if (CollectionUtils.isEmpty(invokers)) &#123; return null; &#125; // 方法名称 String methodName = invocation == null ? StringUtils.EMPTY_STRING : invocation.getMethodName(); // 获取sticky配置，sticky表示粘滞连接，所谓粘滞连接是指Consumer会尽可能地调用同一个Provider节点，除非这个Provider无法提供服务 boolean sticky = invokers.get(0).getUrl() .getMethodParameter(methodName, CLUSTER_STICKY_KEY, DEFAULT_CLUSTER_STICKY); //ignore overloaded method if (stickyInvoker != null &amp;&amp; !invokers.contains(stickyInvoker)) &#123; stickyInvoker = null; &#125; //ignore concurrency problem if (sticky &amp;&amp; stickyInvoker != null &amp;&amp; (selected == null || !selected.contains(stickyInvoker))) &#123; if (availablecheck &amp;&amp; stickyInvoker.isAvailable()) &#123; return stickyInvoker; &#125; &#125; // doSelect选择新的Invoker Invoker&lt;T&gt; invoker = doSelect(loadbalance, invocation, invokers, selected); if (sticky) &#123; stickyInvoker = invoker; &#125; return invoker; &#125; Dubbo中的AbstractCluster常用的 ClusterInvoker 实现都继承了 AbstractClusterInvoker 类型，对应的 Cluster 扩展实现都继承了 AbstractCluster 抽象类。AbstractCluster 抽象类的核心逻辑是在 ClusterInvoker 外层包装一层 ClusterInterceptor，从而实现类似切面的效果。 1234567891011121314151617181920212223242526@Override public &lt;T&gt; Invoker&lt;T&gt; join(Directory&lt;T&gt; directory) throws RpcException &#123; // 2. 外层对ClusterInvoker进行切面包装 返回InterceptorInvokerNode （继承自AbstractClusterInvoker接口） return buildClusterInterceptors( // 1. 先doJoin获取最终要调用的Invoker对象 比如直接返回一个FailOverClusterInvoker doJoin(directory), directory.getUrl().getParameter(REFERENCE_INTERCEPTOR_KEY) ); &#125; // buildClusterInterceptors方法 private &lt;T&gt; Invoker&lt;T&gt; buildClusterInterceptors(AbstractClusterInvoker&lt;T&gt; clusterInvoker, String key) &#123; AbstractClusterInvoker&lt;T&gt; last = clusterInvoker; // ClusterInterceptor是SPI加载自动激活的扩展实现 List&lt;ClusterInterceptor&gt; interceptors = ExtensionLoader.getExtensionLoader(ClusterInterceptor.class).getActivateExtension(clusterInvoker.getUrl(), key); if (!interceptors.isEmpty()) &#123; for (int i = interceptors.size() - 1; i &gt;= 0; i--) &#123; // 将InterceptorInvokerNode收尾连接到一起，形成调用链 final ClusterInterceptor interceptor = interceptors.get(i); final AbstractClusterInvoker&lt;T&gt; next = last; last = new InterceptorInvokerNode&lt;&gt;(clusterInvoker, interceptor, next); &#125; &#125; return last; &#125; Dubbo中具体的集群容错实现FailOverCluster默认是FailOverCluster，可以看到其实现的doJoin方法就是创建一个对应的FailOverClusterInvoker对象并返回。12345@Override public &lt;T&gt; AbstractClusterInvoker&lt;T&gt; doJoin(Directory&lt;T&gt; directory) throws RpcException &#123; // 直接委托给FailoverClusterInvoker return new FailoverClusterInvoker&lt;&gt;(directory); &#125; 而在FailOverClutserInvoker中实现的doList方法中，有对失败重试容错的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475@Override @SuppressWarnings(&#123;"unchecked", "rawtypes"&#125;) public Result doInvoke(Invocation invocation, final List&lt;Invoker&lt;T&gt;&gt; invokers, LoadBalance loadbalance) throws RpcException &#123; List&lt;Invoker&lt;T&gt;&gt; copyInvokers = invokers; checkInvokers(copyInvokers, invocation); String methodName = RpcUtils.getMethodName(invocation); // 获取重试次数 int len = getUrl().getMethodParameter(methodName, RETRIES_KEY, DEFAULT_RETRIES) + 1; if (len &lt;= 0) &#123; // 默认重试次数是1 len = 1; &#125; // retry loop. 循环来实现重试 // 最近的一次异常 RpcException le = null; // last exception. // 记录已经尝试调用过的Invoker List&lt;Invoker&lt;T&gt;&gt; invoked = new ArrayList&lt;Invoker&lt;T&gt;&gt;(copyInvokers.size()); // invoked invokers. // 记录执行过的provider地址 Set&lt;String&gt; providers = new HashSet&lt;String&gt;(len); for (int i = 0; i &lt; len; i++) &#123; //Reselect before retry to avoid a change of candidate `invokers`. //NOTE: if `invokers` changed, then `invoked` also lose accuracy. if (i &gt; 0) &#123; // 第二次进来就是重试的逻辑 需要再次走一遍 服务目录list（内部会走route服务路由）拉取最新的Invoker列表并检查 checkWhetherDestroyed(); // list重新从服务目录中获取集合 copyInvokers = list(invocation); // check again checkInvokers(copyInvokers, invocation); &#125; // AbstractClusterInvoker.select() 有自己的粘连逻辑（优先调用一个provider） 也会根据负载均衡算法来选出provider Invoker&lt;T&gt; invoker = select(loadbalance, invocation, copyInvokers, invoked); invoked.add(invoker); RpcContext.getContext().setInvokers((List) invoked); try &#123; // 进行调用 Result result = invoker.invoke(invocation); if (le != null &amp;&amp; logger.isWarnEnabled()) &#123; logger.warn("Although retry the method " + methodName + " in the service " + getInterface().getName() + " was successful by the provider " + invoker.getUrl().getAddress() + ", but there have been failed providers " + providers + " (" + providers.size() + "/" + copyInvokers.size() + ") from the registry " + directory.getUrl().getAddress() + " on the consumer " + NetUtils.getLocalHost() + " using the dubbo version " + Version.getVersion() + ". Last error is: " + le.getMessage(), le); &#125; return result; &#125; catch (RpcException e) &#123; if (e.isBiz()) &#123; // biz exception. dubbo内部的biz异常 throw e; &#125; // 记录异常 le = e; &#125; catch (Throwable e) &#123; // 封装为rpc异常 然后记录下次重试 le = new RpcException(e.getMessage(), e); &#125; finally &#123; // 记录已经尝试过的provider地址 providers.add(invoker.getUrl().getAddress()); &#125; &#125; // 超过重试次数 抛出异常 throw new RpcException(le.getCode(), "Failed to invoke the method " + methodName + " in the service " + getInterface().getName() + ". Tried " + len + " times of the providers " + providers + " (" + providers.size() + "/" + copyInvokers.size() + ") from the registry " + directory.getUrl().getAddress() + " on the consumer " + NetUtils.getLocalHost() + " using the dubbo version " + Version.getVersion() + ". Last error is: " + le.getMessage(), le.getCause() != null ? le.getCause() : le); &#125;]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>集群容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[duubo调用经历源码路径]]></title>
    <url>%2Fblog%2F2020%2F06%2F21%2Fduubo%E8%B0%83%E7%94%A8%E7%BB%8F%E5%8E%86%E6%BA%90%E7%A0%81%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo调用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[duubo的proxy代理]]></title>
    <url>%2Fblog%2F2020%2F06%2F21%2Fduubo%E7%9A%84proxy%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[Proxy层 proxy层是dubbo中的动态代理层，主要是为了屏蔽dubbo内部Cluster、Protocol概念设计，能让业务层像调用本地方法一样去发起RPC调用。 在Consumer层进行引用服务时，通过动态代理把引用的dubbo接口转化为Invoker对象，Dubbo内部也是通过Invoker和Cluster、Protocol层进行交互。 对应着ProxyFactory的方法：123456789 /** * create proxy. * 创建代理 在引用dubbo服务时使用，引用的最终是代理对象 * * @param invoker * @return proxy */@Adaptive(&#123;PROXY_KEY&#125;)&lt;T&gt; T getProxy(Invoker&lt;T&gt; invoker) throws RpcException; 在Provider层进行导出暴露服务时，通过动态代理实现Invoker对象和真正dubbo服务实现类的转换，进而调用到dubbo服务。 对应着ProxyFactory的getInvoker方法123456789101112/** * create invoker. * 代理工程创建Invoker 在export导出服务时使用 * * @param &lt;T&gt; * @param proxy * @param type * @param url * @return invoker */@Adaptive(&#123;PROXY_KEY&#125;)&lt;T&gt; Invoker&lt;T&gt; getInvoker(T proxy, Class&lt;T&gt; type, URL url) throws RpcException; 客户端的代理逻辑在客户端Invoker模型是Dubbo内部封装的客户端发起网络调用的模型，通过ProxyFactory.getProxy(invoker)生成代理对象供客户端业务引用调用，屏蔽了内部细节。 客户端于服务引用时调用的getProxy方法生成的代理类，可以看到都是将拦截逻辑用InvokerInvocationHandler实现，也就是消费端引用的dubbo远程对象，都是一个代理对象，会在发起调用时走到InvokerInvocationHandler的invoke方法：123456789101112131415161718192021222324252627282930313233343536@Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (method.getDeclaringClass() == Object.class) &#123; // object类方法直接调用 return method.invoke(invoker, args); &#125; String methodName = method.getName(); Class&lt;?&gt;[] parameterTypes = method.getParameterTypes(); if (parameterTypes.length == 0) &#123; if ("toString".equals(methodName)) &#123; return invoker.toString(); &#125; else if ("$destroy".equals(methodName)) &#123; invoker.destroy(); return null; &#125; else if ("hashCode".equals(methodName)) &#123; return invoker.hashCode(); &#125; &#125; else if (parameterTypes.length == 1 &amp;&amp; "equals".equals(methodName)) &#123; return invoker.equals(args[0]); &#125; // 封装创建RPCInvocation RpcInvocation rpcInvocation = new RpcInvocation(method, invoker.getInterface().getName(), args); // Invoker的url生成serviceKey String serviceKey = invoker.getUrl().getServiceKey(); rpcInvocation.setTargetServiceUniqueName(serviceKey); if (consumerModel != null) &#123; rpcInvocation.put(Constants.CONSUMER_MODEL, consumerModel); rpcInvocation.put(Constants.METHOD_MODEL, consumerModel.getMethodModel(method)); &#125; // 这里调用被代理的Invoker对象 主要看几个层级的 @See 集群容错的ClusterInvoker、 Consumer端的Filter链、AbstractInvoke.invoke方法、DubboInvoker.doInvoke()方法 // 返回值 @See AsyncRpcResult // invoke完成之后 调用调用结果的recreate()方法 如果存在异常 在客户端抛出对应的异常 return invoker.invoke(rpcInvocation).recreate(); &#125; 服务端代理逻辑在服务端也用到了动态代理和Invoker模型。服务端的Invoker代表可执行对象，provider端的真正dubbo实现类被封装为ProxyInvoker（通过ProxyFactory.getInvoker(proxy)方法，这里的proxy是dubbo真正实现类），对不同的dubbo接口服务实现都抽象为ProxyInvoker模型来实现调用。 为dubbo服务实现类生成代理也有jdk动态代理和Javassist动态代理两种实现。无论哪种实现Dubbo服务实现类都会被AbstractProxyInvoker，调用invoke方法时，会委托给子类的doInvoke方法，两个代理框架有不一样的实现。 其中invoke方法如下：12345678910111213141516171819202122232425262728293031323334@Override public Result invoke(Invocation invocation) throws RpcException &#123; try &#123; // 子类实现doInvoke方法 即具体的ProxyFactory实现 （jdk或者javassist） // 比如 JavassistProxyFactory中实现doInvoke是通过调用字节码动态生成的Wrapper的子类 调用其invokeMethod方法 // 内部会调用dubbo服务实现类的方法 Object value = doInvoke(proxy, invocation.getMethodName(), invocation.getParameterTypes(), invocation.getArguments()); // 基于value创建一个CompltableFuture CompletableFuture&lt;Object&gt; future = wrapWithFuture(value); CompletableFuture&lt;AppResponse&gt; appResponseFuture = future.handle((obj, t) -&gt; &#123; // 根据dubbo返回值封装的future 注册一个任务 将真正返回值包装到AppResponse AppResponse result = new AppResponse(); if (t != null) &#123; if (t instanceof CompletionException) &#123; result.setException(t.getCause()); &#125; else &#123; result.setException(t); &#125; &#125; else &#123; result.setValue(obj); &#125; return result; &#125;); // 包装一层AsyncRpcResult返回给上层Invoker return new AsyncRpcResult(appResponseFuture, invocation); &#125; catch (InvocationTargetException e) &#123; if (RpcContext.getContext().isAsyncStarted() &amp;&amp; !RpcContext.getContext().stopAsync()) &#123; logger.error("Provider async started, but got an exception from the original method, cannot write the exception back to consumer because an async result may have returned the new thread.", e); &#125; return AsyncRpcResult.newDefaultAsyncResult(null, e.getTargetException(), invocation); &#125; catch (Throwable e) &#123; throw new RpcException("Failed to invoke remote proxy method " + invocation.getMethodName() + " to " + getUrl() + ", cause: " + e.getMessage(), e); &#125; &#125; doInvoke的两种实现： jdkJdkRpcProxyFactory实现：就是用java反射调用具体的dubbo服务实现类 123456789101112131415// JdkRpcProxyFactory.getInvoker @Override public &lt;T&gt; Invoker&lt;T&gt; getInvoker(T proxy, Class&lt;T&gt; type, URL url) &#123; // jdk反射 来封装dubbo服务实现类的代理封装 return new AbstractProxyInvoker&lt;T&gt;(proxy, type, url) &#123; @Override protected Object doInvoke(T proxy, String methodName, Class&lt;?&gt;[] parameterTypes, Object[] arguments) throws Throwable &#123; // 直接反射调用 Method method = proxy.getClass().getMethod(methodName, parameterTypes); return method.invoke(proxy, arguments); &#125; &#125;; &#125; JavassistProxyFactory实现：利用Wrapper的invokeMethod方法封装实现调用具体的dubbo服务实现类。其中Wrapper是利用Javassist框架生成的动态代理类，来完成对dubbo服务实现类真正方法的调用。 12345678910111213141516171819// getInvoker 服务导出时使用 封装dubbo服务实现类为invoker @Override public &lt;T&gt; Invoker&lt;T&gt; getInvoker(T proxy, Class&lt;T&gt; type, URL url) &#123; // TODO Wrapper cannot handle this scenario correctly: the classname contains '$' // 字节码生成的Wrapper实现类 对传入的proxy（export场景是dubbo服务实现类）进行包装 final Wrapper wrapper = Wrapper.getWrapper(proxy.getClass().getName().indexOf('$') &lt; 0 ? proxy.getClass() : type); return new AbstractProxyInvoker&lt;T&gt;(proxy, type, url) &#123; // 实现doInvoke方法 会从AbstractProxyInvoker.invoke中调用 @Override protected Object doInvoke(T proxy, String methodName, Class&lt;?&gt;[] parameterTypes, Object[] arguments) throws Throwable &#123; // 调用Invoker.invoke方法时会委托给wrapper.invokeMethod方法 return wrapper.invokeMethod(proxy, methodName, parameterTypes, arguments); &#125; &#125;; // Invoker.invoke --&gt; AbstractProxyInvoker.invoke --&gt; AbstractProxyInvoker.doInvoke --&gt; Wrapper.getWrapper.invokeMethod &#125; 代理层总结代理层在在dubbo-rpc-api包内，Consumer层的Proxy屏蔽了复杂的网络交互、集群策略、Dubbo内部的Invoker等概念，提供给上层业务是Dubbo接口，像本地方法一样的去发起远程调用；而Provider端的Wrapper的动态代理封装了不同的Dubbo服务实现类，统一转换为Dubbo的Invoker模型。代理层让业务接口和Dubbo框架无缝对接。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo中的代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql慢查的原因总结]]></title>
    <url>%2Fblog%2F2020%2F06%2F17%2Fsql%E6%85%A2%E6%9F%A5%E7%9A%84%E5%8E%9F%E5%9B%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[SQL的where没有对应的索引，导致走了全表查询。 索引失效 索引列的隐式转换。类型不匹配。 索引列直接使用内置函数。 多个列排序时顺序不一致。order by a asc,b desc会导致索引失效。 查询条件包含or可能会导致索引失效。 比如sql，优化器成本分析走name索引之后还要去全表扫描过滤age条件，不如全表扫描，所以最后没有走索引。 12-- name有索引 age没有select * from user where name = 'xxx' or age =10; like通配符可能导致索引失效。比如查询条件是 like ‘%aaa’。这里可以优化的一个点是优化为最左前缀原则和select的字段尽量只有索引值，让sql可以走索引覆盖。 联合索引没有走最左前缀原则。这里可以注意下二级联合索引的icp索引下推减少回表次数的特性。 索引值进行运算。走不了索引的。 in、not in、is null、is not null使用如果扫描过多行记录，优化器可能选择全表扫描。 左右连接，用于关联的字段编码格式不一样，这个比较隐蔽。 Mysql优化器的选择不一定是最佳的，如果想让走一个索引可以使用force inedx。 limit深度分页问题 limit的过程： 1select * from table where created_time &gt; '2020-01-01' limit 100000, 10; 通过二级索引created_time，过滤条件，找到符合条件的id。 主键id回表查询。 依次扫描符合条件的100010行，取最后10行返回。 深度分页慢的原因 limit深度分页会扫描前100000行，然后再取到对应步长的数据。 扫描这么多行，意味着需要回表这么多次，回表查询是一个随机IO的过程。 如何优化？ 标签记录。 1select id,name,balance FROM account where id &gt; 100000 limit 10; 这个方法有一定的局限性，比如要求查询条件能定位到id这种标签，走聚簇索引去直接拿到对应的记录去捞取10条记录。 延迟关联这个方法的思路就是拆分成join查询，因为之前一次查询只能走二级索引之后回表去查询对应深度的分页数据，这里思路是先在二级索引上查找符合条件的主键id，再与原来的表join通过主键id关联，这里的连接查询是主键关联，不需要二级索引每条查出来主键id去回表，速度也是不慢的。 12select id, name, balance from account b inner join (select a.id from account a where a.created_time &gt; '2020-01-01' limit 100000, 10) on a.id = b.id 单表中的数据太大，三层B+树大概能承载2000w的记录，所以如果单表数据量太大，那么B+树的高度会变高，加多了加载数据的磁盘IO次数，即索引的效率会变慢。 过多的表连接查询。一般不建议超过3个表进行连接查询，连接查询也要用索引列关联，否则会占用内存创建join_buffer，来用块的嵌套循环查询算法，加大对内存的压力。可以尝试在应用代码里做关联。 数据库在刷脏页时可能会阻塞sql的执行。 redo log文件写满了，要将循环写的redo log文件中记录的数据脏页刷入磁盘，已能为redo log提供空间。这时会阻塞写sql的性能。 Buffer Pool没有额外的空闲缓存页，这时候要根据lru链表将一些脏页、不常用的冷数据刷入到磁盘。因为脏页刷入磁盘是随机IO的过程。 order by 文件排序 如果order by不能使用索引排序，则会使用file sort文件排序，这里超过sort_buffer之后会到磁盘中排序，性能很差。索引天生有序这个特点要在order by这个语句中使用，尽量让排序走索引。 锁等待。 如果出现同一行数据被多个事务竞争锁，那么会造成事务锁等待，会加大sql的执行时间。 group by 默认排序和使用临时表 group by默认是排序的，而且为了实现分组可能使用临时表进行统计。这两点可能造成慢查询。优化点可以有： 让group by 的字段不排序，则少了一个排序的过程。这个取决于业务上只要分组。 注意内存临时表参数的设置，避免使用磁盘临时表。 group by使用索引，因为索引天生有序，所以统计可以直接扫描索引树。 本身数据库的机器问题和一些参数设置。本身机器比如IOPS（机器随机读写的性能）、CPU、内存、网络带宽都会影响到数据库的效率。 比如测试环境的机器打开了索引下推ICP或者index merge，而线上机器没有打开，则sql查询使用索引情况肯定不一样，这里要注意下。 还有Buffer Pool设置的大小，如果设置的太小，频繁造成刷页到内存中，会造成数据库慢查询。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>sql慢查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo分层架构]]></title>
    <url>%2Fblog%2F2020%2F06%2F12%2Fdubbo%E5%88%86%E5%B1%82%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[dubbo整体分层 左边淡蓝色背景的是服务消费者使用的接口，右边淡绿色背景的是服务提供者使用的接口，位于中轴线是双方都使用到的接口。 由上到下分为十层，各层为单向依赖，右边黑色箭头为依赖关系。每一层都可以剥离上层被复用，其中Service层和config层是api，其他层是SPI。 绿色小块是扩展接口，蓝色小块为实现类。 蓝色虚线是初始化部分，启动时候的export和refer流程；红色实线是调用过程，即发起一次dubbo调用。紫色箭头为继承。 各层的说明 config配置层：对外配置接口，以ServiceConfig、ReferenceConfig为中心，初始化配置类，也可以通过Spring容器生成配置类。 Proxy层：服务接口透明代理，生成服务的客户端Stub和服务器端Skeleton，可以理解为在客户端生成代理来发起远程调用，服务端生成统一的可调用代理，扩展接口是ProxyFactory，主要实现是Javassist动态代理。 registry注册中心层：封装服务注册和发现，以服务URL为中心，实现注册、订阅、监听回调等功能，扩展接口为RegistryFactory、RegistryService、Registry，代码层面分为api和具体实现，有多种实现比如etcd、redis、zk等。 cluster路由层：封装多个服务提供者的服务目录、路由、负载均衡，以Invoker为中心，并桥接注册中心，扩展接口为Cluster、Directory、Router、LoadBalance。 monitor层：RPC调用次数和调用时间监控。 protocol远程调用层：封装RPC调用，以Invocation、Result为中心，扩展接口为Protocol、Invoker、Exporter。代码层面有dubbo-rpc-api和具体的协议实现。 exchange信息交换层：封装请求响应模式，同步转异步，以Request、Response为中心，扩展接口为Exchanger、ExchangeChannel、ExchangeClient、ExchangeServer。 transport网络传输层：抽闲mina和netty等为统一接口，以Message为中心，扩展接口是Channel、Transporter、Client、Server、Codec serialize 数据序列化层：提供一些可复用的序列化算法和工具，扩展接口为 Serialization、ObjectInput、ObjectOutput、ThreadPool 各层之间的关系 在dubbo中，Protocol是核心层，也就是只要有Protocol+Invoker+Exporter就可以完成非透明的RPC调用。 Cluster不是必须的，只是服务提供者都是集群，Cluster目的是伪装为一个Invoker，外层只需要关注Protocol层Invoker即可，只有一个服务提供者是不需要Cluster的。 Proxy层是和用户（服务端、客户端）交互时屏蔽RPC的细节使用的。让PRC调用变得更透明，调用者就像调用本地的接口。 Remoting包含了Exchang信息交换层和Transport信息传输层，Transport只是封装了单向的信息传输，比如Netty接收发送消息，而在Exchange层封装了Request-Reponse语义。 调用时经历的模块]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo分层</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[duubo注解整合spring原理]]></title>
    <url>%2Fblog%2F2020%2F06%2F01%2Fduubo%E6%B3%A8%E8%A7%A3%E6%95%B4%E5%90%88spring%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[dubbo整合Spring使用的注解对于一个Provider的启动类：12345678910111213141516171819public class Application &#123; public static void main(String[] args) throws Exception &#123; AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(ProviderConfiguration.class); context.start(); System.in.read(); &#125; @Configuration @EnableDubbo(scanBasePackages = "org.apache.dubbo.demo.provider") @PropertySource("classpath:/spring/dubbo-provider.properties") static class ProviderConfiguration &#123; @Bean public RegistryConfig registryConfig() &#123; RegistryConfig registryConfig = new RegistryConfig(); registryConfig.setAddress("zookeeper://127.0.0.1:2181"); return registryConfig; &#125; &#125;&#125; 可以看到在配置类中加入了@EnableDubbo注解，还利用Spring的@PropertySource(path)导入了provider的配置，后者其实就是把配置文件的properties属性导入到Spring的Environment中，方便使用。 @EnableDubbo肯定也是采用了@Enable + @Import模式导入了BeanDefinition来实现Dubbo和Spring的整合。 暴露一个Dubbo接口使用@Service注解，引用一个Dubbo服务使用Dubbo@Reference注解。 Dubbo和Spring整合要做的事情总结一下Dubbo和Spring整合要做的一些事情： 把配置properties文件实例化为一个个bean来管理，方便注入到ServiceBean（dubbo暴露的服务bean）和ReferenceBean（@Reference注解引用的bean）。这里可以看下ProviderConfig、ApplicationConfig等配置类，内部的变量属性就是配置文件中的key。 扫描应用中使用@Service注解的服务，生成其对应的ServiceBean，表示其为一个dubbo服务。然后在一定的时机（比如收到上下文刷新的事件之后）触发此接口的export导出逻辑。export是dubbo内部的逻辑，在Spring整合Dubbo这块不纠结。 扫描@Reference注解的属性或者方法，注入引用的Dubbo服务代理对象。这里要完成对dubbo对象的属性注入，且完成其需要的代理逻辑，然后调用dubbo内部的refer()方法。 整体流程和原理 @Service的处理在Spring扩展点：BeanDefinitionRegistryPostProcessor.postProcessBeanDefinitionRegistry。 @Reference的处理在Spring扩展点：InstantiationAwareBeanPostProcessorAdapter.postProcessPropertyValues。 @Service注解会先注册Spring bean（和dubbo无关），比如DemoServiceImpl，这个就是最普通的bean。还会生成Dubbo标识暴露服务的ServiceBean。 在Spring启动完成之后，会通过事件或者回调函数来完成export即dubbo接口导出的工作。 @Reference的解析会在Spring容器中存放ReferenceBean，如果是本地的Dubbo服务，会直接将ServiceBean作为ReferenceBean，且代理逻辑会直接调用类的方法；而如果是远程的Dubbo服务，会调用get()方法内部走代理逻辑，其中也会走refer()方法。 @EnableDubbo注解123456@EnableDubboConfig@DubboComponentScan// @EnableDubbo注解来标注在spring boot应用上 来开启dubbo接口暴露、引用（dubbo的@Service、@Reference注解的扫描）。还有dubbo config配置初始化为bean的过程// 主要功能在@EnableDubboConfig、@EnableComponentScanpublic @interface EnableDubbo &#123;&#125; 主要是由内部两个注解完成。 @EnableDubboConfig1234567891011121314151617@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documented@Import(DubboConfigConfigurationRegistrar.class)// 解析Dubbo 配置 （properties文件）的支持 导入了DubboConfigConfigurationRegistrarpublic @interface EnableDubboConfig &#123; /** * It indicates whether binding to multiple Spring Beans. * * @return the default value is &lt;code&gt;false&lt;/code&gt; * @revised 2.5.9 */ boolean multiple() default true;&#125; 可以看到了Import了DubboConfigConfigurationRegistrar这个bean定义的注册器，这个注册器其中主要注册了两个DubboConfigConfiguration内部类的Bean定义，这两个Bean上的又注解了@EnableConfigurationBeanBindings。 @EnableConfigurationBeanBindings注解import了ConfigurationBeanBindingPostProcessor，这个bean定义注册器中： 开启import各个具体的配置bean 解析内部的每一个BeanBinding 生成对应的configBean。 注册一个后置处理器（ConfigurationBeanBindingPostProcessor），去利用Spring的DataBinder去将properties中的配置值映射到对应的configBean中。 比如处理ApplicationConfig对象中的所有属性字段，值从properties文件中获取。 12345678910111213141516171819202122232425public class DubboConfigConfigurationRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; // 获取EnableDubboConfig注解信息 AnnotationAttributes attributes = AnnotationAttributes.fromMap( importingClassMetadata.getAnnotationAttributes(EnableDubboConfig.class.getName())); // multiple的配置值 multiple就是 dubbo.applications（复数）这种多个配置 boolean multiple = attributes.getBoolean("multiple"); // Single Config Bindings // 注册Single配置Bean定义 内部主要是@EnableConfigurationBeanBindings注解再次import了具体配置bean定义的注册（ConfigurationBeanBindingsRegister）— // ，并且注册了对应的后置处理器去解析具体配置值 registerBeans(registry, DubboConfigConfiguration.Single.class); if (multiple) &#123; // Since 2.6.6 https://github.com/apache/dubbo/issues/3193 registerBeans(registry, DubboConfigConfiguration.Multiple.class); &#125; // Since 2.7.6 registerCommonBeans(registry); &#125;&#125; 123456789101112131415161718192021public class ConfigurationBeanBindingsRegister implements ImportBeanDefinitionRegistrar, EnvironmentAware &#123; private ConfigurableEnvironment environment; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; AnnotationAttributes attributes = AnnotationAttributes.fromMap( importingClassMetadata.getAnnotationAttributes(EnableConfigurationBeanBindings.class.getName())); AnnotationAttributes[] annotationAttributes = attributes.getAnnotationArray("value"); ConfigurationBeanBindingRegistrar registrar = new ConfigurationBeanBindingRegistrar(); registrar.setEnvironment(environment); for (AnnotationAttributes element : annotationAttributes) &#123; // 对定义好的每个binding（其实就是properties文件中的属性）去注册对应的Bean定义 且注册一个bean后置处理器（ConfigurationBeanBindingPostProcessor）去绑定配置中的值（值来源于properties文件解析之后放入的environment中） registrar.registerConfigurationBeanDefinitions(element, registry); &#125; &#125; @DubboComponentScan可以看到这个注解就是处理@Service和@Reference注解的扫描生成对应的Bean。注解import了DubboComponentScanRegistrar这个注册器（ImportBeanDefinitionRegistrar的实现）123456789101112131415// DubboComponentScanRegistrar dubbo扫描@Service和@Reference注解 解析public class DubboComponentScanRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; // 获取要扫描的包路径 Set&lt;String&gt; packagesToScan = getPackagesToScan(importingClassMetadata); // 注册路径下处理@Service注解的Bean后置处理器 registerServiceAnnotationBeanPostProcessor(packagesToScan, registry); // @since 2.7.6 Register the common beans registerCommonBeans(registry); &#125; 可以看到注册了ServiceAnnotationBeanPostProcessor和ReferenceServiceAnnotationBeanPostProcessor两个bean的后置处理器。 ServiceAnnotationBeanPostProcessor这个后置处理器是实现的Spring的扩展点：BeanDefinitionRegistryPostProcessor 。 会调用其postProcessBeanDefinitionRegistry方法。123456789101112131415161718192021//此方法中处理 dubbo @Service注解 @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; // @since 2.7.5 // 注册DubboBootstrapApplicationListener 监听事件之后启动Dubbo registerBeans(registry, DubboBootstrapApplicationListener.class); // 注解上解析出需要扫描的路径 Set&lt;String&gt; resolvedPackagesToScan = resolvePackagesToScan(packagesToScan); if (!CollectionUtils.isEmpty(resolvedPackagesToScan)) &#123; // 去注册ServiceBean registerServiceBeans(resolvedPackagesToScan, registry); &#125; else &#123; if (logger.isWarnEnabled()) &#123; logger.warn("packagesToScan is empty , ServiceBean registry will be ignored!"); &#125; &#125; &#125; 注册ServiceBean的方法registerServiceBeans如下： 扫描DubboClass路径，将实现类本身的Bean注册到容器中。 为@Service注解标注的Dubbo服务实现类去往容器中注册一个ServiceBean。 Bean生成之后，监听上下文刷新事件来触发ServiceBean去真正的导出服务。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Registers Beans whose classes was annotated &#123;@link Service&#125; * * @param packagesToScan The base packages to scan * @param registry &#123;@link BeanDefinitionRegistry&#125; */ private void registerServiceBeans(Set&lt;String&gt; packagesToScan, BeanDefinitionRegistry registry) &#123; // 创建一个dubbo类路径扫描器 去扫描 DubboClassPathBeanDefinitionScanner scanner = new DubboClassPathBeanDefinitionScanner(registry, environment, resourceLoader); BeanNameGenerator beanNameGenerator = resolveBeanNameGenerator(registry); scanner.setBeanNameGenerator(beanNameGenerator); // refactor @since 2.7.7 serviceAnnotationTypes.forEach(annotationType -&gt; &#123; // 要扫描的@Service注解 兼容版本 scanner.addIncludeFilter(new AnnotationTypeFilter(annotationType)); &#125;); for (String packageToScan : packagesToScan) &#123; // Registers @Service Bean first // scan去扫描路径下的 @Service注解Bean // 1. 先注册类本身的bean到Spring容器中 scanner内部扫描到会注册bean定义 scanner.scan(packageToScan); // Finds all BeanDefinitionHolders of @Service whether @ComponentScan scans or not. Set&lt;BeanDefinitionHolder&gt; beanDefinitionHolders = findServiceBeanDefinitionHolders(scanner, packageToScan, registry, beanNameGenerator); if (!CollectionUtils.isEmpty(beanDefinitionHolders)) &#123; for (BeanDefinitionHolder beanDefinitionHolder : beanDefinitionHolders) &#123; // 2. 再去去注册每个Dubbo服务接口的ServiceBean 内部会构建ServiceBean定义并注册在Spring容器中 registerServiceBean(beanDefinitionHolder, registry, scanner); &#125; if (logger.isInfoEnabled()) &#123; logger.info(beanDefinitionHolders.size() + " annotated Dubbo's @Service Components &#123; " + beanDefinitionHolders + " &#125; were scanned under package[" + packageToScan + "]"); &#125; &#125; else &#123; if (logger.isWarnEnabled()) &#123; logger.warn("No Spring Bean annotating Dubbo's @Service was found under package[" + packageToScan + "]"); &#125; &#125; &#125; &#125; ReferenceServiceAnnotationBeanPostProcessorReferenceServiceAnnotationBeanPostProcessor是继承的Spring的扩展点后置处理器InstantiationAwareBeanPostProcessorAdapter，实现了其postProcessPropertyValues方法来为@Reference注解标注的字段来注入对应的ReferenceBean。 12345678910111213141516@Override public PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeanCreationException &#123; InjectionMetadata metadata = findInjectionMetadata(beanName, bean.getClass(), pvs); try &#123; // 也是借助metadata.inject来实现的注入字段 metadata.inject(bean, beanName, pvs); &#125; catch (BeanCreationException ex) &#123; throw ex; &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, "Injection of @" + getAnnotationType().getSimpleName() + " dependencies is failed", ex); &#125; return pvs; &#125; 这里inject会走到AnnotatedMethodElement.inject方法，内部会调用子类实现的doGetInjectedBean方法。Dubbo在ReferenceAnnotationBeanPostProcessor中实现了这个方法doGetInjectedBean。12345678910111213141516171819202122232425262728293031@Override protected void inject(Object bean, String beanName, PropertyValues pvs) throws Throwable &#123; Class&lt;?&gt; injectedType = pd.getPropertyType(); Object injectedObject = getInjectedObject(attributes, bean, beanName, injectedType, this); ReflectionUtils.makeAccessible(method); method.invoke(bean, injectedObject); &#125; // getInjectedObject方法 protected Object getInjectedObject(AnnotationAttributes attributes, Object bean, String beanName, Class&lt;?&gt; injectedType, InjectionMetadata.InjectedElement injectedElement) throws Exception &#123; String cacheKey = buildInjectedObjectCacheKey(attributes, bean, beanName, injectedType, injectedElement); Object injectedObject = injectedObjectsCache.get(cacheKey); if (injectedObject == null) &#123; // doGetInjectedBean 方法 dubbo实现了ReferenceBean注入的关键 injectedObject = doGetInjectedBean(attributes, bean, beanName, injectedType, injectedElement); // Customized inject-object if necessary injectedObjectsCache.putIfAbsent(cacheKey, injectedObject); &#125; return injectedObject; &#125; doGetInjectedBean方法：12345678910111213141516171819202122232425262728293031323334353637383940/** * 实现的doGetInjectedBean方法来解析@DubboReference字段 来注入dubbo服务（最终是ReferenceBean的代理对象） * @param attributes * @param bean * @param beanName * @param injectedType * @param injectedElement * @return * @throws Exception */ @Override protected Object doGetInjectedBean(AnnotationAttributes attributes, Object bean, String beanName, Class&lt;?&gt; injectedType, InjectionMetadata.InjectedElement injectedElement) throws Exception &#123; /** * The name of bean that annotated Dubbo's &#123;@link Service @Service&#125; in local Spring &#123;@link ApplicationContext&#125; * 先在本地的Spring上下文中查找ServiceBean是否存在 * ServiceBean: com.xxx.DemoService:group:version */ String referencedBeanName = buildReferencedBeanName(attributes, injectedType); /** * The name of bean that is declared by &#123;@link Reference @Reference&#125; annotation injection * 根据@Reference注解的属性和注入的类属性 来生成referenceBean的名称 */ String referenceBeanName = getReferenceBeanName(attributes, injectedType); // 从缓存中取ReferenceBean 如果不存在 会去创建（内部的配置也会注入） ReferenceBean referenceBean = buildReferenceBeanIfAbsent(referenceBeanName, attributes, injectedType); // 是否是本地的bean 上面的ServiceBeanName来在容器中寻找 boolean localServiceBean = isLocalServiceBean(referencedBeanName, referenceBean, attributes); // 容器中注册referenceBean registerReferenceBean(referencedBeanName, referenceBean, attributes, localServiceBean, injectedType); cacheInjectedReferenceBean(referenceBean, injectedElement); // 最终返回注入给@Reference变量是 ReferenceBean的代理对象 是个FactoryBean 代理逻辑在其get()方法中（远程bean） return getOrCreateProxy(referencedBeanName, referenceBean, localServiceBean, injectedType); &#125; 因为客户端去引用Dubbo远程服务bean时要屏蔽一些细节，让引用的dubbo bean具有调用远程接口的能力，所以这里为ReferenceBean去生成代理，走的是getOrCreateProxy方法：12345678910111213private Object getOrCreateProxy(String referencedBeanName, ReferenceBean referenceBean, boolean localServiceBean, Class&lt;?&gt; serviceInterfaceType) &#123; if (localServiceBean) &#123; // If the local @Service Bean exists, build a proxy of Service // 本地的dubbo服务 创建jdk动态代理 内部直接调用ServiceBean的ref的方法 return newProxyInstance(getClassLoader(), new Class[]&#123;serviceInterfaceType&#125;, newReferencedBeanInvocationHandler(referencedBeanName)); &#125; else &#123; // 如果需要export 帮助依赖的ServiceBean 去 export exportServiceBeanIfNecessary(referencedBeanName); // If the referenced ServiceBean exits, export it immediately // get方法创建代理对象 return referenceBean.get(); &#125; &#125; 这里是直接调用referenceBean.get()方法去生成的代理对象。 在Spring容器中存放的是ReferenceBean对象，但本身这个Bean也是实现FactoryBean接口的，所以会调用其getObject()方法来取出真正的代理之后的bean。其实getObejct方法也是调用自身的get()方法来走代理逻辑的：1234567getObject()方法： @Override public Object getObject() &#123; // 调用get生成代理对象 return get(); &#125;]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo注解整合Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orika线程死循环记录]]></title>
    <url>%2Fblog%2F2020%2F05%2F24%2Forika%E7%BA%BF%E7%A8%8B%E6%AD%BB%E5%BE%AA%E7%8E%AF%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[问题现象在测试环境看到机器cpu报警，且cpu是突然升起来并且一直稳定跑满在百分之90左右。观察流量和接口的qps，并没有突然增加或者有突刺。 问题排查上机器top -H -p pid + jstack观察之后发现很多http线程卡在orika的一个weakHashMap的get方法中： 很明显，这里是触发了经常看到HashMap一类分析文章中的map链表成环并且死循环的问题，然后就去查看了orika中的这个类，代码维护了一个全局的weakHashMap： 12// 定义了一个WeakHashMap private static volatile WeakHashMap&lt;java.lang.reflect.Type, Integer&gt; knownTypes = new WeakHashMap&lt;java.lang.reflect.Type, Integer&gt;(); 这里有点不明白的是使用的是java1.8，记得代码中是更改了扩容时候链表的迁移方式，避免了成环操作，然后就去看了WeakHashMap的操作，发现原来WeakHashMap并没有去树化和改变迁移链表的方式，还是可能出现成环，然后在get的时候死循环导致cpu异常。 其实在WeakHashMap的注释中也看到了对应不同步的说法： 1234* &lt;p&gt; Like most collection classes, this class is not synchronized.* A synchronized &lt;tt&gt;WeakHashMap&lt;/tt&gt; may be constructed using the* &#123;@link Collections#synchronizedMap Collections.synchronizedMap&#125;* method. 问题解决这里尝试去找了网上的文章和orika的issue，发现有遇到同样问题的文章和issue: 参考blog) 官方issue 这里都提示了在高版本解决了这个问题。我这里使用的是orika1.5.1版本的包，升级为最新的1.5.4之后，再看这个weakHashMap加上了同步方法：]]></content>
      <categories>
        <category>bug记录</category>
      </categories>
      <tags>
        <tag>orika</tag>
        <tag>map死循环</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC日志和G1垃圾回收器]]></title>
    <url>%2Fblog%2F2020%2F05%2F24%2FGC%E6%97%A5%E5%BF%97%E5%92%8CG1%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[GC日志输出的GC日志是分析线上问题的很关键的点，要能看懂不同收集器下对应的垃圾回收日志，比如CMS回收器的每次GC的每个阶段都在GC日志里详细标出。 对于应用程序可以配置jvm参数：12-Xloggc:./gc-%t.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCCause-XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M 其中： -Xloggc:./gc-%t.log：日志文件存放的位置，./是当前位置 -XX:PrintGCDetails：打印GC日志详细信息 -XX:PrintGCDateStamps：打印GC发生日期戳 -XX:PrintGCTimeStamps：打印GC发生时间戳 -XX:PrintGCCause：打印GC原因 -XX:UseGCLogFileRotation：滚动日志记录 -XX:NumberOfGCLogFiles=10：分为10个日志文件记录 -XX:GCLogFileSize=100m：每个日志文件100兆大小 GC日志关于内存展示：num1 -&gt; num2(num3)。其中： num1：GC之前此区域的使用大小 num2：GC之后此区域的使用大小 num3：此区域的总容量 Parallel收集器的GC日志Parallel年轻代的收集器是Parallel Scavenge，老年代的垃圾收集器是Parallel Old。 MinorGC1234562020-07-10T17:45:40.296+0800: 1.285:[GC (Allocation Failure) [PSYoungGen: 49152K-&gt;3892K(57344K)] 49152K-&gt;3900K(188416K), 0.0080059 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] GC (Allocation Failure)：代表一次MinorGC。Eden区分配内存失败引起的。 PSYoungGen：年轻代，收集器是Parallel Scavenge 49152K-&gt;3892K(57344K)：年轻代GC之前使用大小49152K，GC之后使用3892K，年轻代总内存57344K。 49152K-&gt;3900K(188416K)：堆内存GC之前使用大小49152K，GC之后3900K使用大小，总共188416K。 0.0080059 secs：总共耗时 Times: user=0.03 sys=0.00, real=0.01 secs：分别代表用户态消耗的CPU、内核态消耗的CPU时间、real是操作从开始到结束经理的墙钟时间。墙钟时间和CPU时间区别是：墙钟时间包含各种非运行的等待耗时，比如等待IO、等待磁盘、等待线程阻塞等，而CPU时间不包含这些，现在是多CPU或者多核机器，user+sys &gt; real是很正常的。 FullGC123456782020-07-10T17:45:44.450+0800: 5.439: [Full GC (Metadata GC Threshold) [PSYoungGen: 4286K-&gt;0K(201216K)][ParOldGen: 3289K-&gt;7277K(74752K)] 7575K-&gt;7277K(275968K), [Metaspace: 20854K-&gt;20854K(1069056K)],0.0542303 secs] [Times: user=0.11 sys=0.00, real=0.06 secs] Full GC (Metadata GC Threshold)：因为元空间不足引起的FullGC PSYoungGen: 4286K-&gt;0K(201216K。)：年轻代GC前占用4286K，GC后占用0K，总共201216K。 ParOldGen: 3289K-&gt;7277K(74752K)：老年代使用Parallel Old垃圾收集器，GC前占用3289K，GC后占用7277K，总共大小74752K。 MetaSpace: 20854K-&gt;20854K(1069056K)：不赘述和上面一样。 CMS垃圾回收器的GC日志123456789101112131415161718192021222324252627# 1.CMS初始标记 标记GCRoots直接关联的对象 会STW 第一个内存是老年代 第二个内存数据是堆内存0.245: [GC (CMS Initial Mark) [1 CMS-initial-mark: 32776K(53248K)] 41701K(99328K), 0.0061676 secs] [Times: user=0.01 sys=0.00, real=0.01 secs]# 2. CMS并发标记，进行GCRoots分析，标记存活对象，不会STW，用户线程和GC线程并发，且多个GC线程并行去标记0.251: [CMS-concurrent-mark-start]0.270: [CMS-concurrent-mark: 0.004/0.020 secs] [Times: user=0.08 sys=0.01, real=0.02 secs]# 3. CMS在重新标记之前会预清理 # preClean：清理 card marking 标记的 dirty card，更新引用记录。方便后边并发扫描的时候去扫描跨代引用。# abortable-preclean：调节final-remark阶段的时机，这个阶段不一定存在0.270: [CMS-concurrent-preclean-start]0.272: [CMS-concurrent-preclean: 0.001/0.001 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]0.272: [CMS-concurrent-abortable-preclean-start]0.291: [CMS-concurrent-abortable-preclean: 0.004/0.019 secs] [Times: user=0.09 sys=0.00, real=0.02 secs]# 4. CMS的最终重新标记 会STW 可以多线程并行去标记 会STW，修正之前并发执行用户线程带来的浮动对象或者引用的错误。0.291: [GC (CMS Final Remark) [YG occupancy: 17928 K (46080 K)]0.291: [Rescan (parallel) , 0.0082702 secs]0.299: [weak refs processing, 0.0000475 secs]0.299: [class unloading, 0.0002451 secs]0.299: [scrub symbol table, 0.0003183 secs]0.300: [scrub string table, 0.0001611 secs][1 CMS-remark: 49164K(53248K)] 67093K(99328K), 0.0091462 secs] [Times: user=0.04 sys=0.00, real=0.01 secs]# 5. CMS的并发清理 进行GC回收垃圾 多线程执行0.300: [CMS-concurrent-sweep-start]0.300: [CMS-concurrent-sweep: 0.000/0.000 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]# 6.CMS的并发重置 清除对象的标记（比如对象的三色标记），为下次GC做准备0.300: [CMS-concurrent-reset-start]0.300: [CMS-concurrent-reset: 0.000/0.000 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] CMS在资料里主要是四个阶段：初始标记、并发标记、最终重新标记、并发清理。在GC日志中会比较细和多了预清理、并发重置的过程。 CMS的concurrent mode failureCMS在并发标记或者并发清理阶段（不会STW）中，可能因为之前浮动垃圾的产生，或者老年代担保机制失败，或者启动CMS垃圾回收的阈值太大导致剩余空间不够用等原因，可能又会触发了FullGC，那么会出现Concurrent mode failure，此时CMS会退化为单线程的Serial Old垃圾回收器。12345678# CMS并发标记开始2022-05-23T16:34:29.502-0800: 0.687: [CMS-concurrent-mark-start]# FullGC（可能是浮动垃圾+用户线程触发）2022-05-23T16:34:29.503-0800: 0.688: [Full GC (Allocation Failure) 2022-05-23T16:34:29.503-0800: 0.688: [CMS2022-05-23T16:34:29.529-0800: 0.714: [CMS-concurrent-mark: 0.026/0.027 secs] [Times: user=0.04 sys=0.00, real=0.02 secs] # concurrent mode failure 退化为SerialOld。 (concurrent mode failure): 194559K-&gt;194559K(194560K), 0.1069558 secs] 203774K-&gt;203771K(203776K), [Metaspace: 3306K-&gt;3306K(1056768K)], 0.1070098 secs] [Times: user=0.12 sys=0.00, real=0.11 secs] 可以看到在并发标记开始之后，触发了一次FullGC（用户线程没有STW），此时FullGC之后会有concurrent mode failure，CMS退化为Serial Old。 G1垃圾回收器深入理解JVM中的知识点G1（Garbage-First）收集器也是追求很低的停顿垃圾回收时间的收集器，在高版本的jdk中建议使用。 在G1之前垃圾收集器都是收集单独的年轻代或者老年代对象，而G1收集器将整个Java堆空间分为多个大小相等的Region，虽然还在概念上保留着新生代和老年代，但不再是物理隔离划分的Region，都是一部分Region(不需要连续)的集合。 G1垃圾回收器之所以能建立可预测的停顿时间模型，是因为其可有计划的在Java堆中进行全区域的垃圾收集。各个Region定义了一个垃垃圾堆积的价值大小（回收所获得的空间大小以及回收所需要时间的经验值），G1在后台维护了一个优先列表， 每个根据允许的回收时间，来选择优先回收价值最大的Region（这解释了为什么叫做Garbage First）。这种使用了Region划分内存的方式和具有优先级回收的方式，保证了G1在有限时间内尽可能获取到最高的回收效率。 回想之前垃圾回收器的跨代引用，如果回收新生代时也要扫描老年代的引用，那么MinorGC的效率会很低。G1也存在这个问题，每个Region不可能是单独独立的，可能存在Region之间的相互引用，如果要扫描整个堆空间，那效率是不可接受的。和跨代引用一样，Region之间的对象引用扫描效率问题都是使用RememberdSet来避免扫描全部区域的。G1每个Region都有一个对应的RememberedSet，虚拟机在发现对Reference类型的对象赋值操作时，会通过内部写屏障来实现将跨Region的引用信息维护被引用对象的RememberedSet中，所以在GCRoots扫描过程中只扫描存在跨Region的区域即可，不需要扫描整个堆，提高了扫描效率。 G1垃圾回收的过程 初始标记（Inital Marking)：仅仅是标记一下GCRoots能直接关联的对象，这部分只扫描直接关联的对象，很快，同时此阶段需要STW。 并发标记（Concurrent Marking）：此阶段是从GCRoots开始对堆中对象进行可达性分析，标记连通还能存活的对象。和CMS一样，这个阶段是主要的耗时阶段，但不会STW，即可以和用户线程并发的执行。 最终标记（Final Marking）：修正在并发标记阶段因用户线程继续运行而导致标记有误的对象，这阶段会STW，可以并行标记，且时间也不会很长。 筛选回收（Live Data Counting and Evacuation）：筛选回收阶段首先对每个Region按照回收的价值和成本进行排序（G1自己维护的优先级列表），根据所期望的GC停顿时间计划来回收。注意这部分会STW，且GC线程可以并行执行。 G1的优点 并发和并行：G1能在多核环境下使用多个CPU来加快STW的时间（并行），而且在一些耗时阶段是可以也可以和用户线程一起运行，并STW用户线程（并发）。 分代收集：G1的分代收集还是得以保留。G1可以不搭配其他垃圾收集器配合就能独立管理整个堆，但其保留了分代思想，对新创建的对象、已经存在一段时间、熬过很多次GC的旧对象都采用了不同的方式 空间整合：G1从整体看是采用的标记-整理算法实现。但是G1从两个Region来看是基于复制算法实现的。空间整合代表G1在GC之后不会产生垃圾碎片，GC之后都是规整的内存。 可预测的停顿：这是G1相比CMS垃圾收集器最大的优势。G1除了和CMS一样追求低停顿外，还建立了可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，停留在垃圾收集上的时间不超过N毫秒。 G1的垃圾回收分类G1的mixedGC就是把老年代（逻辑分区）的一部分区域加载Eden和Surivor区域的后面，所有要回收的区域叫做Collection Set(CSet)，最后用年轻代的算法进行回收。 这里也能看出逻辑分代的好处，没有真正的物理划分，而是通过划分更细粒度的Region来实现的mixedGC要回收的区域。 在执行mixedGC时，其实就是筛选回收的过程，会回收young region和部分old Region还有大对象Region，这其中对回收对象进行了价值排序，根据用户设置的期望停顿时间来优先回收价值高的对象，即G1的可预测的停顿模型。 相关问题 G1的特点是什么 并行和并发 分代收集（逻辑分代），内存划分为Region 垃圾回收分类为youngGC、mixedGC、fullGC 整体来看采用标记-整理算法，不会有空间碎片产生，region之间采用复制算法清除（年轻代的younggc和mixedgc） 最显著的特点：可预测的停顿模型。可以按照用户设置的在GC上停顿的时间来执行价值排序优先级的回收，实现追求最低停顿的GC时间。 适用大内存服务器，来设置可预测的GC STW时间 G1和CMS的区别 G1对整个内存区域回收、CMS是老年代的回收器，需要搭配ParNew等年轻代垃圾回收器一起使用 CMS是标记-清除算法，G1是标记-整理算法 增量阶段的处理，CMS采用增量更新，G1采用的是satb（快照） G1分代但是逻辑分代。 G1最大的特点：提供了用户可设置的GC停顿时间，提供了可预测的停顿模型，适用于大内存。 G1如何控制暂停时间 对young gc控制新生代Region的大小，会根据设置的-XX:MaxGCPauseMills值来调整，来减少触发YoungGC的次数。 对mixed gc控制回收region的个数，根据参数-XX:InitiatingHeapOccupancyPercent值触犯，回收所有的Young区、部分Old区（根据期望的GC停顿时间对old区垃圾收集排序选出）、和大对象区，采用复制算法。 fullgc：会暂停应用程序，退化为单线程去标记、清除和压缩整理。 大内存系统为什么适合使用G1类似于Kafka这种支撑高并发系统，会部署很大内存的机器，比如年轻代可能都会很大，这种情况下因为eden区的回收可能变得很慢（相对于常见4G的机器的eden区），那young gc带来的stw对于系统来说不能接收。那么在这种情况下G1可以设置期望的最大GC暂停时间，-XX:MaxGCPauseMills，整个系统在GC上的时间大幅降低，可以最大限度的一遍用户线程在跑，一遍去GC。G1天生适合大内存机器来对JVM进行垃圾回收，可以解决大内存机器垃圾回收时间过长造成停顿的问题。]]></content>
      <categories>
        <category>Java基础</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>GC日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo2.7中时间轮的应用]]></title>
    <url>%2Fblog%2F2020%2F05%2F21%2Fdubbo2-7%E4%B8%AD%E6%97%B6%E9%97%B4%E8%BD%AE%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言dubbo内部有比较多定时任务的管理功能，JDK也提供了Timer和DelayedQueue等工具类，可以实现简单的定时任务管理，其底层实现就是使用的堆这种数据结构，存取的时间复杂度是O(nlogN)，无法支持大量的定时任务。dubbo内部采用了时间轮的方式来管理定时任务。应用场景比如：dubbo的心跳机制、dubbo客户端超时检测等。 时间轮是一种高效的、批量管理的定时任务的调度模型。时间轮一般会实现一个环形结构，类似于时钟，分为很多槽，每个槽代表一个时间间隔，每个槽使用双向链表来存储定时任务；指针周期性地跳动，跳动到一个槽位，执行对应的定时任务。 注意下单层时间轮的容量和精度是有限的，如果时间跨度特比大，精度要求很高，或者海量定时任务需要调度的场景，通常会使用多级时间轮以及持久化存储的方案。比如多级时间轮以及持久化存储与时间轮结合，每级时间轮的时钟周期不一样，比如年级别时间轮、月级别时间轮、日级别时间轮、毫秒级别时间轮，定时任务在创建时先持久化，在时钟指针接近时预读到内存，并且需要定期清理磁盘上的过期任务。 Dubbo中，时间轮的实现方式是主要在dubbo-common的org.apache.dubbo.common.timer包中。dubbo和netty的实现基本一致，netty时间轮一个应用场景简单提下，Redisson实现分布式锁提供了watchdog锁续期的功能，为了避免每加锁一次起一个线程去扫描是否需要续期以及执行续期逻辑带来的压力，采用了netty的时间轮来注册续期任务，只用一个线程和合适的时间周期完成了续期逻辑。 核心接口Timer接口：定义了定时器的基本行为。核心方法是newTimeout方法，提交一个定时任务（TimerTask）返回关联的Timeout对象。123456789101112131415161718192021222324252627282930313233343536/** * 定义了定时器的基本行为 * 核心方法是newTimeout方法：提交一个定时任务（TimerTask）并且返回关联的Timeout对象，类似于线程池中提交任务 * Schedules &#123;@link TimerTask&#125;s for one-time future execution in a background * thread. */public interface Timer &#123; /** * Schedules the specified &#123;@link TimerTask&#125; for one-time execution after * the specified delay. * 向时间轮中提交一个定时任务 * * @return a handle which is associated with the specified task * @throws IllegalStateException if this timer has been &#123;@linkplain #stop() stopped&#125; already * @throws RejectedExecutionException if the pending timeouts are too many and creating new timeout * can cause instability in the system. */ Timeout newTimeout(TimerTask task, long delay, TimeUnit unit); /** * Releases all resources acquired by this &#123;@link Timer&#125; and cancels all * tasks which were scheduled but not executed yet. * * @return the handles associated with the tasks which were canceled by * this method */ Set&lt;Timeout&gt; stop(); /** * the timer is stop * * @return true for stop */ boolean isStop();&#125; HashedWheelTimer实现类：Timer接口的实现类。通过时间轮算法实现了一个定时器。执行过程为： 根据当前时间轮指针选定对应的槽。 遍历槽上的定时任务（HashedWheelTimeout），对每个定时任务进行计算，是当前时钟周期则去除，如果不是则将任务中的剩余时钟周期-1，代表距离执行又接近了一圈。 TimerTask接口：所有定时任务都要继承TimerTask接口。123456789101112131415/** * 所有定时任务都需要继承的接口 * A task which is executed after the delay specified with * &#123;@link Timer#newTimeout(TimerTask, long, TimeUnit)&#125; (TimerTask, long, TimeUnit)&#125;. */public interface TimerTask &#123; /** * Executed after the delay specified with * &#123;@link Timer#newTimeout(TimerTask, long, TimeUnit)&#125;. * * @param timeout a handle which is associated with this task */ void run(Timeout timeout) throws Exception;&#125; Timeout接口：TimerTask中run()方法的参数，可以查看定时任务的状态，还可以操作取消定时任务 HashedWheelTimeout：Timeout接口的唯一实现，是HashedWheelTimer的内部类。扮演两个角色： 第一个，时间轮中双向链表的节点，即定时任务TimerTask在HashedWheelTimer中的容器 第二个，定时任务TimerTask提交到HashedWheelTimer之后的句柄，用于在时间轮外查看和控制定时任务。 HashedWheelTimeout的核心字段123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 实现了Timeout接口 内部类 HashedWheelTimeout是Timeout的唯一实现。 两个作用 * 1.时间轮中双向链表的节点，定时任务TimerTask在HashedWheelTimer中的容器 * 2.TimerTask提交到HashedWheelTimer之后返回的句柄（Handle)，用于在时间轮外部查看和控制定时任务 */ private static final class HashedWheelTimeout implements Timeout &#123; private static final int ST_INIT = 0; private static final int ST_CANCELLED = 1; private static final int ST_EXPIRED = 2; // 状态控制 private static final AtomicIntegerFieldUpdater&lt;HashedWheelTimeout&gt; STATE_UPDATER = AtomicIntegerFieldUpdater.newUpdater(HashedWheelTimeout.class, "state"); private final HashedWheelTimer timer; // 实际被调度的任务 private final TimerTask task; // 定时任务被执行的时间 单位纳秒 // 计算公式：currentTime（创建 HashedWheelTimeout 的时间） + delay（任务延迟时间） - startTime（HashedWheelTimer 的启动时间） private final long deadline; @SuppressWarnings(&#123;"unused", "FieldMayBeFinal", "RedundantFieldInitialization"&#125;) // 状态有三种 INIT(0)、CANCELLED(1)、EXPIRED(2) private volatile int state = ST_INIT; // 状态字段 /** * RemainingRounds will be calculated and set by Worker.transferTimeoutsToBuckets() before the * HashedWheelTimeout will be added to the correct HashedWheelBucket. * 当前任务剩余的时钟周期数。时间轮表示的时间长度有限 在任务到期时间与当前时刻的时间差，超过时间轮单圈能表示的时长 * 就出现套圈的情况，这时需要该字段表示剩余的时钟周期 */ long remainingRounds; /** * 当前定时任务在链表中的前驱和后继节点 * 单线程操作不需要加锁控制 */ HashedWheelTimeout next; HashedWheelTimeout prev; /** * The bucket to which the timeout was added */ HashedWheelBucket bucket; HashedWheelTimeout的核心方法 isCancelled()、isExpired()、state()方法：主要用来检查HashedWheelTimeout的状态。 cancel()方法：将当前HashedWheelTimeout状态设置为CANCELLED，将当前HashedWheelTimeout添加到canceledTimeouts队列等待销毁。 expire()方法：当任务到期时，会调用该方法将当前HashedWheelTimeout设置为Expired状态，然后调用其中的TimerTask的run()方法执行定时任务。 remove()方法：将当前的HashedWheelTimeout从时间轮中删除。 HashedWheelTimer 时间轮上面有提到HashedWheelTimer实现类是时间轮的具体实现，工作原理是根据当前时间轮的指针选定对应的槽（HashedWheelBucket），从双向链表的头节点开始迭代，对每个HashedWheelTimeout进行计算，属于当前时钟周期则取出运行，不属于则将其时钟周期-1，等待下一圈的判断。 核心字段123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// 实现了Timer接口 通过时间轮算法实现了一个定时器 // 根据当前时间轮指针选定对应的槽（HashedWheelBucket），// 从双向链表头开始遍历，对每个定时任务（HashedWheelTimeout）进行计算，属于当前时钟周期取出运行，否则将 剩余时钟周期数 减1public class HashedWheelTimer implements Timer &#123; /** * may be in spi? */ public static final String NAME = "hased"; private static final Logger logger = LoggerFactory.getLogger(HashedWheelTimer.class); private static final AtomicInteger INSTANCE_COUNTER = new AtomicInteger(); private static final AtomicBoolean WARNED_TOO_MANY_INSTANCES = new AtomicBoolean(); private static final int INSTANCE_COUNT_LIMIT = 64; // 状态变更器 private static final AtomicIntegerFieldUpdater&lt;HashedWheelTimer&gt; WORKER_STATE_UPDATER = AtomicIntegerFieldUpdater.newUpdater(HashedWheelTimer.class, "workerState"); // 真正执行的定时任务逻辑 private final Worker worker = new Worker(); private final Thread workerThread; private static final int WORKER_STATE_INIT = 0; private static final int WORKER_STATE_STARTED = 1; private static final int WORKER_STATE_SHUTDOWN = 2; /** * 时间轮当前状态 * 0 - init, 1 - started, 2 - shut down */ @SuppressWarnings(&#123;"unused", "FieldMayBeFinal"&#125;) private volatile int workerState; // 时间指针每次加1所代表的实际时间 单位为纳秒 即槽与槽之间的时间间隔 private final long tickDuration; // 时间轮中的槽 即时间轮的环形队列 一般为大于且最靠近n的2的幂次方 private final HashedWheelBucket[] wheel; // 掩码 mask=wheel.length-1 执行ticks &amp; mask能定位到对应的时钟槽 private final int mask; // 确认时间轮中的startTime的闭锁 private final CountDownLatch startTimeInitialized = new CountDownLatch(1); // 两个队列是对于添加的定时任务和取消的定时任务的缓冲。 // 缓冲外部提交时间轮的定时任务 private final Queue&lt;HashedWheelTimeout&gt; timeouts = new LinkedBlockingQueue&lt;&gt;(); // 暂存取消的定时任务 会被销毁掉 private final Queue&lt;HashedWheelTimeout&gt; cancelledTimeouts = new LinkedBlockingQueue&lt;&gt;(); // 当前时间轮剩余的定时任务总数 private final AtomicLong pendingTimeouts = new AtomicLong(0); // 阈值 private final long maxPendingTimeouts; // 时间轮启动时间 提交到时间轮的定时任务deadline字段值以该时间为起点进行计算 private volatile long startTime;&#125; newTimeout方法提交定时任务123456789101112131415161718192021222324252627282930313233343536373839// 时间轮 去加入一个新的任务 @Override public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) &#123; if (task == null) &#123; throw new NullPointerException("task"); &#125; if (unit == null) &#123; throw new NullPointerException("unit"); &#125; //剩余定时任务数+1 long pendingTimeoutsCount = pendingTimeouts.incrementAndGet(); if (maxPendingTimeouts &gt; 0 &amp;&amp; pendingTimeoutsCount &gt; maxPendingTimeouts) &#123; // 阈值判断 pendingTimeouts.decrementAndGet(); throw new RejectedExecutionException("Number of pending timeouts (" + pendingTimeoutsCount + ") is greater than or equal to maximum allowed pending " + "timeouts (" + maxPendingTimeouts + ")"); &#125; // 确定时间轮的startTime start()方法会初始化时间轮，确定startTime // 启动workerThread 开始执行worker任务 start(); // 计算deadline // Add the timeout to the timeout queue which will be processed on the next tick. // During processing all the queued HashedWheelTimeouts will be added to the correct HashedWheelBucket. long deadline = System.nanoTime() + unit.toNanos(delay) - startTime; // Guard against overflow. if (delay &gt; 0 &amp;&amp; deadline &lt; 0) &#123; deadline = Long.MAX_VALUE; &#125; // 封装为HashedWheelTimeout 加入到队列中 HashedWheelTimeout timeout = new HashedWheelTimeout(this, task, deadline); timeouts.add(timeout); return timeout; &#125; newTimeout主要做了几件事： 维护了时间轮中剩余定时任务数量字段 start()方法 计算了时间轮的startTime方法，且修改时间轮的状态 worker线程调用start()方法，开启执行扫描时间轮的线程。 根据startTime来计算定时任务要调度的deadline，当前时间+延时时间 - 启动时间。 封装task为HashedWheelTimeout添加到timeouts执行队列中。 worker线程扫描时间轮并执行任务在worker线程的run()方法中：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687private final class Worker implements Runnable &#123; private final Set&lt;Timeout&gt; unprocessedTimeouts = new HashSet&lt;Timeout&gt;(); // tick周期 private long tick; @Override public void run() &#123; // Initialize the startTime. startTime = System.nanoTime(); if (startTime == 0) &#123; // We use 0 as an indicator for the uninitialized value here, so make sure it's not 0 when initialized. startTime = 1; &#125; // Notify the other threads waiting for the initialization at start().唤醒外边的线程 startTimeInitialized.countDown(); do &#123; // 等待下一个执行周期 即槽之间的时间间隔 比如每个槽之间是1s执行一次 final long deadline = waitForNextTick(); if (deadline &gt; 0) &#123; // 确认索引 int idx = (int) (tick &amp; mask); // 清理已取消的定时任务 processCancelledTasks(); // 对应的槽 HashedWheelBucket bucket = wheel[idx]; // 转移缓存在timeouts队列中的已提交定时任务到时间轮对应的槽中 transferTimeoutsToBuckets(); // 处理当前槽中的定时任务 bucket.expireTimeouts(deadline); tick++; &#125; &#125; while (WORKER_STATE_UPDATER.get(HashedWheelTimer.this) == WORKER_STATE_STARTED); // 模拟时间轮转动 // Fill the unprocessedTimeouts so we can return them from stop() method. for (HashedWheelBucket bucket : wheel) &#123; bucket.clearTimeouts(unprocessedTimeouts); &#125; for (; ; ) &#123; HashedWheelTimeout timeout = timeouts.poll(); if (timeout == null) &#123; break; &#125; if (!timeout.isCancelled()) &#123; // 状态变更之后 未被加入到槽中的未取消任务加入到unprocessedTimeouts队列 unprocessedTimeouts.add(timeout); &#125; &#125; processCancelledTasks(); &#125; // bucket.expireTimeouts 执行定时任务 /** * Expire all &#123;@link HashedWheelTimeout&#125;s for the given &#123;@code deadline&#125;. * 遍历双向链表中的全部 HashedWheelTimeout 节点。 在处理到期的定时任务时，会通过 remove() 方法取出， * 并调用其 expire() 方法执行；对于已取消的任务，通过 remove() 方法取出后直接丢弃；对于未到期的任务， * 会将 remainingRounds 字段（剩余时钟周期数）减一。 */ void expireTimeouts(long deadline) &#123; HashedWheelTimeout timeout = head; // process all timeouts while (timeout != null) &#123; HashedWheelTimeout next = timeout.next; if (timeout.remainingRounds &lt;= 0) &#123; next = remove(timeout); if (timeout.deadline &lt;= deadline) &#123; // 调用expire 内部会执行定时任务的run方法 timeout.expire(); &#125; else &#123; // The timeout was placed into a wrong slot. This should never happen. throw new IllegalStateException(String.format( "timeout.deadline (%d) &gt; deadline (%d)", timeout.deadline, deadline)); &#125; &#125; else if (timeout.isCancelled()) &#123; // 取消直接remove next = remove(timeout); &#125; else &#123; // 未到期任务的remainingRounds - 1 timeout.remainingRounds--; &#125; timeout = next; &#125; &#125; 时间轮一次转动的流程： 时间轮转动，时间周期开始 清理用户主动取消的任务，会记录在cancelledTimeouts队列中，在每次指针转动的时候，时间轮也会清理该队列。 将缓存在timeouts队列中的定时任务转移到时间轮对应的槽中。 根据当前指针定位槽，遍历双向链表，来执行对应的任务，方法实现在HashedWheelBucket.expireTimeouts方法中： 循环遍历双向链表，当定时任务的remainingRounds小于等于0，则说明是当前时钟周期内的任务，判断是否达到了deadline（满足时间的触发，兜底判断，一般在时钟周期内都会满足），如果满足调用expire()方法执行任务，内部会调用TimerTask的run方法，即真正的定时任务的逻辑。 如果用户取消，则直接remove()从链表上摘除。 继续遍历下一个Timeout节点。 时间轮一直是运行状态，则重复上述轮询的操作；如果时间轮为停止状态，则遍历每个槽位，来清除注册的定时任务。最后再清理cancelledTimeouts队列中用户取消的任务。 Dubbo中时间轮的应用客户端的超时检查客户端发起调用时会创建一个DefaultFuture，用于发起远程调用且阻塞同步等待结果。 1234567891011121314151617181920// DefaultFuture的newFuture方法public static DefaultFuture newFuture(Channel channel, Request request, int timeout, ExecutorService executor) &#123; final DefaultFuture future = new DefaultFuture(channel, request, timeout); future.setExecutor(executor); // ThreadlessExecutor needs to hold the waiting future in case of circuit return. if (executor instanceof ThreadlessExecutor) &#123; ((ThreadlessExecutor) executor).setWaitingFuture(future); &#125; // timeout check // 创建客户端的超时检查任务 时间轮去定时检查是否超时 timeoutCheck(future); return future; &#125; // timeoutCheck方法 private static void timeoutCheck(DefaultFuture future) &#123; // 超时时间的检查 TimeoutCheckTask task = new TimeoutCheckTask(future.getId()); future.timeoutCheckTask = TIME_OUT_TIMER.newTimeout(task, future.getTimeout(), TimeUnit.MILLISECONDS); &#125; 可以看到timeoutCheck方法中创建了一个TimeoutCheckTask（实现了TimerTask接口），然后利用时间轮调用newTimeout加入了一个定时任务。 客户端检查超时公用的时间轮：1234public static final Timer TIME_OUT_TIMER = new HashedWheelTimer( new NamedThreadFactory("dubbo-future-timeout", true), 30, TimeUnit.MILLISECONDS); TimeoutCheckTask的逻辑：1234567891011121314151617181920212223242526272829303132333435363738394041// 超时检查的任务 private static class TimeoutCheckTask implements TimerTask &#123; private final Long requestID; TimeoutCheckTask(Long requestID) &#123; this.requestID = requestID; &#125; // 超时检查的逻辑 在达到对应的超时时间触发 @Override public void run(Timeout timeout) &#123; // 根据requestId从Future缓存中获取future DefaultFuture future = DefaultFuture.getFuture(requestID); // if (future == null || future.isDone()) &#123; // 完成正常返回 return; &#125; // 否则响应超时 isSent区分是客户端执行超时，还是服务端的超时。 if (future.getExecutor() != null) &#123; future.getExecutor().execute(() -&gt; notifyTimeout(future)); &#125; else &#123; notifyTimeout(future); &#125; &#125; private void notifyTimeout(DefaultFuture future) &#123; // create exception response. // 客户端在超时之后创建一个超时的返回 Response timeoutResponse = new Response(future.getId()); // set timeout status. // 根据future的isSent确定状态是客户端响应超时 还是 服务端响应超时 timeoutResponse.setStatus(future.isSent() ? Response.SERVER_TIMEOUT : Response.CLIENT_TIMEOUT); timeoutResponse.setErrorMessage(future.getTimeoutMessage(true)); // handle response. // 响应超时 DefaultFuture.received(future.getChannel(), timeoutResponse, true); &#125; &#125; 与注册中心交互的失败重试1234567891011121314private void addFailedRegistered(URL url) &#123; // 如果注册失败 会添加到重试任务到时间轮 进行后面的异步重试 FailedRegisteredTask oldOne = failedRegistered.get(url); if (oldOne != null) &#123; return; &#125; FailedRegisteredTask newTask = new FailedRegisteredTask(url, this); oldOne = failedRegistered.putIfAbsent(url, newTask); if (oldOne == null) &#123; // never has a retry task. then start a new task for retry. // Failback 容错。 如果注册失败 启动一个时间轮去异步重试注册节点 （时间轮的一个应用） retryTimer.newTimeout(newTask, retryPeriod, TimeUnit.MILLISECONDS); &#125; &#125; 其中FailedRegisteredTask是TimerTask的实现，代表一个失败重新注册到注册中心的任务，内部就是调用了注册服务的逻辑（比如zk去创建临时节点）；retryTimer是一个时间轮，30毫秒去转动指针扫描槽来执行任务。 1234567891011// Timer for failure retry, regular check if there is a request for failure, and if there is, an unlimited retryprivate final HashedWheelTimer retryTimer;public FailbackRegistry(URL url) &#123; super(url); this.retryPeriod = url.getParameter(REGISTRY_RETRY_PERIOD_KEY, DEFAULT_REGISTRY_RETRY_PERIOD); // since the retry task will not be very much. 128 ticks is enough. // 利用时间轮注册重试和注册中心连接的任务 retryTimer = new HashedWheelTimer(new NamedThreadFactory("DubboRegistryRetryTimer", true), retryPeriod, TimeUnit.MILLISECONDS, 128);&#125;]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo时间轮</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ParNew和CMS垃圾回收器]]></title>
    <url>%2Fblog%2F2020%2F05%2F21%2FParNew%E5%92%8CCMS%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[垃圾收集算法分代收集理论JVM采用分代收集的算法，根据对象存活周期的不同将内存分为几块。堆内存分为新生代和老年代，可以根据各个年代的特点来选择合适的算法。 比如在新生代中，每次收集都会大量对象被回收，所以剩余很少，可以采用标记-复制算法，只需要很少的复制成本就可以完成垃圾收集，所以新生代都是采用复制算法。优点是新生代对象大量都被回收，效率高，复制之后内存可以用指针碰撞移动指针分配新的内存，比freelist方式产生空间碎片好的多。复制算法的缺点就是空间利用率会降低，S区只能有一块区域存放存活对象，但是一般Survivor区不会设置太大；还有个缺点就是如果分代年龄阈值设置过大可能会经历多次复制才能晋升到老年代。 而在老年代中，对象存活几率比较大，而且没有额外的空间可以再去晋升或者担保，所以必须选择标记-清除、标记-整理等算法进行垃圾回收。 这种算法比复制算法要慢10倍以上。 Remember Set和cardtable卡表在新生代做GCRoots可达性扫描过程中可能碰到跨代引用，比如老年代引用了新生代的对象，而如果直接去老年代扫描效率很低，引入Remember Set来记录非回收区到收集区的指针集合，避免把整个老年代加入到扫描的范围。 在很多垃圾收集器都有这样的问题，而Remember Set这样的结构就是解决跨代引用的问题。 cardtable卡表是RemembeSet的具体实现，卡表比如在新生代记录一个字节数组byte[]，再将老年代划分多个区域（卡页），字节数组记录对应老年代每个区域是否有跨代引用和地址，如果存在跨代引用，则叫做脏页，这样GC时可以通过卡表只扫描对应脏页内的对象，增加了扫描效率。 标记-复制算法为了解决效率问题，复制算法出现。将内存划分为大小相同的两块，每次只能用其中的一块。（联想年轻代中的Survivor区）。当这一块内存使用完之后，将GC之后存活的对象复制到另一块，把使用过的空间一次性清理掉，效率高，每次可以直接对一半内存进行直接回收。但是空间利用率低。好在年轻代大多数对象生命周期很短，所以每次GC之后的存活对象很少，所以对Survivor两块区域不需要占用年轻代过大的比例。 标记-清除算法两个阶段：标记和清除。标记即为利用GCRoots可达性分析标记存活的对象，统一回收清除未标记（可回收）的对象。 优点：空间利用率高，整块内存都可以使用。缺点： 效率问题（如果需要标记的对象很多，效率会受影响，且这过程如果伴随STW，对应用程序有影响） 空间问题（在对回收对象清除之后，会产生大量不连续的空间碎片，对下次要求连续内存的分配可能会产生影响） 标记-整理算法标记过程和标记-清除算法一样，也是标记GCRoots进行可达性分析寻找存活对象，然后在第二部不是直接清除回收垃圾对象，而是先让存活对象向一端移动，整理成连续的内存，然后清理掉剩余的内存。 这个算法标记过程还是可能存在效率问题。但是避免了标记-清除算法中的空间碎片问题，留下的空闲内存都是连续的空间。 垃圾收集器垃圾收集器是垃圾回收算法的具体实现。主流的垃圾收集器如下： Serial收集器Serial（串行）收集器是单线程收集器，在GC时会单线程GC且会STW直到收集结束。 Serial提供了新生代垃圾回收器，采用复制算法，如果使用需要配置参数-XX:+UseSerialGC。同时也提供了老年代的回收器，采用标记-整理算法，需要配置参数：-XX:+UseSerialOldGC。 Serial虽然是单线程的，但是优点是简单且高效（对比其他收集器的单线程版本），比如在单个CPU环境下可能是比较好的选择（不会频繁的上下文切换）。同时Serial Old也会作为CMS垃圾收集器在concurrent mode failure场景下退化的fullgc老年代的垃圾收集器；也可以搭配Parallel Scavenge收集器使用。 ParNew收集器ParNew是新生代的垃圾回收器，采用复制算法。可以用参数-XX:+UseParNewGC来控制使用ParNew作为新生代的垃圾收集器。如果设置了CMS作为老年代的垃圾收集器，那么默认新生代使用ParNew作为新生代进行搭配。 ParNew就是Serial的多线程版本，有多个线程回收新生代的垃圾对象。同时也是除了Serial之外可以和CMS进行分代配合的收集器，所以很多都选择ParNew+CMS这样的垃圾回收器的组合。 ParNew在进行回收时多个线程并行回收，但是这里还是会STW用户线程的，和Serial一样。可以使用+XX:ParallelGCThreads参数来设置GC线程的个数。其工作过程如下： 垃圾回收器中的并发、并行在讨论垃圾收集器的上下文语境中，并行和并发如下解释： 并行（parallel）：指多个线程并行的去GC，如ParNew新生代垃圾回收器，但是用户线程还是会被暂停STW。 并发（concurrent）：指GC线程和用户线程并发执行，不会暂停用户线程的执行，都去分不同的时间片。 Parallel Scavenge/Old收集器Parallel Scavenge是新生代垃圾回收器，可以使用-XX:+UseParallelGC参数指定使用。其也是采用复制算法、多线程进行parallel的gc。 工作过程和ParNew的多线程进行GC，STW用户线程一致，但在设计上有点区别。 而Parallel Old则是多线程GC的老年代垃圾回收器，采用的是标记-整理垃圾回收算法，同时也是更注重吞吐量，而不是尽力缩短GC的停顿时间。 GC的停顿时间的缩短是牺牲吞吐量和新生代空间来换取的：比如新生代调整小一些，收集肯定更快发生，这样新生代的GC会更频繁。比如原来每10s回收一次新生代，每次停顿100ms，现在就变成了每5s一次新生代回收，每次停顿70ms，吞吐量降低了，但是每次停顿时间变少了。 Parallel Scavenge和ParNew的区别 Parallel Scavenge更追求吞吐量（即CPU运行用户线程时间 / (CPU运行用户线程时间 + CPUGC的时间)），对后台计算和CPU敏感型的任务更友好。而ParNew等其他收集器更注重GC的停顿时间，即减少STW时间，对和用户交互的程序更友好。 Paralle Scavenge只能和Serial Old或者Parallel Old老年代的回收器搭配使用，不能和CMS搭配使用。而ParNew是可以和CMS搭配使用的。 Parallel Scavenge可以自适应调整新生代的Eden、Surrvivor区域的比例，参数是-XX:+UseAdaptiveSizePolicy。 CMS垃圾回收器CMS（Concurrent Mark Sweep）是老年代的垃圾回收器，其目标是获取最短停顿时间。也是HotSpot实现的真正意义上并发（Concurrent注意不是并行，意味着不会停顿用户线程）收集器，实现了在一定阶段让用户线程和GC线程一起工作。 CMS采用标记-清除算法，但是内部也提供了在清除之后整理防止出现内存碎片的功能和参数。 CMS的步骤过程CMS每个阶段的工作过程如下： CMS一定不会STW用户线程吗？CMS的并发（不STW用户线程）也只是发生在某些阶段的，而不是整个CMS的过程不会STW。比如初始标记和再次重新标记的过程都是会STW的，只不过这两个阶段会占用整个CMS GC的很小一部分时间，最多的还是在并发标记和并发清理阶段。 同时CMS在并发标记或者并发清理阶段，因为不会STW用户线程，可能又触发了old区的垃圾回收，即一次fullgc，那么会出现“Concurrent mode failure”，即会退化为单线程Serial Old垃圾回收器，会单线程的GC，且也会STW。 初始标记（会STW）初始标记会STW，其实标记一下和GC Roots直接关联到的对象，速度很快。 并发标记（GC线程和用户线程并发、耗时、标记会有误差）初始化标记之后，会对GCRoots直接关联的对象进行Traceing，即通过可达性分析去寻找引用链上的对象进行标记，这个过程是并发的，即GC线程和用户线程可以同时执行，不会STW。 当然这个阶段很耗费时间，因为是要寻找对象引用的链。 还有这个阶段因为不会STW用户线程，所以标记的对象可能会多标（比如用户线程已经释放了对象关联的GCroots，可能是栈帧中的局部变量），那么这些就会在这个阶段浮动垃圾；也可能少标（用户线程在可达性分析的完成之后，对象又关联上了GCroots的引用）。这部分依赖底层三色标记的算法和后面重新标记的过程来修正或者直接当做浮动垃圾（多标的场景） 预清理阶段理解CMS回收器的preclean阶段：https://blog.csdn.net/enemyisgodlike/article/details/106960687。主要是为了后面重新标记而提前清理cardTable(记录跨代引用)，和调整最终重新标记的时机。 重新标记（会STW）重新标记就是为了修正并发标记阶段因为用户线程继续运行导致标记产生变动的对象，这个阶段的时间一般会比初始标记的时间长，但远比并发标记阶段时间短。重新标记阶段会STW（肯定的不然又多标记或者漏标），依赖底层三色标记算法的增量更新算法（JVM中的赋值写屏障）。 并发清理（GC清理和用户线程并发执行）开启用户线程，和GC线程对未标记（垃圾对象）进行清理，如果设置了整理压缩内存的参数再去整理内存，这个过程对于三色标记法中标记黑色的对象不进行处理。 并发重置重置对象的一些信息，方便下次GC时重新标记。 CMS总结优点： 分成多个阶段，其中STW的时间占用很少，最大限度的减少了STW停顿时间，多线程和GC线程进行并发标记和并发清除，加快了效率。 缺点： 对CPU资源敏感。设置GC线程不当或者CPU资源紧张时，多个GC线程切换可能会抢占用户线程资源，使得应用总吞吐量变低，负载变高。 CMS在整个过程中会产生浮动垃圾，浮动垃圾即并发标记或者并发清理过程中用户线程可能产生的新的垃圾对象。这部分可以等待下一次gc再进行清理。（因为存在重新标记的过程，这部分浮动垃圾会比较少） CMS因为产生了浮动垃圾，且参数-XX:CMSInitiatingOccupancyFraction参数（老年代占用多少启动CMS GC）默认为92，假设在CMS过程中因为用户线程并发，预留的内存空间不足以容纳程序需要，则会出现Concurrent mode failure，即再进行一次full gc，此时CMS也会退化为Serial Old，即单线程回收，整个过程都STW，效率变得很低。 CMS本身采用“标记-清除”算法，这个算法本身可能导致大量的空间碎片，因为用户线程并发，在清理之后可能无法容纳新晋升的大对象的连续内存导致FullGC。JVM提供了清除之后压缩的两个参数：-XX:+UseCMSCompactAtFullCollection（在CMS之后开启压缩整理合并碎片）、-XX:CMSFullGCsBeforeCompaction（执行多少次CMS FullGC之后才进行压缩整理，默认为0，代表每次CMS之后都会整理碎片空间，因为碎片压缩整理也会STW，所以提供了这个值）。 CMS相关核心参数]]></content>
      <categories>
        <category>Java基础</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>垃圾回收器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM对象创建与内存分配]]></title>
    <url>%2Fblog%2F2020%2F05%2F20%2FJVM%E5%AF%B9%E8%B1%A1%E5%88%9B%E5%BB%BA%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%2F</url>
    <content type="text"><![CDATA[JVM对象创建的流程 1. 类加载的检查new指令可以是new对象、对象克隆、对象序列化等。 虚拟机会检查常量池中是否能定位到类的符号引用（字面量），以及这个类是否被类加载器加载、校验、准备、解析、初始化等工作。如果没有，则进行类加载的工作。 2. 分配内存从堆中划分一块区域为对象分配内存。划分有两种方式： 指针碰撞（默认使用）：内存是规整的，用过的内存放一边，空闲的再另一边，中间有个指针作为分界点，那么分配内存就是移动指针向空闲那边分配内存。 空闲列表（Free List）：内存不是规整的，空闲和已使用的交错，这时维护一个空闲列表来存空闲内存块，划分一块区域给对象实例。 这其中的并发问题解决： CAS失败重试，对分配的空间做同步处理 本地线程分配缓存（TLAB)：每个线程划分自己的一块缓冲区，-XX:+UseTLAB来控制大小，这样避免并发问题。 3. 初始化对分配到的内存初始化零值，这时成员变量字段会初始化为默认值，此时其实已经可以被访问到。 4. 设置对象头初始化零值之后，JVM对对象设置对象头。 JVM一个对象有三部分组成：对象头、实例数据、对齐补齐的区域（要求是8字节的倍数） 对象头是存放比如这个对象是哪个类的实例（KClass对象的类型指针）、MarkWord（一些记录信息比如hashcode、GC分代年龄、偏向锁id、锁标志、线程持有锁、偏向锁时间戳等）、数组长度（如果是数组对象） 5. 执行方法对成员变量进行赋值，且执行类的构造方法。对应着助记符invokespecial，比如下面代码对应的助记符：1234567891011public static void main(String[] args) &#123; Math math = new Math(); &#125; public static void main(java.lang.String[]); Code: 0: new #2 // class jvm/内存区域/Math 3: dup 4: invokespecial #3 // Method "&lt;init&gt;":()V 执行init方法 5: return java对象的指针压缩在64位操作系统中支持JVM的指针压缩。jvm配置参数：-XX: +UseCompressedOops。其中compressed–压缩、oop(ordinary object pointer)–c++中的对象指针。 为什么要进行指针压缩 在64位操作系统使用32位指针，内存使用会多1.5倍左右。比如32位的操作系统支持寻址内存最大是4g，而一个对象假设在64位操作系统中只需要33位来（即8g，夸张的假设），这里可以用压缩算法来存储对象，即指针压缩之后用4个字节就能表达对象，在真正使用时解压使用。 指针压缩的好处是减少对象大小，能承载更多的对象，减少GC的压力，且复制对象数据也更节省效率。即用4个字节即32位地址就能支持更大的内存。 当然对堆的大小有要求，小于4g是不需要指针压缩。jvm用去除高32位地址，即使用低虚拟地址空间；大于32g时压缩指针会失效，强制使用64位来进行寻址，所以也不建议堆内存特别大。 对象内存分配的细节对象内存分配的流程： 逃逸分析、标量替换和对象栈上分配大多数对象都是在堆上进行分配，对象没有被GCROOTS引用时依靠GC进行回收内存。但也不全是在堆上分配。JVM通过逃逸分析可以确定对象不会在外部访问，即不会发生逃逸即可在栈上分配，这样对象随着栈帧出栈而销毁，不需要进行GC。 逃逸分析的JVM参数：-XX:+DoEscapeAnalysis开启逃逸分析。 标量替换：通过逃逸分析确定对象不会被外部访问，会进一步尝试将对象进行标量（基本数据类型）的替换，将整个对象里的成员拆分为标量在栈帧或者寄存器上分配，这样不需要一大块内存来存放内存在栈上。 对应的JVM参数是：-XX:+EliminateAllocations。 demo:123456789101112131415161718192021222324252627282930313233/** * 逃逸分析和标量替换带来的栈上分配的优化 * 调用1亿次 大约需要1GB的内存 15m肯定会发生GC * * 堆大小20m 开启逃逸分析和标量替换 栈上分配对象 所以不会大量GC * -Xms20M -Xmx20M -Xmn10M -XX:+PrintGC -XX:+DoEscapeAnalysis -XX:+EliminateAllocations * * 下面这两种因为没有进行栈上分配 所以会大量GC * * 关闭逃逸分析 * -Xms20M -Xmx20M -Xmn10M -XX:+PrintGC -XX:-DoEscapeAnalysis -XX:+EliminateAllocations * 关闭标量替换 * -Xms20M -Xmx20M -Xmn10M -XX:+PrintGC -XX:-DoEscapeAnalysis -XX:-EliminateAllocations */public class AllocateInStackDemo &#123; public static void main(String[] args) &#123; long start = System.currentTimeMillis(); for (int i = 0;i &lt; 1000000000;i++) &#123; alloc(); &#125; long end = System.currentTimeMillis(); System.out.println("运行完毕" + (end - start)); &#125; private static void alloc() &#123; User user = new User(); // user只有标量变量 可以直接替换 user.setId(1); user.setName("aaa"); &#125;&#125; 在Eden区分配 MinorGC/YoungGC：年轻代（新生代）发生的垃圾收集动作，MinorGC非常频繁，回收速度也很快，应用中大多数对象都是在年轻代通过MinorGC被回收的。 MajorGC/FullGC：会回收年轻代、老年代、方法区（元空间）的垃圾。FullGC的速度比较慢，一般比MinorGC慢上10倍左右。 一般什么场景下会发生fullgc？ 老年代内存不足 方法区（元空间）内存不足 System.gc()调用时 老年代担保分配场景中，担保失败的场景（promotion failture），年轻代对象总和或者每次MinorGC之后晋升老年代的平均对象大小 &gt; 老年代剩余空间，则会进行一次fullgc来回收老年代的对象，来担保能晋升到老年代。 CMS垃圾回收器使用在并发标记或者并发清理阶段，产生的浮动垃圾导致没有足够的空间分配新的对象，出现Concurrent mode failure，会触发再一次FullGC清理堆空间，CMS也退化为Serial Old单线程垃圾收集器。 在堆内存的年轻代的Eden区分配对象才是大多数对象内存分配的途径。当Eden区没有足够空间分配对象，JVM会发起一次MinorGC。 对象新创建之后会分配在Eden区（没有超过大对象直接老年代分配的阈值），eden不满足大小会触发一次minorGC，剩余存活的对象会复制到Survivor区域（S0或者S1)，下一次eden区满了之后会再次触发minorGC，会回收eden和survivor所有垃圾对象，把剩余存活的对象再一次复制到另一块空的Survivor区域。 年轻代中Eden和S0、S1区默认8：1：1。这里因为新生代的对象都是很快消亡的（可能就是一次请求），所以Eden区尽量的大，而Survivor区是用来存储GC之后存活的复制对象的，够用即可。JVM参数-XX:+UseAdaptiveSizePolicy（默认开启）来自适应Eden和Survivor的比例。 大对象直接进入老年代JVM参数 -XX:PretenureSizeThreshold设置了直接进入老年代分配内存的对象大小， 控制了大对象（需要大量连续内存空间，比如字符串、数组）避免在年轻代中的Eden和S区中的复制算法降低效率。这个参数在Serial和ParNew两个垃圾收集器下有效。 比如设置：1-XX:PretenureSizeThreshold=1000000(单位是字节) -XX:+UseSerialGC 大于设置值大小的对象会直接分配至老年代的内存。 长期存活的对象进入老年代每个对象在对象头的MarkWord中记录了对象的GC分代年龄，对象在Eden分配完经历第一次MinorGC之后还存活，且能被Survivor容纳，将会被移动到Survivor区域（S0区），并设置对象的GC分代年龄为1。在Survivor区每经过一次MinorGC年龄都会+1。JVM参数：-XX:MaxTenuringThreshold来控制晋升到老年代的阈值。 对象动态年龄判断在MinorGC之后，会触发一个动态年龄判断机制。即MinorGC之后Survivor区有一批对象（剩余存活的），年龄1+年龄2+年龄3+…+年龄n的多个年龄对象总和超过了Survivor区的50%，此时会把年龄n以上的对象直接放入到老年代，这个规则其实就是希望长期存活的大对象提前进入到老年代，而不是在年轻代多次复制之后达到阈值再进入老年代。 老年代空间分配担保机制年轻代每次MinorGC之前都会计算下老年代的剩余空间。如果这个可用空间小于年轻代所有对象（包括未清理的垃圾对象）就会依赖一个JVM参数：-XX:-HandlePromotionFailure（jdk1.8默认设置了）的参数是否设置了，如果设置了，就会判断老年代当前可用内存大小，是否大于之前每一次MinorGC之后进入老年代的对象的平均大小，如果上一步结果是小于或者参数没有设置，则触发一次fullgc，让老年代和年轻代一起进行一次GC，如果回收完还是没有足够的空间存放新的对象则会发生OOM。 当然如果没有触发老年代空间分配担保机制，MinorGC之后剩余空间需要挪到老年代的对象还是大于可用空间，也会触发full gc，full gc之后如果还不够就会触发OOM。 整个过程如下： 对象内存回收引用计数法每个对象维护一个引用计数器，每当有一个地方引用其对象，计数器加1。计数器为0时，对象可以被回收。这个方法效率高，但是存在循环引用的场景，很难去解决。所以主流JVM没有采用这个算法来确定无用的对象。 可达性分析算法以GCROOTS为起点，从这些对象节点向下搜索引用的对象，能连通找到的对象都是非垃圾对象，其余未被标记的都是垃圾可回收对象。 能作为GCRoots的根节点： 线程栈的本地变量（局部变量）引用的对象。 本地方法（native方法）引用的对象。 元空间中类的静态变量。 元空间中常量引用的对象。 java中的引用常见的强引用就是内存中存在一块内存区域存放对象的地址值，则这个对象被内存地址引用。而其他引用出现的背景是当需要描述一些这些引用：内存足够的时候存活，内存不足够的时候可以被GC，许多缓存的场景可以用这些引用。 强引用这个没啥说的就是 object = new Object，只要强引用在，GC不会手机回收被引用的对象。 软引用SoftReference是描述有用但是不是必须的对象。在GC之后还没有足够可用的内存，会回收软引用的对象，如果还没有足够的空间会抛出内存溢出异常。 使用场景可以想想浏览器前进后退保留的页面内容，如果GC内存不足清除，再重新加载即可，不需要一直保留缓存。 弱引用WeakReference是弱引用，描述非必须对象，弱引用关联的对象只生存到下一次GC之前，无论是否内存足够，都会在GC时回收只有弱引用的对象。应用可以看看ThreadLocal中实现的ThreadLocalMap。 虚引用最弱的一种引用关系，虚引用不影响对象存活，也无法通过引用获取到对象实例。在GC时会收到一个通知，PhantomReference一个应用场景是jdk里DirectByteBuffer被回收之后，Cleaner会把分配的堆外内存释放了；netty里用来探测内存泄漏。当堆内的对象被GC时收到通知可以对堆外内存进行手动调用Unsafe进行回收。 finalize()方法对象宣告死亡需要两次标记的过程：（1）GCRoots可达性分析连通不到之后，即没被标记不可清除对象（2）没有覆盖finalize()方法或者已经执行过finalize()方法 这种才会真正GC。 在覆盖finalize()方法中如果把当前this引用关联上别的静态变量或者成员变量，则会逃过之后的GC，相当于给了一次逃亡的机会。这个方法不建议去覆盖。 元空间的回收元空间主要是对常量池和无用类的回收。 确定废弃常量池中的字面量也是看有没有对象引用常量池中的字面量，可以对常量进行废弃回归。 而无用类的确定十分苛刻： 该类对应的实例都被回收，堆中没有该类对应的对象实例。 加载该类的ClassLoader被回收。 对应的Class对象都没被使用，没有反射调用该类的方法。 满足这三个条件才会进行元空间中无用类的回收。且对于频繁动态代理、反射、动态JSP的系统，因为运行中动态生成类或者替换类加载器，要关注元空间的类卸载情况，避免元空间溢出。]]></content>
      <categories>
        <category>Java基础</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>内存分配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbofilter的路径问题]]></title>
    <url>%2Fblog%2F2020%2F05%2F20%2Fdubbofilter%E7%9A%84%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[自定义dubbofilter在使用dubbo框架的时候可以使用filter去实现一些拦截功能和调整拦截顺序，在每次调用的过程中，Filter的拦截都会被执行。当然除了Dubbo默认的filter，用户也可以自定义dubbo filter来实现对应的功能。这里记录一个遇到的spi文件路径问题。 问题现象在测试自定义一个dubbo filter之后，发现并没有生效。 对应的filter代码： 123456789101112131415@Activate(group = Constants.PROVIDER, order = Integer.MIN_VALUE)public class HelloFilter implements Filter &#123; /** * @param invoker * @param invocation * @return * @throws RpcException */ @Override public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException &#123; System.out.println("filter调用了"); return invoker.invoke(invocation); &#125;&#125; 对应的spi文件内容： 1dubboLoggerFilter=com.xxx.demo.xxx.filter.HelloFilter 经过排查之后发现代码写的没啥问题，但dubbo并没有加载这个自定义filter的spi。隐约感觉是路径问题。 问题解决因为是对应一个已生效的filter去设置的，所以当时看到已有项目中的文件是这样的： 所以在建立spi文件的文件目录时直接new了一个目录名字叫做META-INF.dubbo。但其实看官方blog中的介绍是要在maven资源文件下建立如下的结果的spi文件： 这里恍然大悟才发现自己的目录路径建立错误了。 这里其实是idea展示的一个坑，比如我在现在改对的基础上去建立META-INF.dubbo目录，其实是和正确目录展示是一样的： 但是在你提交git文件的时候，其实是能明显看到对应的差别的： 这里记录下踩到的这个坑。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo filter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubboSPI的实现]]></title>
    <url>%2Fblog%2F2020%2F05%2F20%2Fdubbo%E7%9A%84spi%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[dubbo的spi概述采用spi是为了更好的达到OCP原则（对扩展开放，对修改封闭）。dubbo采用微内核+插件的架构。内核部分功能稳定，面向功能的可拓展性实现都是由插件来完成的，内核只是管理插件和应用插件实现。这样更灵活。 dubbo就是采用spi来加载插件的。 SPI原理jdk中的spi使用需要在resource目录下的META-INFO/services下新建对应SPI接口名称为名字的文件，然后将实现类的全限类名作为文件内容。 其文件内容： 然后利用ServiceLoader接口去加载和使用对应的spi接口即可。123456789101112public class 测试jdk_spi &#123; @Test public void testJdkSPI() &#123; ServiceLoader&lt;IShot&gt; serviceLoader = ServiceLoader.load(IShot.class); Iterator&lt;IShot&gt; iterator = serviceLoader.iterator(); while (iterator.hasNext()) &#123; IShot next = iterator.next(); System.out.println(next.shot()); &#125; &#125;&#125; 输出：12i am a dogi am a cat 原理ServiceLoader接口在调用load时，会创建一个ServiceLoader对应的实例，其中维护了一个providers变量，是一个LinkedHashMap，其会将spi文件中的每个接口实现的名称作为key，具体实例化的实现作为value存储，并且会生成一个LazyIterator作为迭代器的实现。 在调用ServiceLoader迭代器的hasNext和next方法时，会调用到上边的Lazy迭代器，其就是去读取配置文件中的内容，保存到providers中。 123456789101112131415161718192021222324252627private S nextService() &#123; if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?&gt; c = null; try &#123; c = Class.forName(cn, false, loader); &#125; catch (ClassNotFoundException x) &#123; fail(service, &quot;Provider &quot; + cn + &quot; not found&quot;); &#125; if (!service.isAssignableFrom(c)) &#123; fail(service, &quot;Provider &quot; + cn + &quot; not a subtype&quot;); &#125; try &#123; S p = service.cast(c.newInstance()); providers.put(cn, p); return p; &#125; catch (Throwable x) &#123; fail(service, &quot;Provider &quot; + cn + &quot; could not be instantiated&quot;, x); &#125; throw new Error(); // This cannot happen &#125; dubbo中的SPIdubbo没有直接使用java的SPI来实现自己的插件加载，而是在SPI基础上进行改造。因为java的SPI需要加载文件中所有扩展点的实现， java SPI是需要加载文件中所有的实现类，会造成资源浪费，且不能动态加载某个实现类。 而dubbo的spi文件首先拓展了三个目录下： META-INF/services/ 目录：该目录下的 SPI 配置文件用来兼容 JDK SPI 。 META-INF/dubbo/ 目录：该目录用于存放用户自定义 SPI 配置文件。 META-INF/dubbo/internal/ 目录：该目录用于存放 Dubbo 内部使用的 SPI 配置文件。 其次dubbo的SPI文件的配置改为了KV形式，实现了只加载对应key的值的扩展点具体实现。 配置文件举例：1dubbo=org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol 这时因为扩展点的key：extensionName为dubbo，所以找到DubboProtocol这个Protocol接口的实现。 1. @Spi注解Dubbo中接口用@SPI注解注释时，标注为扩展点接口，其value值的作用是在加载Protocol接口实现时，如果没有明确指定扩展名，则取value值作为扩展名去加载spi文件中对应的实现类。 ExtensionLoader如何处理@SPI注解ExtensionLoader是SPI实现的核心工具类，对于每个扩展点接口都会有一个ExtensionLoader实例。同时还有一些静态字段作为缓存加载过的扩展点实现。 静态字段： strategies(LoadingStrategy[]类型)：LoadingStrategy接口有三个实现，对应是三个SPI配置文件的加载路径。其也都实现了优先级接口，优先级为： 1DubboInternalLoadingStrategy &gt; DubboLoadingStrategy &gt; ServicesLoadingStrateg EXTENSION_LOADERS (ConcurrentHashMap&lt;Class&lt;?&gt;, ExtensionLoader&lt;?&gt;&gt;类型)：表示type类型对应的extensionLoader实例映射缓存。 EXTENSION_INSTANCES：表示扩展实现类和其实例对象的映射 实例字段： type：当前的ExtensionLoader实例负责加载的扩展接口 objectFactory：所属的对象工厂，注入所依赖的其他扩展点接口时所用 cachedDefaultName(String类型)：默认扩展名。即@SPI接口的value值 cachedNames (ConcurrentHashMap&lt;Class&lt;?&gt;, String&gt;类型)：缓存了该ExtensionLoader实例加载的扩展实现类与扩展名之间的映射关系。 cachedClasses (Holder&lt;ConcurrentHashMap&lt;String, Class&lt;?&gt;&gt;&gt;类型)：缓存了扩展点名称和扩展点实现类之间的映射关系 cachedInstances (ConcurrentMap&lt;String, Holder&gt;类型)：缓存了该ExtensionLoader实例加载的扩展名与扩展实现对象之间的映射关系。 cachedAdaptiveInstance：缓存了adaptive扩展点实例 cachedAdaptiveClass：缓存该extensionLoader加载过程中直接标注@Adaptive注解的扩展实现类 cachedWrapperClasses：缓存该extensionLoader加载过程中的包装类wrapper实现 ExtensionLoader.getExtensionLoader() 方法会创建对应的ExtensionLoader对象：12345678910public static &lt;T&gt; ExtensionLoader&lt;T&gt; getExtensionLoader(Class&lt;T&gt; type) &#123; // 从缓存中找 ExtensionLoader&lt;T&gt; loader = (ExtensionLoader&lt;T&gt;) EXTENSION_LOADERS.get(type); if (loader == null) &#123; // 如果缓存为null 则初始化一个 再放入缓存 EXTENSION_LOADERS.putIfAbsent(type, new ExtensionLoader&lt;T&gt;(type)); loader = (ExtensionLoader&lt;T&gt;) EXTENSION_LOADERS.get(type); &#125; return loader; &#125; 这里的new ExtensionLoader就是初始化type和objectFactory两个字段。 初始化完ExtensionLoader实例之后，可以根据getExtension方法用name加载扩展点：1234567891011121314151617181920212223242526public T getExtension(String name) &#123; // getOrCreateHolder()方法中封装了查找cachedInstances缓存的逻辑 Holder&lt;Object&gt; holder = getOrCreateHolder(name); Object instance = holder.get(); if (instance == null) &#123; // double-check防止并发问题 synchronized (holder) &#123; instance = holder.get(); if (instance == null) &#123; // 根据扩展名从SPI配置文件中查找对应的扩展实现类 instance = createExtension(name); holder.set(instance); &#125; &#125; &#125; return (T) instance; &#125;// getOrCreateHolder方法： private Holder&lt;Object&gt; getOrCreateHolder(String name) &#123; Holder&lt;Object&gt; holder = cachedInstances.get(name); if (holder == null) &#123; cachedInstances.putIfAbsent(name, new Holder&lt;&gt;()); holder = cachedInstances.get(name); &#125; return holder; &#125; 看到是调用了createExtension(name) 来实例化扩展点实现类：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 获取 cachedClasses 缓存，根据扩展名从 cachedClasses 缓存中获取扩展实现类。 * 如果 cachedClasses 未初始化，则会扫描前面介绍的三个 SPI 目录获取查找相应的 SPI 配置文件， * 然后加载其中的扩展实现类，最后将扩展名和扩展实现类的映射关系记录到 cachedClasses 缓存中。 * 这部分逻辑在 loadExtensionClasses() 和 loadDirectory() 方法中。 * * 根据扩展实现类从 EXTENSION_INSTANCES 缓存中查找相应的实例。如果查找失败，会通过反射创建扩展实现对象。 * * 自动装配扩展实现对象中的属性（即调用其 setter）。这里涉及 ExtensionFactory 以及自动装配的相关内容， * * 自动包装扩展实现对象。这里涉及 Wrapper 类以及自动包装特性的相关内容 * * 如果扩展实现类实现了 Lifecycle 接口，在 initExtension() 方法中会调用 initialize() 方法进行初始化。 * @param name * @return */ @SuppressWarnings(&quot;unchecked&quot;) private T createExtension(String name) &#123; // 获取扩展名对应的扩展实现类 Class&lt;?&gt; clazz = getExtensionClasses().get(name); if (clazz == null) &#123; throw findException(name); &#125; try &#123; // 从扩展实现类和其实例对象中获取 T instance = (T) EXTENSION_INSTANCES.get(clazz); if (instance == null) &#123; // newInstance放入缓存中 EXTENSION_INSTANCES.putIfAbsent(clazz, clazz.newInstance()); instance = (T) EXTENSION_INSTANCES.get(clazz); &#125; // 处理依赖的其他扩展点实现 调用了setter方法 injectExtension(instance); // 实现wrapper包装 Set&lt;Class&lt;?&gt;&gt; wrapperClasses = cachedWrapperClasses; if (CollectionUtils.isNotEmpty(wrapperClasses)) &#123; for (Class&lt;?&gt; wrapperClass : wrapperClasses) &#123; // 遍历全部wrapper类包装到当前的扩展点实现 instance = injectExtension((T) wrapperClass.getConstructor(type).newInstance(instance)); &#125; &#125; // 初始化instanced对象 如果扩展点实现了LifeCycle接口的话 initExtension(instance); return instance; &#125; catch (Throwable t) &#123; throw new IllegalStateException(&quot;Extension instance (name: &quot; + name + &quot;, class: &quot; + type + &quot;) couldn&apos;t be instantiated: &quot; + t.getMessage(), t); &#125; &#125; 2. @Adaptive注解和适配器@Adaptive表示自适应的扩展点实现。 当@Adaptive注解在类上注解时，表示此类为扩展实现，在SPI中使用并不多，只使用在了ExtensionFactory接口上。 可以看到ExtensionFactory接口的自适应实现AdaptiveExtensionFactory即为ExtensionLoader.getAdaptiveExtension()方法的返回值，即注解在类上即为自适应实现，且会缓存在ExtensionLoader实例中的cachedAdaptiveClass变量中。 AdaptiveExtensionFactory内部逻辑也比较简单，即根据注入的其他两个ExtensionFactory具体实现去加载对应的扩展点实现。一个是Dubbo SPI自适应扩展点实现加载，一个是Spring上下文获取bean。 当@Adaptive注解在方法上时，Dubbo会动态代理生成Adaptive实现类（比如Transporter$Adaptive)，此动态代理类也会实现扩展点接口。 代理类中的逻辑也是根据@Adaptive注解中值作为从url获取扩展名称的key，然后再根据ExtensionLoader获取扩展实现类。 12345678910111213141516171819public class Transporter$Adaptive implements Transporter &#123; public org.apache.dubbo.remoting.Client connect(URL arg0, ChannelHandler arg1) throws RemotingException &#123; // 必须传递URL参数 if (arg0 == null) throw new IllegalArgumentException(&quot;url == null&quot;); URL url = arg0; // 确定扩展名，优先从URL中的client参数获取，其次是transporter参数 // 这两个参数名称由@Adaptive注解指定，最后是@SPI注解中的默认值 String extName = url.getParameter(&quot;client&quot;, url.getParameter(&quot;transporter&quot;, &quot;netty&quot;)); if (extName == null) throw new IllegalStateException(&quot;...&quot;); // 通过ExtensionLoader加载Transporter接口的指定扩展实现 Transporter extension = (Transporter) ExtensionLoader .getExtensionLoader(Transporter.class) .getExtension(extName); return extension.connect(arg0, arg1); &#125; ... // 省略bind()方法 &#125; 以上这两种为自适应的适配器实现。获取适配器的代码为getAdaptiveExtension()方法：123456789101112131415161718192021222324252627282930313233343536public T getAdaptiveExtension() &#123; // 从缓存中取 Object instance = cachedAdaptiveInstance.get(); if (instance == null) &#123; if (createAdaptiveInstanceError != null) &#123; throw new IllegalStateException(&quot;Failed to create adaptive instance: &quot; + createAdaptiveInstanceError.toString(), createAdaptiveInstanceError); &#125; synchronized (cachedAdaptiveInstance) &#123; // double check instance = cachedAdaptiveInstance.get(); if (instance == null) &#123; try &#123; // 创建自适应的扩展点实现 instance = createAdaptiveExtension(); // 缓存 cachedAdaptiveInstance.set(instance); &#125; catch (Throwable t) &#123; createAdaptiveInstanceError = t; throw new IllegalStateException(&quot;Failed to create adaptive instance: &quot; + t.toString(), t); &#125; &#125; &#125; &#125; // createAdaptiveExtension方法中调用的getAdaptiveExtensionClass方法 private Class&lt;?&gt; getAdaptiveExtensionClass() &#123; // 触发loadClass 内部如果有直接加了@Adaptive注解的扩展点实现，则会维护到cacheAdaptiveClass变量中 getExtensionClasses(); if (cachedAdaptiveClass != null) &#123; return cachedAdaptiveClass; &#125; // 动态代理类选择出的扩展点实现也维护在cacheAdaptiveClass变量中 return cachedAdaptiveClass = createAdaptiveExtensionClass(); &#125; 3. 自动包装特性扩展点的实现中可能有很多通用逻辑，dubbo SPI中的自动包装特性将多个扩展实现类的公共逻辑抽象到Wrapper类中，Wrapper类和普通扩展点实现一样，也实现了扩展接口，在获取真正的扩展对象时，在外面包一层Wrapper对象，是装饰器模式的实现。 判断是否为Wrapper的实现：12345678910private boolean isWrapperClass(Class&lt;?&gt; clazz) &#123; try &#123; // 检查是否是wrapper包装方式：是否有当前扩展点接口为参数的构造函数 // wrap类是为了解决多个扩展点实现的公共逻辑 clazz.getConstructor(type); return true; &#125; catch (NoSuchMethodException e) &#123; return false; &#125; &#125; 在加载spi文件时，会去缓存当前扩展点的wrapper实现类到一个集合变量中：cachedWrapperClasses，然后在之后遍历wrapper实现包装：1234567Set&lt;Class&lt;?&gt;&gt; wrapperClasses = cachedWrapperClasses;if (CollectionUtils.isNotEmpty(wrapperClasses)) &#123; for (Class&lt;?&gt; wrapperClass : wrapperClasses) &#123; instance = injectExtension((T) wrapperClass .getConstructor(type).newInstance(instance)); &#125; &#125; 4. 自动装配特性自动装配在injectExtension()方法中。其会扫描所有setter方法，并根据setter方法的名称以及参数的类型，加载相应的扩展实现，然后反射调用setter方法填充属性。 1234567891011121314151617181920212223private T injectExtension(T instance) &#123; if (objectFactory == null) &#123; // 检测objectFactory字段 return instance; &#125; for (Method method : instance.getClass().getMethods()) &#123; ... // 如果不是setter方法，忽略该方法(略) if (method.getAnnotation(DisableInject.class) != null) &#123; continue; // 如果方法上明确标注了@DisableInject注解，忽略该方法 &#125; // 根据setter方法的参数，确定扩展接口 Class&lt;?&gt; pt = method.getParameterTypes()[0]; ... // 如果参数为简单类型，忽略该setter方法(略) // 根据setter方法的名称确定属性名称 String property = getSetterProperty(method); // 加载并实例化扩展实现类 Object object = objectFactory.getExtension(pt, property); if (object != null) &#123; method.invoke(instance, object); // 调用setter方法进行装配 &#125; &#125; return instance; &#125; 5. @Activate注解和自动激活特性以Filter接口为例，扩展点的实现非常多，不同场景下需要不同Filter一起执行，根据配置决定哪些场景下哪些Filter自动激活且加入到拦截链中就是@Activate注解的作用。 group：是Provider端还是Consumer端的 value：修饰的实现类只在URL参数指定key时才会激活 order：排序 对@Activate注解的扫描在loadClass对自动激活的注解进行扫描：1234567891011121314151617181920212223private void loadClass()&#123; if (clazz.isAnnotationPresent(Adaptive.class)) &#123; // 处理@Adaptive注解 cacheAdaptiveClass(clazz, overridden); &#125; else if (isWrapperClass(clazz)) &#123; // 处理Wrapper类 cacheWrapperClass(clazz); &#125; else &#123; // 处理真正的扩展实现类 clazz.getConstructor(); // 扩展实现类必须有无参构造函数 ...// 兜底:SPI配置文件中未指定扩展名称，则用类的简单名称作为扩展名(略) String[] names = NAME_SEPARATOR.split(name); if (ArrayUtils.isNotEmpty(names)) &#123; // 将包含@Activate注解的实现类缓存到cachedActivates集合中 cacheActivateClass(clazz, names[0]); for (String n : names) &#123; // 在cachedNames集合中缓存实现类-&gt;扩展名的映射 cacheName(clazz, n); // 在cachedClasses集合中缓存扩展名-&gt;实现类的映射 saveInExtensionClass(extensionClasses, clazz, n, overridden); &#125; &#125; &#125; &#125; getActivateExtension方法在此方法中： 如果传入配置没有-default配置，会触发写入自动激活的缓存。 然后遍历需要自动激活的扩展点接口，如果符合group（provider端或consumer端），并且没有出现在names配置的和被去除的，则加载激活扩展点实现，放入到activateExtensions且sort方法排序。 遍历传入的filter配置（这里和配置文件的顺序保持一致），会处理与default扩展点实现（上一步加载的自动激活扩展点实现）的顺序和配置保持一致。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public List&lt;T&gt; getActivateExtension(URL url, String[] values, String group) &#123; List&lt;T&gt; activateExtensions = new ArrayList&lt;&gt;(); // names是dubbo配置传入的顺序 List&lt;String&gt; names = values == null ? new ArrayList&lt;&gt;(0) : asList(values); if (!names.contains(REMOVE_VALUE_PREFIX + DEFAULT_KEY)) &#123; // 无-default // 触发 cachedActivate缓存字段的加载 getExtensionClasses(); for (Map.Entry&lt;String, Object&gt; entry : cachedActivates.entrySet()) &#123; String name = entry.getKey(); // 扩展名 Object activate = entry.getValue(); // @Activate注解 String[] activateGroup, activateValue; if (activate instanceof Activate) &#123; activateGroup = ((Activate) activate).group(); activateValue = ((Activate) activate).value(); &#125; else if (activate instanceof com.alibaba.dubbo.common.extension.Activate) &#123; activateGroup = ((com.alibaba.dubbo.common.extension.Activate) activate).group(); activateValue = ((com.alibaba.dubbo.common.extension.Activate) activate).value(); &#125; else &#123; continue; &#125; if (isMatchGroup(group, activateGroup) // 匹配group &amp;&amp; !names.contains(name) // 没有出现在names中 走默认激活的 &amp;&amp; !names.contains(REMOVE_VALUE_PREFIX + name) // 未在配置中去除的 &amp;&amp; isActive(activateValue, url)) &#123; // 检查url中是否出现指定的key // 加载扩展实现的实例对象 这些是不在传入的 names 里的且被去除掉的 activateExtensions.add(getExtension(name)); &#125; &#125; // 对不在filter配置中加载的激活扩展点进行排序 activateExtensions.sort(ActivateComparator.COMPARATOR); &#125; List&lt;T&gt; loadedExtensions = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; names.size(); i++) &#123; String name = names.get(i); if (!name.startsWith(REMOVE_VALUE_PREFIX) // -开头的不加载 -开头的直接跳过 因为在上边已经过滤了 &amp;&amp; !names.contains(REMOVE_VALUE_PREFIX + name)) &#123; if (DEFAULT_KEY.equals(name)) &#123; // 这里在default之前的都会放在上边加载过的默认自动激活扩展点之前 if (!loadedExtensions.isEmpty()) &#123; // 按照顺序 把自定义的扩展添加 到 默认扩展集合之前 activateExtensions.addAll(0, loadedExtensions); loadedExtensions.clear(); &#125; &#125; else &#123; // 根据扩展名去加载对应的扩展点实现类 loadedExtensions.add(getExtension(name)); &#125; &#125; &#125; if (!loadedExtensions.isEmpty()) &#123; // 在default之后的会加载到default之后 activateExtensions.addAll(loadedExtensions); &#125; return activateExtensions; &#125; 比如有如下几个Filter 传入filter配置是为Provider端的：”demoFilter3、-demoFilter2、default、demoFilter1”。那么最终Filter链的结果是： [demoFilter3, demoFilter6, demoFilter4, demoFilter1]。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo SPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存模型]]></title>
    <url>%2Fblog%2F2020%2F04%2F20%2FJVM%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[JVM的整体结构和内存模型 关于方法区和元空间 jdk1.8之前存放方法元信息、Class信息、常量池等的为方法区，在堆内存储。而在1.8之后废弃了永久代，替换为元空间，常量池和静态变量并入堆内存储，MetaSpace来存储类、方法元信息，同时使用了本地内存来存储。 相当于1.8版本之后，方法区一部分归到堆内存中，一部分移动到堆外内存，受机器本地内存大小限制。不过设置了Metaspace参数也会在内存不足时OOM。 JVM内存参数设置（简要说明）运行时数据区的一些基本参数 -Xms：堆的最小值参数 -Xmx：堆的最大值参数 -Xmn：新生代的内存大小 -XX: MaxDirectMemorySize：直接内存的最大值。即NIO使用的堆外内存的最大值。 这里注意heap dump不会记录堆外内存的移除，但也可以造成OOM，现象是dump文件很小，即堆内存其实使用很少。 -XX：MetaspaceSize：设置元空间触发fullgc的初始阈值（元空间没有固定初始值大小）。默认不设置为21M。到达该值就会触发full gc进行类型卸载，同时收集器会对该值进行调整，如果释放了大量空间，则调小；如果释放了很少的空间，则在不超过-XX: MaxMetaspaceSize的情况下适当提高此值。 在jdk1.8之前的永久代参数-XX: PermsSize参数标识为初始化的容量，1.8之后调整为元空间后不一样 -XX: MaxMetaspaceSize：元空间最大值，默认是-1，不限制的话，受限于直接内存大小。 -Xss：线程栈的大小（默认为1M） 常量池Class常量池和运行时常量池Class文件中包含常量池，也包含字段、方法、接口、类版本等描述信息。常量池用于存放编译期间的字面量和符号引用。 当常量池字面量和符号引用被加载到内存中，符号才会有内存的地址，常量池就是运行时常量池。而动态链接的过程就是把运行时常量池的符号引用变为直接内存地址引用。 常量池在1.8之后从永久代（元空间）拿出，移动到了堆内存。 字符串常量池创建字符串对象时，做了一些优化： 如果字符串常量在常量池中存在，返回此常量的地址引用； 如果不存在，则实例化改字符串放入常量池中，是字面量的引用直接返回，是new String()这种则会在堆内新创建一个对象。 字符串操作 字面量直接赋值1String a = "aaa"; 这样会用equals()方法判断字符串常量池是否存在，如果存在返回引用，不存在创建一个放入字符串常量池之后返回引用。 new String()复制1String a = new String("aaa"); 这里也会判断字符串常量池和堆中是否有这个对象，没有则都创建，然后返回堆中的对象引用。也就是可能会创建两个对象。 intern方法12String s1 = "aaa";String s2 = s1.intern(); 一个native方法，调用intern()时如果字符串常量池中存在常量，则返回池中的引用地址，否则将池中的地址直接指向堆中字符串对象（例子是s1）的地址。 常见面试题 关于String是不可变的除非下面的a、b、c三个变量被final修饰，即变为常量，那么执行String s1 = a + b + c时会编译为常量进行优化，也就是和String s = “a” + “b” + “c” 字面量直接相加一样，都是字符串常量池中常量对象的引用。 为什么是不可变的？ 安全。最常用的String内部char[]数组是final的，规避了直接改变对象内容，封装了内部数据。（享元模式） 线程安全，final在要求构造函数中初始化char[]数组 hashcode初始化时确定，String很适合作为hash表的key。 彩蛋：比较对象时为什么要重写hashcode()方法equals()方法相等，则对象一定相等，为什么还要hashcode方法呢？ 因为hashcode效率高，一般约束都是equals()方法相等，则hashcode()一定相等；而hashcode()不相等，一定不是一个对象。 HashSet这种数据结构是去重的，比较时候为了效率如果hashcode不相等，那么即使equals方法相等也会认为不是一个对象。]]></content>
      <categories>
        <category>Java基础</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM类加载机制详解]]></title>
    <url>%2Fblog%2F2020%2F04%2F20%2FJVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[类加载的过程通过Java命令执行代码的大体流程： 其中loadClass的过程如下：加载 -&gt; 验证 -&gt; 准备 -&gt; 解析 -&gt; 初始化 -&gt; 使用 -&gt; 卸载 加载：在硬盘上查找并通过IO读入字节码文件，使用类时才会记载，调用类的main()方法、new类的对象等等。在加载阶段会在内存生成一个代表这个类的java.lang.Class对象，作为方法区这个类各种数据的访问入口。 验证：校验字节码文件的正确性 准备：给类的静态变量初始化内存并复制默认值 解析：将符号引用替换为直接引用，将静态方法（符号引用比如main()方法）替换为指向数据所存内存的指针或者句柄等（直接引用），这种叫静态链接过程（类加载期间完成）。动态链接是在程序运行期间完成的将符号引用替换为直接引用。 初始化：对类的静态变量初始化为指定的值，且加载static代码块。 类被加载到方法区之后主要包含 运行时常量池、类型信息、字段信息、方法信息、类加载器的引用、对应class实例的引用等信息。其中Class对象是开发人员访问方法区中类定义的入口和切入点。 类的初始化对于类的初始化只有在主动引用的场景下才会进行初始化操作。即类加载了（取决jvm的实现）但类不一定去初始化。主动引用有五种场景： new实例、读取设置类的static字段、调用类的静态方法时 对类进行反射调用时 初始化一个类时发现父类没有初始化，触发父类的初始化 main方法调用时要对类进行初始化 jdk1.7之后对动态语言支持，MethodHandler实例解析结果句柄对应的类没有初始化时要初始化。 这里注意： 对应static final修饰的常量被调用不会触发主动引用。但是运行时常量是会触发类初始化的，比如下面这样的运行时常量： 12// 运行时常量是特殊的 不会加入到对应类的常量池中 会主动调用常量所在的类 导致类初始化 对应的助记符也是getstatic助记符static final String rom = UUID.randomUUID().toString(); 引用类型数组new出来不会造成引用类的初始化，这里比较特殊，会JVM自己生成一个数组对象，不算主动引用。 类初始化换个角度来说就是对类所有变量赋值和执行static代码块的过程，即执行类构造器（不是构造函数）()方法的过程。 ()方法是由编译器自动收集类所有类变量的赋值动作和静态static块合并产生的，其在执行子类的clinit()方法之前保证父类的clinit()方法已经执行。虚拟机保证对一个类的clinit()方法在多线程 环境下正确的加锁、同步、只有一个线程会执行clinit()方法（所以单例模式中有静态变量初始化这一个例子）。 类加载器和双亲委派机制。jdk自带的类加载器： 引导类加载器：负责加载支撑JVM运行的位于jre的lib目录下的核心类库，比如rt.jar、charsets.jar等。 扩展类加载器：负责加载支撑JVM允许的位于jre的lib目录下的ext扩展目录中的jar包类 应用程序类加载器：负责加载ClassPath路径下的类包，主要是开发人员写的类。 关于类加载器的一个demo：1234567891011121314151617181920212223242526272829303132333435public class TestClassLoader &#123; public static void main(String[] args) &#123; System.out.println(String.class.getClassLoader()); // 输出null，因为引导类加载器是由C++实现的 不是直接在Launcher中的内部类 System.out.println(DESKeyFactory.class.getClassLoader()); // ext扩展类加载器 System.out.println(TestClassLoader.class.getClassLoader()); // app类加载器 加载开发者写在classpath的class // 看下维护的parent父加载器 ClassLoader systemClassLoader = ClassLoader.getSystemClassLoader(); // systemClassLoader默认是app类加载器 ClassLoader extClassLoader = systemClassLoader.getParent(); ClassLoader bootstarpClassLoader = extClassLoader.getParent();// 在逻辑上是引导类加载器 但是引导类加载器是由c++实现的。 这里ext的parent是null System.out.println("systemClassLoader:" + systemClassLoader); System.out.println("extClassLoader:" + extClassLoader); System.out.println("bootstarpClassLoader:" + bootstarpClassLoader); // 线程上下文保存的类加载器 有些SPI场景需要通过这个来打破双亲委派原则 ClassLoader contextClassLoader = Thread.currentThread().getContextClassLoader(); //线程上下文类加载器默认是appClassLoader System.out.println(contextClassLoader); // 加载路径 APP类加载器对应 classpath的路径 这里注意也会有rt.jar和ext的包路径 但是因为双亲委派会优先给引导类加载器去加载核心jar包 自己写的类在target下最终会在appClassLoader中加载 System.out.println(System.getProperty("java.class.path")); &#125;&#125;// 输出nullsun.misc.Launcher$ExtClassLoader@2c7b84desun.misc.Launcher$AppClassLoader@18b4aac2systemClassLoader:sun.misc.Launcher$AppClassLoader@18b4aac2extClassLoader:sun.misc.Launcher$ExtClassLoader@2c7b84debootstarpClassLoader:nullsun.misc.Launcher$AppClassLoader@18b4aac2省略appClassLoader路径的输出 类加载器初始化过程AppClassLoader和ExtClassLoader是JVM启动类sun.misc.Launcher的内部类，其都继承了URLClassLoader。在JVM启动时，虚拟机会创建一个Launcher类的实例，且保证全局单例，而在Launcher的构造函数中初始化了ExtClassLoader和AppClassLoader：1234567891011121314151617181920 public Launcher() &#123; Launcher.ExtClassLoader var1; try &#123; // 内部DCL单例创建ExtClassLoader var1 = Launcher.ExtClassLoader.getExtClassLoader(); &#125; catch (IOException var10) &#123; throw new InternalError("Could not create extension class loader", var10); &#125; try &#123; // 内部DCL创建AppClassLoader var1（extClassLoader）作为参数传入赋值给parent变量 this.loader = Launcher.AppClassLoader.getAppClassLoader(var1); &#125; catch (IOException var9) &#123; throw new InternalError("Could not create application class loader", var9); &#125; // 线程上下文加载器 Thread.currentThread().setContextClassLoader(this.loader); // 省略代码&#125; 可以看到先初始化了一个extClassLoader，然后将ext作为参数传入了AppClassLoader的构造过程中，其中APP类加载器中的parent父类加载器会赋值为ext类加载器，方便后续的双亲委派。 JVM默认设置线程上下文的类加载器为AppClassLoader。ClassLoader.getSystemClassLoader()返回的也是APP类加载器。 双亲委派机制由上面的代码可以看到jdk自带的类加载器在逻辑上是有继承父子关系的,而类加载器的加载是存在双亲委派的机制。 双亲委派机制：加载类时会先委托给父类加载器去加载，找不到再委托给上层父类加载器去加载，如果所有父类加载器在自己的加载路径下找不到目标类，则此时才在子类加载器加载目标类。 来看下代码是怎么实现双亲委派机制的：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// AppClassLoader.loadClass方法 public Class&lt;?&gt; loadClass(String var1, boolean var2) throws ClassNotFoundException &#123; int var3 = var1.lastIndexOf(46); if (var3 != -1) &#123; SecurityManager var4 = System.getSecurityManager(); if (var4 != null) &#123; var4.checkPackageAccess(var1.substring(0, var3)); &#125; &#125; if (this.ucp.knownToNotExist(var1)) &#123; Class var5 = this.findLoadedClass(var1); if (var5 != null) &#123; if (var2) &#123; this.resolveClass(var5); &#125; return var5; &#125; else &#123; throw new ClassNotFoundException(var1); &#125; &#125; else &#123; // 直接委托给super.loadClass方法 return super.loadClass(var1, var2); &#125; &#125; // ClassLoader中的loadClass方法 protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded // 检查当前类加载器是否已经加载了该类 如果存在直接返回 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; // 如果此类加载器还有parent类加载器 则委托给父类loadClass c = parent.loadClass(name, false); &#125; else &#123; // parent为null 让引导类加载器去加载 这里相当于extClassLoader的parent是引导类加载器 c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); // 都会调用URLClassLoader的findClass方法在加载器的类路径里查找并加载该类 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; // 返回类 return c; &#125; &#125; 从代码里可以看到，双亲委派就是委托给parent父类加载器变量来load的，如果parent为null则委托给引导类加载器。如果父类加载器加载失败，会回到子类加载器在findClass方法中根据name加载对应的类。如果自定义类加载器想符合双亲委派机制，可以直接重写findClass这个方法，且默认自定义类加载器的父加载器是AppClassLoader，这样自定义类加载器会遵循双亲委派机制来加载类。 双亲委派的优点 沙箱安全机制：自己写的java.lang.String类不会被加载，因为优先委托给引导类加载器来加载，只会加载rt.jar中的核心api，这样可以防止核心API被篡改。 避免类的重复加载：父类加载器已经加载过的类，因为双亲委派不会被加载第二次。保证加载类的唯一性。 双亲委派机制的打破双亲委派机制本身也存在限制和缺点。双亲委派使得类有了层级划分（跟随类加载器），越基础的类越上层的类加载器去加载，但是如果基础的类要调用用户写的类时（不在加载基础类加载器包路径下），这个模型就不灵活。或者比如在Tomcat这种容器中，想实现不同war包之间的隔离和部分共享，双亲委派机制也不能满足。所以需要打破双亲委派机制。 打破方式： 利用线程上下文类加载器来加载对应类。比如SPI的应用，JNDI或者JDBC。对应JNDI来说，基础的资源查找和管理都在rt.jar中由启动类加载器加载，而其需要加载的spi文件有的在classpath里，这时就依赖线程上下文类加载器来加载，打破了双亲委派机制。 自己实现自定义的类加载器（默认的parent为应用程序类加载器），重写loadClass方法，不委托对应的parent去加载。 Tomcat打破双亲委派的机制Tomcat作为web容器要解决的问题： web容器可能要部署不同的war包，要依赖三方包不同的版本，不能要求同一个类只加载了一次，要实现war包之间的隔离。 可以共享的类在不同的war包也不应该加载10次，有web程序可共享的类。 server自己和web程序依赖的类应该是隔离的，为了安全。 web容器要支持jsp的修改，支持jsp的热更新。 显然，默认的双亲委派机制是支持不聊tomcat这个需求的。第一点和第三点其实都是要多个相同全限定类名的类加载多次实现隔离，这显然不能去双亲委派。而jsp的要求热更新，jsp对应的class文件在加载之后变更，可以卸载jsp的类加载器，再重新去生成一个类加载器去加载对应的jsp文件实现热更新。 在tomcat中有三组目录（/common/、/server/、/shared/*），分别对应所有共享、server容器自己所用、web程序共享，当然每个web程序都有自己的/WEB-INF/目录来存放自己的web应用程序的资源。tomact的类加载器依赖了这些目录的功能，如下：]]></content>
      <categories>
        <category>Java基础</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>类加载机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类加载系列之理解类加载]]></title>
    <url>%2Fblog%2F2020%2F04%2F12%2F%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%B3%BB%E5%88%97%E4%B9%8B%E7%90%86%E8%A7%A3%E7%B1%BB%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[类的阶段在Java代码中，类型的加载、连接和初始化都是在程序运行期完成的。类有以下几个阶段： 类加载：即查找并加载类的二进制数据 类连接： 验证：确保被加载类的正确性 准备：为类的静态变量分配内存，并将其初始化为默认值 解析：类的符号引用转换为直接引用 类初始化：为类的静态变量赋予正确的初始值，运行static快中的代码。 类使用 类卸载 类加载类加载：类的加载是指将类的.class文件的二进制数据读入内存中，将其放在运行时数据区的方法区中，然后在内存中创建了一个java.lang.Class对象（HotSpot中这个Class对象在方法区）用来封装类在方法区的数据结构。（比如反射就是用的Class对象） 加载.class文件的方式 从本地系统磁盘直接加载（classpath下的文件） 通过网络下载.class文件 通过zip、jar等归档文件加载.class文件（多用于第三方的jar包） 将java源文件动态编译成.class文件（比如动态代理，运行期生成代理类的类加载过程） 类的初始化java程序对类的使用方式有两种： 主动使用 被动使用 类初始化的时机所有的jvm实现都必须保证在每个类或接口都被程序“首次主动使用”时才进行初始化。（这里是类初始化） 使用类主动使用类有下面几种方式： 创建类的实例 访问类或者接口的静态变量，或者对这个静态变量进行赋值。 调用类的静态方法 反射加载一个类 初始化一个类的子类，则父类也算被主动调用。 启动类，比如test 动态语言支持，java.lang.invoke.MethodHandle实例的解析结果的句柄对应类没有初始化，则进行初始化 被动使用：除了主动使用类的方式使用类或接口，都会被看做是类的被动使用，不会导致类的初始化。 一个例子解释类的主动使用和被动使用12345678910111213141516171819202122232425262728293031public class TestClassLoading &#123; public static void main(String[] args) &#123; // （1）并不会初始化类 Children 因为没有主动使用 Children；会初始化Parent 因为访问了str1 System.out.println(Children.str1); System.out.println("==========================="); // 会初始化Parent 也会初始化Children。因为访问了str2，要初始化Children，又因为子类被初始化，所以父类也被主动使用，进行初始化 System.out.println(Children.str2); &#125; static class Parent &#123; static String str1 = "hello"; static &#123; System.out.println("Parent初始化。。。。"); &#125; &#125; static class Children extends Parent &#123; static String str2 = "world"; static &#123; System.out.println("Children初始化。。。。"); &#125; &#125;&#125; 对应的输出： 12345Parent初始化。。。。hello===========================Children初始化。。。。world]]></content>
      <categories>
        <category>Java基础</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>类加载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存设计的常见原则]]></title>
    <url>%2Fblog%2F2020%2F04%2F02%2F%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%B8%B8%E8%A7%81%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[缓存设计中的一般性原则 热点数据一律进缓存； 缓存场景优先采取本地缓存+分布式缓存的综合方案； 优先读取本地缓存，以本地缓存为主，远端分布式缓存为辅； 所有缓存设置过期时间，本地缓存过期时间控制在秒级； 本地缓存务必同时设置容量驱逐和时间驱逐两种方式，减轻对内存的压力和防止内存泄漏等问题； 缓存KEY具有业务可读性，杜绝不同场景出现相同KEY； 缓存列表数据时，仅缓存第一页，缓存数量不超过20； 杜绝并发更新缓存，防止缓存击穿； 空数据进缓存，防止缓存穿透； 读数据时，先读缓存，再读数据库； 写数据时，先写数据库，再写缓存。]]></content>
      <categories>
        <category>系统设计</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ArrayList拾遗]]></title>
    <url>%2Fblog%2F2020%2F03%2F29%2FArrayList%E6%8B%BE%E9%81%97%2F</url>
    <content type="text"><![CDATA[1.ArrayList简介 ArrayList底层是object数组，容量能在添加元素的过程中动态扩容。并且在可预知添加大量元素时，调用ensureCapactiy方法提前扩容，减少递增式的扩容次数。 实现了RandomAccess接口，表示可以快速随机访问。根据下标访问。 实现了Cloneable接口，覆盖了函数克隆，不过也是潜拷贝 实现了Serializable接口，支持序列化进行传输 和Vector容器的区别 两者都是List接口的实现类，但是ArrayList线程不安全，Vector线程安全，方法都加了synchronize锁。 和LinkedList区别 两者都不保证线程安全 底层数据结构：ArrayList底层是object数组，而LinledList底层是双向链表。（jdk1.6是双向循环链表，而1.7之后取消了循环。） LinkedList不支持高效的快速随机访问，而ArrayList支持快速随机根据下标访问。 插入和删除受元素位置的影响 ArrayList在末尾add方法追加元素时，时间复杂度是O(1)，而如果是在指定位置插入和删除元素时，时间复杂度就是O(N-i)，因为这时需要把第i个和后面的n-i个元素向后/前移动一位。 LinkedList采用链表存储，对于add(E e)方法的插入、删除时间复杂度不受元素位置影响。对于指定位置i的插入和删除元素，因为也需要遍历到要插入位置，时间复杂度近似为O(N) 内存占用：ArrayList的内存浪费主要体现在元素个数比数组长度小时，会预留长度的空间浪费。而LinkedList没有扩容问题，但其每个节点除了存储元素之外，还要维护前驱结点和后继节点，所以单节点空间消耗大。 ArrayList关键源码走读静态和实例字段12345678910111213141516171819202122232425262728public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; //用于默认大小空实例的共享空数组实例。 //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * ArrayList 所包含的元素个数 */ private int size; 构造函数无参数1234567/** *默认无参构造函数 *DEFAULTCAPACITY_EMPTY_ELEMENTDATA 为0.初始化为10，也就是说初始其实是空数组 当添加第一个元素的时候数组容量才变成10 */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; 带有初始容量参数的构造函数12345678910111213public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //如果传入的参数大于0，创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //如果传入的参数等于0，创建空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; //其他情况，抛出异常 throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); &#125; &#125; 包含一个指定集合，按照集合的迭代器返回的顺序1234567891011121314public ArrayList(Collection&lt;? extends E&gt; c) &#123; //将指定集合转换为数组 elementData = c.toArray(); //如果elementData数组的长度不为0 if ((size = elementData.length) != 0) &#123; // 如果elementData不是Object类型数据（c.toArray可能返回的不是Object类型的数组所以加上下面的语句用于判断） if (elementData.getClass() != Object[].class) //将原来不是Object类型的elementData数组的内容，赋值给新的Object类型的elementData数组 elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 长度为0，用空数组代替 this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; contains和containsAll方法1234567891011121314151617181920212223242526272829303132333435363738394041/** * 如果此列表包含指定的元素，则返回true 。 */public boolean contains(Object o) &#123; //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 return indexOf(o) &gt;= 0;&#125; /** *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 */public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) //equals()方法比较 if (o.equals(elementData[i])) return i; &#125; return -1;&#125;// lastIndexOf方法 /** * 返回此列表中指定元素的最后一次出现的索引，如果此列表不包含元素，则返回-1。. */public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; get方法12345678910111213141516public E get(int index) &#123; // 检查在size内 rangeCheck(index); // 直接从数组中根据下标读 return elementData(index); &#125; private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; @SuppressWarnings(&quot;unchecked&quot;) E elementData(int index) &#123; return (E) elementData[index]; &#125; add 和 set方法add方法会在ensureCapacityInternal中对modCount+1。12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 用指定的元素替换此列表中指定位置的元素。 */ public E set(int index, E element) &#123; //对index进行界限检查 rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; //返回原来在这个位置的元素 return oldValue; &#125; /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; &#125; /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; remove方法remove也会对modCount+112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue;&#125;/** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125;/* * Private remove method that skips bounds checking and does not * return the value removed. */ // 不校验index的快速删除方法private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work&#125; 扩容相关从上边的构造函数中可以知道，调用无参数构造函数时，elementData数组初始化为DEFAULTCAPACITY_EMPTY_ELEMENTDATA空数组，只有当真正对数组添加元素操作时，才真正分配容量，向数据添加第一个元素时，数组容量扩容到默认的capacity=10. 在add方法中调用了ensureCapacityInternal方法来修改了modCount和进行扩容的。12345678910111213141516171819private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 如果现在object数组时初始化时的默认空数组 则重新计算minCapacity值 // 计算规则 default_capacity和minCapacity中较大的值。 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; private void ensureExplicitCapacity(int minCapacity) &#123; // 增加modCount modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) // 计算出的minCapacity如果比当前数组长度大 则调用grow方法进行扩容。 grow(minCapacity); &#125; 当调用了无参数构造函数之后，add第一个元素时，minCapacity = size+1 =1，此时和默认容量相比比较大的是默认容量，此时minCapacity传入grow进行扩容 当add第2个元素时，minCapacity = size + 1 = 2，此时因为mincapacity - length = 2-10 &lt;0 ,所以不会调用grow方法进行扩容，同理第3、4..10个元素被add时都不会扩容。 当第11个元素add进去时，会调用grow方法进行扩容 grow方法是真正对ArrayList中的数组进行扩容。123456789101112131415161718192021222324252627/** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 // `Integer.MAX_VALUE - 8`。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; 当add第一个元素时，会grow方法扩容到10。当add第二个元素时，此时不会调用grow方法，容量不变 当add第11个元素时，会调用grow方法，扩容容量为1.5倍数组长度，即为15]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>ArrayList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap拾遗（一）]]></title>
    <url>%2Fblog%2F2020%2F03%2F28%2FHashMap%E6%8B%BE%E9%81%97%2F</url>
    <content type="text"><![CDATA[开始HashMap是在开发工作中经常使用的集合类之一，熟悉其源码应该是基本要求。这篇文章对jdk1.8版本中的HashMap的一些常用方法的源码进行个记录。ps：这篇文章没有对其中的树化进行深究，比如提供的TreeNode内部类的结构和在扩容、Hash碰撞的时候的静态方法，之后有时间再研究下。 源码分析1.1定义的变量常量123456789101112131415161718192021222324252627282930313233343536373839404142/** * The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8;/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */static final int UNTREEIFY_THRESHOLD = 6;/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */static final int MIN_TREEIFY_CAPACITY = 64; DEFAULT_INITIAL_CAPACITY：默认的初始化容量，必须是2的幂，这个值为16。 MAXIMUM_CAPACITY：最大的容量，是1左移30位，相当于2的30次幂。 DEFAULT_LOAD_FACTOR：默认的负载因子，值是0.75f。 TREEIFY_THRESHOLD：树化的阈值，在整个数组的一个槽内，发生碰撞的链表如果超出整个阈值，就会转换为红黑树，在下面的代码也会分析到。 UNTREEIFY_THRESHOLD：树变为链表的阈值。 MIN_TREEIFY_CAPACITY：树化对应的最小容量阈值，上面的TREEIFY_THRESHOLD常量不是树化的唯一条件，HashMap在树化的方法中判断了当前的容量和这个值的比较，没有达到这个值，会先进行resize扩容操作。 变量字段12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */transient Node&lt;K,V&gt;[] table;/** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;/** * The number of key-value mappings contained in this map. */transient int size;/** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */transient int modCount;/** * The next size value at which to resize (capacity * load factor). * * @serial */// (The javadoc description is true upon serialization.// Additionally, if the table array has not been allocated, this// field holds the initial array capacity, or zero signifying// DEFAULT_INITIAL_CAPACITY.)int threshold;/** * The load factor for the hash table. * * @serial */final float loadFactor; table：HashMap中的Node数组，Node是其中的内部类。 entrySet：键值对的缓存。 size：key-value对的数量。 modCount：修改次数。比如在内部实现的foreach方法，可以看到在循环中应用传入的action之外，还做了对modCount的校验，这里有点cas的感觉，有预期值和当前值，如果不一致，就抛出ConcurrentModificationException这个异常，也被称为是HashMap的fast-fail机制。 threshold：扩容的阈值。构造方法中直接通过tableSizeFor(initialCapacity)进行赋值，因为构造方法中tab数组并没有初始化，后来在put方法中初始化tab数组的时候重新对threshold进行了计算。 loadFactor：负载因子。这里吐槽下经常看到面试问为啥是0.75，没get到问这个的意义在哪，设计jdk的人经过很多实验设置的值，并解释了符合泊松分布啥的，这种事了解下不就可以了吗，在工作中知道能自己设置这个值，不就可以了吗= = Node内部类12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */ static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125; &#125; 可以看到其中只定义了简单的值：hash值、key、value、还有next节点。 1.2 构造方法来看看HashMap的构造方法： 123456789101112131415161718192021222324252627public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; 可以看到提供了四个重载的构造方法，有： 设置初始化容量和负载因子的。 无参数。 设置初始化容量的。 参数是一个map的 可以看到在构造方法中并没初始化数组（除了传入一个map初始化），而是赋值了负载因子和通过tableSizeFor方法赋值了容量的阈值。 tableSizeFor方法来看看这个tableSizeFor方法： 12345678910111213/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 这里将传入的容量参数-1，然后无符号右移1为左或操作，重复无符号右移2位、4位…，最后做了和最大容量值的比较。看注释知道这里返回的容量都是2的幂次方，这与下面定位数组中的位置有关，下面会做出解释。 1.3 put方法123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; hash方法1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; hash方法将key的hashCode和其无符号右移16位的值进行了异或操作，这个操作 也是扰动函数，为了降低哈希码的冲突。右位移16位，正好是32bit的一半，高半区和低半区做异或，就是为了混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。也就是保证考虑到高低Bit位都参与到Hash的计算中。 这里看到我们已经做了相当于两次hash操作：（1）取key的hashCode方法进行计算。 （2）hashCode无符号右移16位，与hashCode进行了异或。 一个key计算hash函数得到的值就是Node类中hash变量的值。 在后面定位元素在数组中的位置用了元素长度（这个上面提到了是2的次幂）-1去和hash值做了与操作：tab[i = (n - 1) &amp; hash]。这也相当于一个key传入HashMap定位到数组中的位置至少经历了3次hash操作。 在后边的方法代码中可以经常看到这行代码来定位数组中的位置，其实这里因为是2的次幂，所以做与操作相当于模除来定位数组中的位置。（参考这篇博客：HashMap中的hash算法） putVal方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * Implements Map.put and related methods * * @param hash hash for key //key的hash值 * @param key the key //key的值 * @param value the value to put // value值 * @param onlyIfAbsent if true, don't change existing value // 是否覆盖 * @param evict if false, the table is in creation mode. // false的话是在创建模式 * @return previous value, or null if none // 返回前一个value，如果没有前值，返回null */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; // 数组变量 Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) // 如果当前数组没有初始化或者length为0，则resize()方法进行初始化 n = (tab = resize()).length;//赋值初始化好的数组长度给n if ((p = tab[i = (n - 1) &amp; hash]) == null) // 如果key对应数组下标i中的值为null tab[i] = newNode(hash, key, value, null); // 初始化一个Node对象作为存储在该位置的值 else &#123; Node&lt;K,V&gt; e; //e 这里理解为exist，即定位到数据下标i处已经有了node元素 K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 如果i坐标下的hash与当前hash值相等 并且 key也和当前要put的值相等或者equals // exist的元素就是p 即当前i的元素 e = p; else if (p instanceof TreeNode) // 如果当前i坐标的的node是树Node，则当前exist是调用了坐标i树Node元素的putTreeVal e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 当前i坐标的node元素不是树Node for (int binCount = 0; ; ++binCount) &#123; // 寻找当前坐标链表的下一个元素 if ((e = p.next) == null) &#123; // 如果下一个元素为null，new一个node对象放在当前节点上 p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 当前循环次数大于树化的阈值，则进行树化 treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 如果当前坐标下链表元素不为空，则比较hash值和key是否相等 如果相等则跳出循环 break; p = e;// 都不满足，则e赋给p，其实是为了下次循环 &#125; &#125; if (e != null) &#123; // existing mapping for key // 在上面的流程中如果exist有了值，说明当前key在map已经有了 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) // oldValue为null或者onlyIfAbsent为false时更新值为value。 e.value = value; // 回调方法 afterNodeAccess(e); // 返回oldValue return oldValue; &#125; &#125; // 如果走到这里说明是插入了这个key、value ++modCount; // 更新modCount if (++size &gt; threshold) resize(); // 去判断是否需要扩容 需要的话就去resize扩容 afterNodeInsertion(evict); return null; // return null &#125; 可以看到putVal方法流程是： 如果当前tab数组没有初始化，先调用resize()方法去初始化数组，数组长度赋给n。 通过tab[(n-1) &amp; hash] 寻找数组位置， 如果当前数组位置对象为null，则new一个Node放到当前数组的位置，走++modCount、判断并且resize扩容、返回null。 把当前数组下标位置的元素赋给p变量，如果p不为null（说明通过该key对应的hash值映射到的数组下标处的元素不为空），那么就看是否存在key相同的元素（映射到同一下标不代表key相同）： 如果满足判断条件：(k = p.key) == key || (key != null &amp;&amp; key.equals(k)))这个判断条件，则p赋值给e，代表exist就是数组tab上的第一个元素。 没有满足条件看p是否为TreeNode的一个实例，这里如果p是TreeNode，说明该hashMap已经树化，此时调用TreeNode的putTreeVal方法去往树中添加当前的key，value。并把这个方法的返回值赋给e。（这个方法先不深究，这里注意可能返回null）。 如果也不是TreeNode的一个实例，则沿当前位置像遍历链表：一个for循环，维护了计数变量bitCount。 将p.next赋值给e，如果这个e为null，则new一个node对象放在p.next的位置上，并且判断当前的bitCount是否大于等于树化的阈值8-1=7，注意bitCount是从0开始的，如果达到，要调用treeIfBin方法进行树化操作，并跳出循环。（调用树化的方法并不一定会树化，因为内部还判断了数组长度，如果未达到数组长度的树化阈值，会先去调用resize方法扩容） 如果p.next不为null，那么再次判断条件(k = e.key) == key || (key != null &amp;&amp; key.equals(k)))`，即判断p.next的key是否和要put的key相等，如果相等，则跳出循环。 如果都不满足（p.next不为null且与键值与key不相等），则将e赋给p，进行下次循环。 判断e这个变量，即当前map中存在与当前要put的key相等的键值对，如果oldValue为null或者onlyIfAbsent是false，则覆盖oldValue并直接返回oldValue。（注意这步不会修改modCount） modCount++、size++，这里判断了是否需要去扩容，如果超过了负载因子和长度的乘积这个阈值，则调用resize()方法进行扩容。返回null。 resize()方法可以在putVal方法的源码中看到resize()方法在多个地方被调用，来看看这个方法的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/** * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * * @return the table */ final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; // table数组赋值oldTab变量 int oldCap = (oldTab == null) ? 0 : oldTab.length;// oldCap为数组长度 int oldThr = threshold;// 老的阈值 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 如果数组已经初始化 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; // 如果当前数组长度已经超过最大的容量限制 threshold = Integer.MAX_VALUE;// 阈值设置外Integer.MAX_VALUE return oldTab;// 返回老数组 &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 如果老数组扩容2倍之后小于最大容量限制并且老数组长度大于等于默认初始化容量 newThr = oldThr &lt;&lt; 1; // double threshold 直接将阈值也乘2 &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold // 如果当前数组长度为0，但是阈值已经初始化了，那么直接将oldThr赋给newCap变量。这里场景即运行了构造函数但是没有进行put操作，那时的threshold值会为tableSizeFor(initCap)方法的结果。 newCap = oldThr; else &#123; // zero initial threshold signifies using defaults // 如果oldThr、oldCap都为0 赋值newCap、newThr都为默认值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; // 如果newThr为0，则通过现在的newCap和负载因子计算newThr float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr;// 赋值threshold @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; // 初始化新的node数组 table = newTab;// newTab赋给table if (oldTab != null) &#123; // 如果oldTab中有数据 for (int j = 0; j &lt; oldCap; ++j) &#123; // 遍历oldTab Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; // 如果oldTab[j]元素不为null oldTab[j] = null; // oldTab[j] 置为 null if (e.next == null) // 如果当前节点的next无元素，则将e（oldTab[i]的值）放在newTab[e.hash &amp; (newCap - 1)]的位置。这个e.hash &amp; newCap - 1使得resize之后分散的更加均匀。（1.8的一个优化） newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 如果为树节点，调用TreeNode的split方法去分离扩容 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // resize处理链表，这里注释可以看到弃用了1.7的头插法，避免形成环。 // 链表可能会被拆分，因为容量扩大，可能要移动到oldCap +当前索引值处。 // loHead存储不需要移动到新的下标处的node Node&lt;K,V&gt; loHead = null, loTail = null; // hiHead存储需要移动到新的下标处的node Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; // 返回newTab return newTab; &#125; 从注释上可以看到，这个方法不仅仅承担着两倍扩容的作用，也负责初始化table数组的作用。 resize方法的流程： 一、初始化变量阶段 当前tab数组赋给oldTab，当前tab数组长度赋给oldCap，当前threshold赋给oldThr。初始化newCap、newThr为0 判断如果oldCap&gt;0，即当前数组长度大于0 如果oldCap大于等于最大的Cap值，那么设置threshold为Integer.MAX_VALUE，直接返回oldTab。 赋值newCap为oldCap的2倍，如果newCap小于最大容量并且oldCap大于等于默认初始化容量，则直接将当前的threshold扩大2倍。 oldCap为0，如果oldThr&gt;0，则直接将newCap设置为oldThr容量。（这里说明下，如果当前数组长度为0，但是阈值已经初始化了，那么直接将oldThr赋给newCap变量。这里场景即运行了构造函数但是没有进行put操作，那时的threshold值会为tableSizeFor(initCap)方法的结果） 如果oldCap和oldThr都为0，则初始化newCap为默认大小，newThr为默认Cap大小 * 默认负载因子。 如果上述流程中newThr为0，那么根据newCap和当前的负载因子去计算threshold值赋给newThr。 二、数据迁移阶段 如果oldTab不为null，那么需要进行数据到扩容之后的数组的映射。 循环老数组 如果当前下标元素old[j]不为null，则进行下面的操作： oldTab[j]元素赋给变量e，oldTab[j]赋值null。 如果e.next为null，则说明直接将当前元素迁移到新的坐标即可。这里定位新的坐标是e.hash &amp; (newCap - 1) 即用hash值和newCap-1去做与操作，1.8中的这个操作很妙的是newCap绝对是2的次幂，具体的可以参考上面提到的扰动函数的位置的说明。 如果e.next不为null并且当前e是一个TreeNode，那么调用TreeNode的split方法去处理数组。这里红黑树的方法不去深究。这个操作中其实也有可能去做树的退化操作。因为和1.7一样，扩容原来在一条链上的元素，在扩容之后可能不在一条链上，这里也一样，如果链的长度达到了树退化的阈值，那么这里的要做树退化为链表的操作。 如果e.next不为null并且是一个链表结构，那么这里会将e所在的链表元素重新计算新的下标值，映射到新的数组上去，刚才也提到了链表可能会被拆分，因为容量扩大，可能要移动到[oldCap +当前索引值]处。(可以看到注释说明了保证了链的顺序，这里弃用了1.7中的头插法避免出现环) 继续循环oldCap 最后返回newTab 1.4 get方法我们来看看也是经常使用的HashMap的get方法: 123456789101112131415161718192021/** * Returns the value to which the specified key is mapped, * or &#123;@code null&#125; if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * &#123;@code k&#125; to a value &#123;@code v&#125; such that &#123;@code (key==null ? k==null : * key.equals(k))&#125;, then this method returns &#123;@code v&#125;; otherwise * it returns &#123;@code null&#125;. (There can be at most one such mapping.) * * &lt;p&gt;A return value of &#123;@code null&#125; does not &lt;i&gt;necessarily&lt;/i&gt; * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to &#123;@code null&#125;. * The &#123;@link #containsKey containsKey&#125; operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */ public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; 看注释有一点是：如果get方法返回了null，那么不一定是这个map中不包含当前传入的key，也有可能是在map中key映射了null值value，并可以通过containsKey方法去区分这两种情况。 可以看到get方法是将hash值传入getNode方法的，下面看看getNode方法的源码： 1234567891011121314151617181920212223242526272829303132333435/** * Implements Map.get and related methods * * @param hash hash for key * @param key the key * @return the node, or null if none */final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab;// tab数组 Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 如果当前的数组不为null，并且数组长度大于0，并且通过(n-1)&amp;hash求得数组下标位置的值不为null if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) // 如果当前位置的第一个node节点的key与相等，直接返回当前节点 return first; // 如果first节点的key与当前传入的key不相等 if ((e = first.next) != null) &#123; // 如果first还有下一个节点 if (first instanceof TreeNode) // 判断当前Node是否TreeNode，如果是则调用树的getTreeNode方法获取value return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 非树节点，其实就是循环链表，挨个对比key是否相等，直到链表尾部。 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; getNode的流程比较简单，注释上写的流程这里不去做过多解释。 1.4 remove方法来看看remove方法的代码： 1234567891011121314/** * Removes the mapping for the specified key from this map if present. * * @param key key whose mapping is to be removed from the map * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; 这里注意返回null可以代表map中没有这个映射，或map中这个key映射的value是null。 同样这里也是把hashCode计算好之后传入一个removeNode方法中： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; // 和getNode一样的套路，这里如果table数组不为null并且长度大于0并且hash值定位到的数组坐标值不为空 Node&lt;K,V&gt; node = null, e; // 定义node变量 K k; V v; if (p.hash == hash &amp;&amp;((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p;// 如果当前坐标的第一个node的key与传入的key相等，p赋值node else if ((e = p.next) != null) &#123; // 如果不相等且还有下边的节点 if (p instanceof TreeNode) // 如果是树化的节点，调用树化的getTreeNode方法获取node node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; // 不是树化的节点，则遍历链表找对应的键值对 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e;// 注意这里遍历时更新了p变量，方便在下面删除时使用 &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; // 如果node不为null，并且matchValue的值和当前value的值设置一致（不是值一致，而是这个判断） if (node instanceof TreeNode) // 树化的node节点调用TreeNode的removeTreeNode方法删除 ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) // 不是树化，则判断node是否为p节点，即数组的第一个节点，如果是则将node的第一个节点在数组位置上删除即可 tab[index] = node.next; else // 不是树化并且不是第一个节点，则将当前node的next置为p.next即可 即删除node节点 p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null; &#125; 可以看到删除时的操作和getNode方法大同小异，只不过在找到对应的key对应的node节点之后，去做了对node节点的删除逻辑而已。 彩蛋上面有提到在1.7中resize时采用的头插法来转移之前数组上的链表，下面这个图是在1.7中扩容的简单的说明： 图片来源： https://blog.csdn.net/pange1991/article/details/82347284 而这个在并发的场景下可能形成环，耗子叔的博客也介绍了很详细： https://coolshell.cn/articles/9606.html 总结本文简单介绍了HashMap中的成员变量、构造函数、改查删方法的源码，还对里面一些hash取值、定位数组下标的方法做了简单介绍，其中对1.8之后的红黑树没有深究，之后可能在后面的拾遗过程中介绍红黑树的用法。这里知道在链表长度过大时，红黑树能提高对应的效率即可。 关于HashMap的结构和容量总的来说： HashMap 底层数据结构在JDK1.7之前是由数组+链表组成的，1.8之后又加入了红黑树；链表长度小于8的时候，发生Hash冲突后会增加链表的长度，当链表长度大于8的时候，会先判读数组的容量，如果容量小于64会先扩容（原因是数组容量越小，越容易发生碰撞，因此当容量过小的时候，首先要考虑的是扩容），如果容量大于64，则会将链表转化成红黑树以提升效率。 hashMap 的容量是2的n次幂，无论在初始化的时候传入的初始容量是多少，最终都会转化成2的n次幂，这样做的原因是为了在取模运算的时候可以使用&amp;运算符，而不是%取余，可以极大的提上效率，同时也降低hash碰撞的概率。 参考文章 https://juejin.im/post/5c8f461c5188252da90125ba https://www.cnblogs.com/ysocean/p/9054804.html https://blog.csdn.net/pange1991/article/details/82347284]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Comparator接口在java8中的优化]]></title>
    <url>%2Fblog%2F2020%2F03%2F22%2FComparator%E6%8E%A5%E5%8F%A3%E5%9C%A8java8%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[开始Comparator接口或者Comparable接口在日常开发工作中是经常用到的，用于比较一组数据或者对象，在java8之后，也可以看到在Comparator接口中加入了一些default方法和static方法，这里做一个简单说明。 Comparator接口和Comparable接口这两个接口首先要做一个简单区别。 Comparable接口123456* Lists (and arrays) of objects that implement this interface can be sorted * automatically by &#123;@link Collections#sort(List) Collections.sort&#125; (and * &#123;@link Arrays#sort(Object[]) Arrays.sort&#125;). Objects that implement this * interface can be used as keys in a &#123;@linkplain SortedMap sorted map&#125; or as * elements in a &#123;@linkplain SortedSet sorted set&#125;, without the need to * specify a &#123;@linkplain Comparator comparator&#125;.&lt;p&gt; 可以看到注释中说明了实现了该接口的对象，在数组中可以使用Collections.sort或者Arrays.sort方法实现排序，或者实现了该接口的对象可以作为sortedMap或者SortedSet的key。这里也提到我们不用制定一个排序或者作为key的Comparator接口。 12345678910public interface Comparable&lt;T&gt; &#123; /** * 省略部分注释 * &lt;p&gt;The implementor must ensure &lt;tt&gt;sgn(x.compareTo(y)) == * -sgn(y.compareTo(x))&lt;/tt&gt; for all &lt;tt&gt;x&lt;/tt&gt; and &lt;tt&gt;y&lt;/tt&gt;. (This * implies that &lt;tt&gt;x.compareTo(y)&lt;/tt&gt; must throw an exception iff * &lt;tt&gt;y.compareTo(x)&lt;/tt&gt; throws an exception.) */ public int compareTo(T o);&#125; 在compareTo方法上的注释中提到，必须确保 x.compareTo(y)和y.compareTo(x)的结果是一致的，并且这也意味着当x.compartTo(y)抛出一个异常，那么y.compareTo(x)也应该去抛出一个异常，那么这里就思考到了一个关于null的设计：null.compareTo(obj)我们肯定知道会有NPE，那么你在实现compareTo方法的时候，如果obj.compareTo(null)这里也应该去抛出NPE。 这里就不去写具体的demo去演示了，这里理解为一个对象实现了Comparable接口，那么这个对象就是可比较的，并且在排序等场景下调用实现接口中的compareTo方法。 Comparator接口Comparator接口要理解为比较器，实现其接口的类其实是比较器的一种实现，相当于一个比较的函数定义。来看下他的注释： 12345678* A comparison function, which imposes a &lt;i&gt;total ordering&lt;/i&gt; on some * collection of objects. Comparators can be passed to a sort method (such * as &#123;@link Collections#sort(List,Comparator) Collections.sort&#125; or &#123;@link * Arrays#sort(Object[],Comparator) Arrays.sort&#125;) to allow precise control * over the sort order. Comparators can also be used to control the order of * certain data structures (such as &#123;@link SortedSet sorted sets&#125; or &#123;@link * SortedMap sorted maps&#125;), or to provide an ordering for collections of * objects that don't have a &#123;@link Comparable natural ordering&#125;.&lt;p&gt; 这里我们看到Arrays、Collections也提供了重载的sort方法，支持传入一个集合/数组和Comparator接口的实例。当然当前列表/数组中的对象不一定是实现了Comparable接口。 类实现了comparable接口之后，可以直接调用排序方法；而当使用comparator时，不需要类实现，具体使用时（也就是调用某些方法时）的需要类和该comparator绑定起来来实现。comparable实现内部排序，Comparator是外部排序。 个人感觉Comparator接口更符合解耦的思想，更好维护些。 java8之后的Comparator接口在java8之后Comparator接口增加了很多default方法和static方法来方便定义比较器。 reserved方法12/** * Returns a comparator that imposes the reverse ordering of this * comparator. * * @return a comparator that imposes the reverse ordering of this * comparator. * @since 1.8 */default Comparator&lt;T&gt; reversed() &#123; return Collections.reverseOrder(this);&#125; comparing方法1234567891011121314151617181920212223/** * Accepts a function that extracts a &#123;@link java.lang.Comparable * Comparable&#125; sort key from a type &#123;@code T&#125;, and returns a &#123;@code * Comparator&lt;T&gt;&#125; that compares by that sort key. * * &lt;p&gt;The returned comparator is serializable if the specified function * is also serializable. * * @apiNote * For example, to obtain a &#123;@code Comparator&#125; that compares &#123;@code * Person&#125; objects by their last name, * * &lt;pre&gt;&#123;@code * Comparator&lt;Person&gt; byLastName = Comparator.comparing(Person::getLastName); * &#125;&lt;/pre&gt; */public static &lt;T, U extends Comparable&lt;? super U&gt;&gt; Comparator&lt;T&gt; comparing( Function&lt;? super T, ? extends U&gt; keyExtractor)&#123; Objects.requireNonNull(keyExtractor); return (Comparator&lt;T&gt; &amp; Serializable) (c1, c2) -&gt; keyExtractor.apply(c1).compareTo(keyExtractor.apply(c2));&#125; comparing方法参数是一个函数式接口keyExtractor，意识即为指定排序对象中的排序键，这里注意排序键这里标注了Comparable接口。 同时我们也可以看到有重载的comparing方法： 12345678910111213141516171819202122232425262728 /** * Accepts a function that extracts a sort key from a type &#123;@code T&#125;, and * returns a &#123;@code Comparator&lt;T&gt;&#125; that compares by that sort key using * the specified &#123;@link Comparator&#125;. * * &lt;p&gt;The returned comparator is serializable if the specified function * and comparator are both serializable. * * @apiNote * For example, to obtain a &#123;@code Comparator&#125; that compares &#123;@code * Person&#125; objects by their last name ignoring case differences, * * &lt;pre&gt;&#123;@code * Comparator&lt;Person&gt; cmp = Comparator.comparing( * Person::getLastName, * String.CASE_INSENSITIVE_ORDER); * &#125;&lt;/pre&gt; */public static &lt;T, U&gt; Comparator&lt;T&gt; comparing( Function&lt;? super T, ? extends U&gt; keyExtractor, Comparator&lt;? super U&gt; keyComparator) &#123; Objects.requireNonNull(keyExtractor); Objects.requireNonNull(keyComparator); return (Comparator&lt;T&gt; &amp; Serializable) (c1, c2) -&gt; keyComparator.compare(keyExtractor.apply(c1), keyExtractor.apply(c2)); &#125; 第二个参数也很好理解，提取完sort key之后，要定义关于这个key的Comparator，在注释中的例子也比较好理解。 这里有个小tips：在String类中，提供了一个实现Comparator接口的常量来标识不对语言敏感的字典序排序器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 /** * A Comparator that orders &#123;@code String&#125; objects as by * &#123;@code compareToIgnoreCase&#125;. This comparator is serializable. * &lt;p&gt; * Note that this Comparator does &lt;em&gt;not&lt;/em&gt; take locale into account, * and will result in an unsatisfactory ordering for certain locales. * The java.text package provides &lt;em&gt;Collators&lt;/em&gt; to allow * locale-sensitive ordering. * * @see java.text.Collator#compare(String, String) * @since 1.2 */ public static final Comparator&lt;String&gt; CASE_INSENSITIVE_ORDER = new CaseInsensitiveComparator(); private static class CaseInsensitiveComparator implements Comparator&lt;String&gt;, java.io.Serializable &#123; // use serialVersionUID from JDK 1.2.2 for interoperability private static final long serialVersionUID = 8575799808933029326L; public int compare(String s1, String s2) &#123; int n1 = s1.length(); int n2 = s2.length(); int min = Math.min(n1, n2); for (int i = 0; i &lt; min; i++) &#123; char c1 = s1.charAt(i); char c2 = s2.charAt(i); if (c1 != c2) &#123; c1 = Character.toUpperCase(c1); c2 = Character.toUpperCase(c2); if (c1 != c2) &#123; c1 = Character.toLowerCase(c1); c2 = Character.toLowerCase(c2); if (c1 != c2) &#123; // No overflow because of numeric promotion return c1 - c2; &#125; &#125; &#125; &#125; return n1 - n2; &#125; /** Replaces the de-serialized object. */ private Object readResolve() &#123; return CASE_INSENSITIVE_ORDER; &#125; &#125;// 这里其实可以看到compareToIgnoreCase也是调用了这个实例的compare方法public int compareToIgnoreCase(String str) &#123; return CASE_INSENSITIVE_ORDER.compare(this, str); &#125; 在Comparator接口中，也直接提供了具体类型的三个comparing方法： 123456789101112131415161718public static &lt;T&gt; Comparator&lt;T&gt; comparingInt(ToIntFunction&lt;? super T&gt; keyExtractor) &#123; Objects.requireNonNull(keyExtractor); return (Comparator&lt;T&gt; &amp; Serializable) (c1, c2) -&gt; Integer.compare(keyExtractor.applyAsInt(c1), keyExtractor.applyAsInt(c2)); &#125;public static &lt;T&gt; Comparator&lt;T&gt; comparingLong(ToLongFunction&lt;? super T&gt; keyExtractor) &#123; Objects.requireNonNull(keyExtractor); return (Comparator&lt;T&gt; &amp; Serializable) (c1, c2) -&gt; Long.compare(keyExtractor.applyAsLong(c1), keyExtractor.applyAsLong(c2)); &#125; public static&lt;T&gt; Comparator&lt;T&gt; comparingDouble(ToDoubleFunction&lt;? super T&gt; keyExtractor) &#123; Objects.requireNonNull(keyExtractor); return (Comparator&lt;T&gt; &amp; Serializable) (c1, c2) -&gt; Double.compare(keyExtractor.applyAsDouble(c1), keyExtractor.applyAsDouble(c2)); &#125; thenComparing方法12345678910111213141516171819202122232425/** * Returns a lexicographic-order comparator with another comparator. * If this &#123;@code Comparator&#125; considers two elements equal, i.e. * &#123;@code compare(a, b) == 0&#125;, &#123;@code other&#125; is used to determine the order. * * &lt;p&gt;The returned comparator is serializable if the specified comparator * is also serializable. * * @apiNote * For example, to sort a collection of &#123;@code String&#125; based on the length * and then case-insensitive natural ordering, the comparator can be * composed using following code, * * &lt;pre&gt;&#123;@code * Comparator&lt;String&gt; cmp = Comparator.comparingInt(String::length) * .thenComparing(String.CASE_INSENSITIVE_ORDER); * &#125;&lt;/pre&gt; */ default Comparator&lt;T&gt; thenComparing(Comparator&lt;? super T&gt; other) &#123; Objects.requireNonNull(other); return (Comparator&lt;T&gt; &amp; Serializable) (c1, c2) -&gt; &#123; int res = compare(c1, c2); return (res != 0) ? res : other.compare(c1, c2); &#125;; &#125; 从方法名称上知道这是当比较相同时的使用的一个排序规则，这里需要注意看具体实现是会先调用比较器实例中的compare方法来进行比较一轮，当结果等于0的时候才会调用other这个比较器规则进行比较。比如下面的代码： 1234567List&lt;String&gt; strings = Arrays.asList("def", "abc", "hel", "world"); strings.sort(Comparator.comparingInt(String::length).reversed() //（1） .thenComparing(String::compareToIgnoreCase) // （2） .thenComparing(Comparator.reverseOrder()) // （3）这个比较器不会被应用 因为比较器（2）已经把结果比较出来了，并且没有相等的结果，这里不会再应用（3）比较器 ); System.out.println(strings); // 输出[world, abc, def, hel] 当然因为有了 comparing方法的支持，所以也就有了下面两个thenComparing的重载方法 12345default &lt;U extends Comparable&lt;? super U&gt;&gt; Comparator&lt;T&gt; thenComparing( Function&lt;? super T, ? extends U&gt; keyExtractor) &#123; return thenComparing(comparing(keyExtractor)); &#125; 123456default &lt;U&gt; Comparator&lt;T&gt; thenComparing( Function&lt;? super T, ? extends U&gt; keyExtractor, Comparator&lt;? super U&gt; keyComparator) &#123; return thenComparing(comparing(keyExtractor, keyComparator)); &#125; 这里也不赘述提供的thenComparingInt、thenComparingDouble这类的方法。 null 友好的比较器看到Comparator接口中有两个对null友好的比较器方法： 1234567891011121314151617181920212223242526/** * Returns a null-friendly comparator that considers &#123;@code null&#125; to be * less than non-null. When both are &#123;@code null&#125;, they are considered * equal. If both are non-null, the specified &#123;@code Comparator&#125; is used * to determine the order. If the specified comparator is &#123;@code null&#125;, * then the returned comparator considers all non-null values to be equal. * * &lt;p&gt;The returned comparator is serializable if the specified comparator * is serializable. * * @param &lt;T&gt; the type of the elements to be compared * @param comparator a &#123;@code Comparator&#125; for comparing non-null values * @return a comparator that considers &#123;@code null&#125; to be less than * non-null, and compares non-null objects with the supplied * &#123;@code Comparator&#125;. * @since 1.8 */ public static &lt;T&gt; Comparator&lt;T&gt; nullsFirst(Comparator&lt;? super T&gt; comparator) &#123; return new Comparators.NullComparator&lt;&gt;(true, comparator); &#125; // null比非null元素都大的 public static &lt;T&gt; Comparator&lt;T&gt; nullsLast(Comparator&lt;? super T&gt; comparator) &#123; return new Comparators.NullComparator&lt;&gt;(false, comparator); &#125; 这里是通过Comparators这个工厂类提供的NullComparator比较器实现的，看到注释有一条需要注意是如果不指定comparator参数，即传入null，那么所有的非null参数都会被视为相等。 集合中插入null记录的场景也不是很常见，知道有这么个null友好的Comparator即可。 彩蛋这里遇到了一个使用比较器 类型推断导致编译不过的问题： 12345 // 这里尝试使用lambda方式去实现根据字符串长度降序排序 使用方法引用不会有这问题 因为指定了String::length list.sort(Comparator.comparingInt(item -&gt; item.length()).reversed()); // 注意如果调用了reversed方法，那么item.length会报编译错误。这里把item推断成了object类型。// 但是不调用reversed是可以编译通过的 // 这里是因为 失去了lambda的类型推断 具体见博客：https://blog.csdn.net/u013096088/article/details/69367260]]></content>
      <categories>
        <category>Java语法</category>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>Comparator接口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[栈与队列相互实现]]></title>
    <url>%2Fblog%2F2020%2F03%2F22%2F%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97%E7%9B%B8%E4%BA%92%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[背景一道很经典的数据结构的题目实现。 栈：一般是后进先出的顺序，可以看下java中的Stack这个类。 队列：一般是先进先出的顺序，但是java中的Queue接口中也写了注释，没有要求是必须严格的先进先出，比如java中也有优先级队列、双端队列Deque。 代码实现在代码的注释中有描述对应的思路，这里不去赘述 两个栈实现队列 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * Created by zlj on 2020/3/18. * 两个栈 实现 队列 * * 思路： * 1.入栈：直接压栈进入stackOne * 2.出栈： * （1）判断stackOne是否为空，如果不为空，则将stackOne中的数据导入stackTwo。取出stackTwo弹栈的元素 * （2）判断stackTwo是否为空，如果不为空，重复（1）操作 * */public class StackToQueue&lt;T&gt; &#123; Stack&lt;T&gt; stackOne = new Stack&lt;&gt;(); Stack&lt;T&gt; stackTwo = new Stack&lt;&gt;(); /** * 入队 * @param data */ private void push(T data) &#123; stackOne.push(data); &#125; /** * 出队列 * @return */ private T pop() &#123; if (stackOne.empty() &amp;&amp; stackTwo.empty()) return null; // 容错 栈的pop方法如果没有元素了会报错 // 其实加不加这个判断相当于Queue接口中poll（做了容错） 和 remove(没做容错) 两个方法的区别 while (!stackOne.isEmpty()) &#123; // 弹栈到stackTwo stackTwo.push(stackOne.pop()); &#125; // 这时弹栈 stackTwo中的元素 T stackTwoFirstEmt = stackTwo.pop(); // 如果出队列的时候元素都在stackTwo while(!stackTwo.isEmpty()) &#123; // 数据倒回stackOne 以便下一次pop的时候 再利用栈的特性 实现队列出队的顺序 stackOne.push(stackTwo.pop()); &#125; return stackTwoFirstEmt; &#125; public static void main(String[] args) &#123; StackToQueue&lt;Integer&gt; stackToQueue = new StackToQueue&lt;&gt;(); // 队列入队 Stream.iterate(1, i -&gt; i+1).limit(10).forEach(stackToQueue::push); // 队列出队 for(int i = 0; i&lt; 10000; i++) &#123; Integer pop = stackToQueue.pop(); if (pop == null) break; System.out.println("队列出队：" + pop); try &#123; Thread.sleep(200); &#125; catch (Exception e) &#123; &#125; &#125; &#125;&#125; 两个队列实现栈 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112/** * Created by zlj on 2020/3/19. * 两个队列实现栈 * 思路： 维护两个队列 Q1 和 Q2 * （1）入栈：即为队列Q1中加入元素 * （2）出栈： 关键就是保持队列q1和q2一直有一个为空 * * - 首先元素入q1，队列中为 [tail]xn-&gt;xn-1...-&gt;x1 [head] 这时将q1中的n-1个元素入q2 q2中元素: [tail] xn-1-&gt;xn-2...-&gt;x1 [head] * - xn从q1中出队 * - 这时q1为空，q2有n-1个元素，重复第一步，只不过现在是将q2中的n-2个元素出队放入q1中，再将q2中的xn-1出队即可。 * - 重复操作直到 q1和q2都为空为止 */public class TwoQueueToStack&lt;T&gt; &#123; // 这里用的Deque 虽然只是需要的是队列 先进先出（FIFO）的特性 但其实Deque既提供了stack的操作、又提供了queue的操作，也提供了对first和end的操作（LinkedList里面叫做head和tail） Queue&lt;T&gt; queue1 = new ArrayDeque&lt;&gt;(); Queue&lt;T&gt; queue2 = new ArrayDeque&lt;&gt;(); /** * 栈的入栈操作 * @param element */ void push(T element) &#123; queue1.offer(element); // 比add更友好 &#125; /** * 栈的出栈操作 * * @return */ T pop() &#123; if (!queue1.isEmpty()) &#123; while (queue1.size() &gt; 1) &#123; // q1出队 入队q2 queue2.offer(queue1.poll()); &#125; // q1的size是1了 return queue1.poll(); &#125; if (!queue2.isEmpty()) &#123; while (queue2.size() &gt; 1) &#123; queue1.offer(queue2.poll()); &#125; // q2的size是1了 return queue2.poll(); &#125; return null; &#125; int size() &#123; if (!queue1.isEmpty()) &#123; return queue1.size(); &#125; else if (!queue2.isEmpty()) &#123; return queue2.size(); &#125; return 0; &#125; /** * 实现一个只查看栈顶元素的操作 * 思路也是先将q1的n-1个元素入队q2，这时将q1中的剩余元素peek出来，不是poll出来（元素不能删除），再将其也导入到q2中 * // 注意这里如果直接用的是双端队列Deque 其实直接可以在不为空的队列中 peekLast = = */ @SuppressWarnings("all") T top() &#123; T top = null; if (!queue1.isEmpty()) &#123; while (queue1.size() &gt; 1) &#123; queue2.offer(queue1.poll()); &#125; top = queue1.peek(); // 再将其入队至q2 queue2.offer(queue1.poll()); &#125; if (!queue2.isEmpty()) &#123; while (queue2.size()&gt;1) &#123; queue1.offer(queue2.poll()); &#125; top = queue2.peek(); queue1.offer(queue2.poll()); &#125; return top; &#125; public static void main(String[] args) &#123; TwoQueueToStack&lt;Integer&gt; twoQueueToStack = new TwoQueueToStack&lt;&gt;(); // 压栈 Stream.iterate(1, i -&gt; i+1).limit(10).forEach(twoQueueToStack::push); // 查看栈顶的元素 System.out.println("栈顶元素：" + twoQueueToStack.top()); // 出栈 int size = twoQueueToStack.size(); for (int i = 0; i &lt;size; i++) &#123; // 注意这里不能写成 i &lt; twoQueueToStack.size() 因为循环中的pop操作会减少stack中的元素 System.out.println("元素出栈" + twoQueueToStack.pop()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>栈、队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[@Async加循环依赖的启动报错问题]]></title>
    <url>%2Fblog%2F2020%2F03%2F18%2FAsync%E5%8A%A0%E5%BE%AA%E7%8E%AF%E4%BE%9D%E8%B5%96%E7%9A%84%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[现象及相关引用博客https://segmentfault.com/a/1190000021217176 问题Spring其实是可以帮助解决循环依赖的，但是在循环依赖的两个bean上有一个加入了@Async注解之后，在启动的时候就报错不能进行循环依赖。 12345678910111213141516171819202122@Componentpublic class A &#123; @Autowired private B b; @Async public void testA() &#123; System.out.println(Thread.currentThread().getName()); &#125;&#125;@Componentpublic class B &#123; @Autowired private A a; public void testB() &#123; System.out.println(&quot;调用到了B&quot;); &#125;&#125; 对应的错误：1org.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name &apos;a&apos;: Bean with name &apos;a&apos; has been injected into other beans [b] in its raw version as part of a circular reference, but has eventually been wrapped. This means that said other beans do not use the final version of the bean. This is often the result of over-eager type matching - consider using &apos;getBeanNamesOfType&apos; with the &apos;allowEagerInit&apos; flag turned off, for example. 注意这里@Transaction虽然也是使用的代理，但是循环引用如果是@Transaction注解 是不影响启动的 可以在最早初始化类实例的时候就能拿到代理对象， 而async是在postProcessor后置处理器当中处理的，所以在循环引用时会放入原始对象而不是代理对象 在之后的check时会报错。这里要做下区分。 问题分析及解决方案报错所在方法：org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory#doCreateBean 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667protected Object doCreateBean( ... )&#123;...boolean earlySingletonExposure = (mbd.isSingleton() &amp;&amp; this.allowCircularReferences &amp;&amp; isSingletonCurrentlyInCreation(beanName));if (earlySingletonExposure) &#123; addSingletonFactory(beanName, () -&gt; getEarlyBeanReference(beanName, mbd, bean));&#125;...// populateBean这一句特别的关键，它需要给A的属性赋值，所以此处会去实例化B~~// 而B我们从上可以看到它就是个普通的Bean（并不需要创建代理对象），实例化完成之后，继续给他的属性A赋值，而此时它会去拿到A的早期引用// 也就在此处在给B的属性a赋值的时候，会执行到上面放进去的Bean A流程中的getEarlyBeanReference()方法 从而拿到A的早期引用~~// 执行A的getEarlyBeanReference()方法的时候，会执行自动代理创建器，但是由于A没有标注事务，所以最终不会创建代理，so B合格属性引用会是A的**原始对象**// 需要注意的是：@Async的代理对象不是在getEarlyBeanReference()中创建的，是在postProcessAfterInitialization创建的代理// 从这我们也可以看出@Async的代理它默认并不支持你去循环引用，因为它并没有把代理对象的早期引用提供出来~~~（注意这点和自动代理创建器的区别~）// 结论：此处给A的依赖属性字段B赋值为了B的实例(因为B不需要创建代理，所以就是原始对象)// 而此处实例B里面依赖的A注入的仍旧为Bean A的普通实例对象（注意 是原始对象非代理对象） 注：此时exposedObject也依旧为原始对象populateBean(beanName, mbd, instanceWrapper);// 标注有@Async的Bean的代理对象在此处会被生成~~~ 参照类：AsyncAnnotationBeanPostProcessor// 所以此句执行完成后 exposedObject就会是个代理对象而非原始对象了exposedObject = initializeBean(beanName, exposedObject, mbd);...// 这里是报错的重点~~~if (earlySingletonExposure) &#123; // 上面说了A被B循环依赖进去了，所以此时A是被放进了二级缓存的，所以此处earlySingletonReference 是A的原始对象的引用 // （这也就解释了为何我说：如果A没有被循环依赖，是不会报错不会有问题的 因为若没有循环依赖earlySingletonReference =null后面就直接return了） Object earlySingletonReference = getSingleton(beanName, false); if (earlySingletonReference != null) &#123; // 上面分析了exposedObject 是被@Aysnc代理过的对象， 而bean是原始对象 所以此处不相等 走else逻辑 if (exposedObject == bean) &#123; exposedObject = earlySingletonReference; &#125; // allowRawInjectionDespiteWrapping 标注是否允许此Bean的原始类型被注入到其它Bean里面，即使自己最终会被包装（代理） // 默认是false表示不允许，如果改为true表示允许，就不会报错啦。这是我们后面讲的决方案的其中一个方案~~~ // 另外dependentBeanMap记录着每个Bean它所依赖的Bean的Map~~~~ else if (!this.allowRawInjectionDespiteWrapping &amp;&amp; hasDependentBean(beanName)) &#123; // 我们的Bean A依赖于B，so此处值为[&quot;b&quot;] String[] dependentBeans = getDependentBeans(beanName); Set&lt;String&gt; actualDependentBeans = new LinkedHashSet&lt;&gt;(dependentBeans.length); // 对所有的依赖进行一一检查~ 比如此处B就会有问题 // “b”它经过removeSingletonIfCreatedForTypeCheckOnly最终返返回false 因为alreadyCreated里面已经有它了表示B已经完全创建完成了~~~ // 而b都完成了，所以属性a也赋值完成儿聊 但是B里面引用的a和主流程我这个A竟然不相等，那肯定就有问题(说明不是最终的)~~~ // so最终会被加入到actualDependentBeans里面去，表示A真正的依赖~~~ for (String dependentBean : dependentBeans) &#123; if (!removeSingletonIfCreatedForTypeCheckOnly(dependentBean)) &#123; actualDependentBeans.add(dependentBean); &#125; &#125; // 若存在这种真正的依赖，那就报错了~~~ 则个异常就是上面看到的异常信息 if (!actualDependentBeans.isEmpty()) &#123; throw new BeanCurrentlyInCreationException(beanName, &quot;Bean with name &apos;&quot; + beanName + &quot;&apos; has been injected into other beans [&quot; + StringUtils.collectionToCommaDelimitedString(actualDependentBeans) + &quot;] in its raw version as part of a circular reference, but has eventually been &quot; + &quot;wrapped. This means that said other beans do not use the final version of the &quot; + &quot;bean. This is often the result of over-eager type matching - consider using &quot; + &quot;&apos;getBeanNamesOfType&apos; with the &apos;allowEagerInit&apos; flag turned off, for example.&quot;); &#125; &#125; &#125;&#125;...&#125; debug看到的对象注入： 可以看到@Async注解标注的Bean的创建代理的时机是在检查bean中引用的之后的。看@EnableAsync注解会通过AsyncConfigurationSelector注入AsyncAnnotationBeanPostProcessor这个后置处理器，在其实现了postProcessAfterInitalization方法，创建代理即在此中。 这里的根本原理是只要能被切面AsyncAnnotationAdvisor切入的Bean都会在后置处理器中生成一个代理对象（如果已经是代理对象，那么加入该切面即可），赋值为上边doCreateBean中的exposedObject作为返回值加入到spring容器中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 关键是这里。当Bean初始化完成后这里会执行，这里会决策看看要不要对此Bean创建代理对象再返回~~~@Overridepublic Object postProcessAfterInitialization(Object bean, String beanName) &#123; if (this.advisor == null || bean instanceof AopInfrastructureBean) &#123; // Ignore AOP infrastructure such as scoped proxies. return bean; &#125; // 如果此Bean已经被代理了（比如已经被事务那边给代理了~~） if (bean instanceof Advised) &#123; Advised advised = (Advised) bean; // 此处拿的是AopUtils.getTargetClass(bean)目标对象，做最终的判断 // isEligible()是否合适的判断方法 是本文最重要的一个方法，下文解释~ // 此处还有个小细节：isFrozen为false也就是还没被冻结的时候，就只向里面添加一个切面接口 并不要自己再创建代理对象了 省事 if (!advised.isFrozen() &amp;&amp; isEligible(AopUtils.getTargetClass(bean))) &#123; // Add our local Advisor to the existing proxy&apos;s Advisor chain... // beforeExistingAdvisors决定这该advisor最先执行还是最后执行 // 此处的advisor为：AsyncAnnotationAdvisor 它切入Class和Method标注有@Aysnc注解的地方~~~ if (this.beforeExistingAdvisors) &#123; advised.addAdvisor(0, this.advisor); &#125; else &#123; advised.addAdvisor(this.advisor); &#125; return bean; &#125; &#125; // 若不是代理对象，此处就要下手了~~~~isEligible() 这个方法特别重要 if (isEligible(bean, beanName)) &#123; // copy属性 proxyFactory.copyFrom(this); 生成一个新的ProxyFactory ProxyFactory proxyFactory = prepareProxyFactory(bean, beanName); // 如果没有强制采用CGLIB 去探测它的接口~ if (!proxyFactory.isProxyTargetClass()) &#123; evaluateProxyInterfaces(bean.getClass(), proxyFactory); &#125; // 添加进此切面~~ 最终为它创建一个getProxy 代理对象 proxyFactory.addAdvisor(this.advisor); //customize交给子类复写（实际子类目前都没有复写~） customizeProxyFactory(proxyFactory); return proxyFactory.getProxy(getProxyClassLoader()); &#125; // No proxy needed. return bean;&#125;// 我们发现BeanName最终其实是没有用到的~~~// 但是子类AbstractBeanFactoryAwareAdvisingPostProcessor是用到了的 没有做什么 可以忽略~~~protected boolean isEligible(Object bean, String beanName) &#123; return isEligible(bean.getClass());&#125;protected boolean isEligible(Class&lt;?&gt; targetClass) &#123; // 首次进来eligible的值肯定为null~~~ Boolean eligible = this.eligibleBeans.get(targetClass); if (eligible != null) &#123; return eligible; &#125; // 如果根本就没有配置advisor 也就不用看了~ if (this.advisor == null) &#123; return false; &#125; // 最关键的就是canApply这个方法，如果AsyncAnnotationAdvisor 能切进它 那这里就是true // 本例中方法标注有@Aysnc注解，所以铁定是能被切入的 返回true继续上面方法体的内容 eligible = AopUtils.canApply(this.advisor, targetClass); this.eligibleBeans.put(targetClass, eligible); return eligible;&#125;]]></content>
      <categories>
        <category>spring</category>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>Async注解</tag>
        <tag>循环依赖</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式id雪花算法]]></title>
    <url>%2Fblog%2F2020%2F03%2F02%2F%E5%88%86%E5%B8%83%E5%BC%8Fid%E9%9B%AA%E8%8A%B1%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[基本原理 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133public class SnowflakeIdWorker &#123; /** * 开始时间截 (2015-01-01) */ private final long twepoch = 1420041600000L; /** * 机器id所占的位数 */ private final long workerIdBits = 5L; /** * 数据标识id所占的位数 */ private final long datacenterIdBits = 5L; /** * 支持的最大机器id，结果是31 (这个移位算法可以很快的计算出几位二进制数所能表示的最大十进制数) */ private final long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); /** * 支持的最大数据标识id，结果是31 */ private final long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); /** * 序列在id中占的位数 */ private final long sequenceBits = 12L; /** * 机器ID向左移12位 */ private final long workerIdShift = sequenceBits; /** * 数据标识id向左移17位(12+5) */ private final long datacenterIdShift = sequenceBits + workerIdBits; /** * 时间截向左移22位(5+5+12) */ private final long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; /** * 生成序列的掩码，这里为4095 (0b111111111111=0xfff=4095) */ private final long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /** * 工作机器ID(0~31) */ private long workerId; /** * 数据中心ID(0~31) */ private long datacenterId; /** * 毫秒内序列(0~4095) */ private long sequence = 0L; /** * 上次生成ID的时间截 */ private long lastTimestamp = -1L; /** * 构造函数 * @param workerId 工作ID (0~31) * @param datacenterId 数据中心ID (0~31) */ public SnowflakeIdWorker(long workerId, long datacenterId) &#123; if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format("worker Id can't be greater than %d or less than 0", maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException(String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId)); &#125; this.workerId = workerId; this.datacenterId = datacenterId; &#125; /** * 获得下一个ID (该方法是线程安全的) * @return SnowflakeId */ public synchronized long nextId() &#123; long timestamp = timeGen(); // 如果当前时间小于上一次ID生成的时间戳，说明系统时钟回退过这个时候应当抛出异常 if (timestamp &lt; lastTimestamp) &#123; throw new RuntimeException( String.format("Clock moved backwards. Refusing to generate id for %d milliseconds", lastTimestamp - timestamp)); &#125; // 如果是同一时间生成的，则进行毫秒内序列 if (lastTimestamp == timestamp) &#123; sequence = (sequence + 1) &amp; sequenceMask; // 毫秒内序列溢出 if (sequence == 0) &#123; //阻塞到下一个毫秒,获得新的时间戳 timestamp = tilNextMillis(lastTimestamp); &#125; &#125; // 时间戳改变，毫秒内序列重置 else &#123; sequence = 0L; &#125; // 上次生成ID的时间截 lastTimestamp = timestamp; // 移位并通过或运算拼到一起组成64位的ID return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) // | (datacenterId &lt;&lt; datacenterIdShift) // | (workerId &lt;&lt; workerIdShift) // | sequence; &#125; /** * 阻塞到下一个毫秒，直到获得新的时间戳 * @param lastTimestamp 上次生成ID的时间截 * @return 当前时间戳 */ protected long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; /** * 返回以毫秒为单位的当前时间 * @return 当前时间(毫秒) */ protected long timeGen() &#123; return System.currentTimeMillis(); &#125; public static void main(String[] args) throws InterruptedException &#123; SnowflakeIdWorker idWorker = new SnowflakeIdWorker(0, 0); for (int i = 0; i &lt; 10; i++) &#123; long id = idWorker.nextId(); Thread.sleep(1); System.out.println(id); &#125; &#125;&#125; 代码实现细节 nextId()方法是加锁的，同进程内需要竞争锁，因为内部有赋值维护sequence（序列号）和lastTimestamp（上次生成id的时间戳）。 参数是组成机器部分的dataCenterId和workerId，各占5个Bit位。 时钟回退是有可能发生的，如果发生了之后是可能会有分布式id重复的问题，这时候直接报错。（条件是当前时间戳 timeStampe &lt; lastTimeStamp） 如果当前时间戳timeStamp和lastTimeStamp相等，说明是同一个时间戳的获取分布式id的流程，这时候通过sequence增加来分配id。 如果sequence + 1之后和sequence的掩码做 &amp;操作，如果算出为0，则说明12位的sequence发生了溢出，这时要将timeStamp更新为下一个时间戳来获取分布式id。 最后根据雪花算法，将移动对应的位之后再做或操作生成对应的64位id。 位运算的运用 sequence++之后判断是否溢出（大于对应的sequence的值的范围），用的是 sequence++ &amp; sequenceMask == 0 来判断，这里掩码是2^12 - 1。与运算是二进制位都是1的时候才是1，其余为0。这里2^12-1的二进制位是 011111111111 做与操作结果为0 说明是和10000000000 做了与操作，即代表了溢出。 最后按照雪花算法从高到低的位置左移对应的长度，再做或操作。或操作是当全为0时，结果的二进制位为0， 其余情况为1。高位比如时间戳已经左移了（10 + 12= 22位），后面的22位都是0， 此时和代表机器位置的数字做或操作，即将机器的10位二进制数字直接填充对应的位置即可。同理sequence的二进制位也一样。这样通过或运算就将 三部分的二进制位拼接了起来。 雪花算法流程]]></content>
      <categories>
        <category>分布式ID</category>
      </categories>
      <tags>
        <tag>分布式ID</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库死锁日志查看]]></title>
    <url>%2Fblog%2F2020%2F01%2F21%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E6%AD%BB%E9%94%81%E6%97%A5%E5%BF%97%E6%9F%A5%E7%9C%8B%2F</url>
    <content type="text"><![CDATA[https://segmentfault.com/a/1190000018730103 1show engine innodb status; 记录锁，间隙锁，Next-key 锁和插入意向锁。这四种锁对应的死锁如下：记录锁（LOCK_REC_NOT_GAP）: lock_mode X locks rec but not gap间隙锁（LOCK_GAP）: lock_mode X locks gap before recNext-key 锁（LOCK_ORNIDARY）: lock_mode X插入意向锁（LOCK_INSERT_INTENTION）: lock_mode X locks gap before rec insert intention 关于显式锁和隐式锁 死锁日志1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556=====================================2018-08-05 21:20:27 0x7fd40c082700 INNODB MONITOR OUTPUT=====================================Per second averages calculated from the last 4 seconds-----------------BACKGROUND THREAD-----------------srv_master_thread loops: 251 srv_active, 0 srv_shutdown, 22663 srv_idlesrv_master_thread log flush and writes: 22905----------SEMAPHORES----------OS WAIT ARRAY INFO: reservation count 513OS WAIT ARRAY INFO: signal count 450RW-shared spins 0, rounds 569, OS waits 286RW-excl spins 0, rounds 127, OS waits 1RW-sx spins 0, rounds 0, OS waits 0Spin rounds per wait: 569.00 RW-shared, 127.00 RW-excl, 0.00 RW-sx------------------------LATEST DETECTED DEADLOCK------------------------2018-08-05 21:15:42 0x7fd40c0b3700*** (1) TRANSACTION:TRANSACTION 1095010, ACTIVE 21 sec insertingmysql tables in use 1, locked 1LOCK WAIT 5 lock struct(s), heap size 1136, 4 row lock(s), undo log entries 2MySQL thread id 16, OS thread handle 140548578129664, query id 3052 183.6.50.229 root updateinsert into t_bitfly values(7,7)*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 2514 page no 4 n bits 72 index num_key of table `test`.`t_bitfly` trx id 1095010 lock_mode X locks gap before rec insert intention waitingRecord lock, heap no 3 PHYSICAL RECORD: n_fields 2; compact format; info bits 32 0: len 4; hex 80000007; asc ;; 1: len 8; hex 8000000000000008; asc ;;*** (2) TRANSACTION:TRANSACTION 1095015, ACTIVE 6 sec insertingmysql tables in use 1, locked 14 lock struct(s), heap size 1136, 4 row lock(s), undo log entries 2MySQL thread id 17, OS thread handle 140548711855872, query id 3056 183.6.50.229 root updateinsert into t_bitfly values(5,5)*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 2514 page no 4 n bits 72 index num_key of table `test`.`t_bitfly` trx id 1095015 lock_mode XRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;;Record lock, heap no 3 PHYSICAL RECORD: n_fields 2; compact format; info bits 32 0: len 4; hex 80000007; asc ;; 1: len 8; hex 8000000000000008; asc ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 2514 page no 4 n bits 72 index num_key of table `test`.`t_bitfly` trx id 1095015 lock_mode X locks gap before rec insert intention waitingRecord lock, heap no 3 PHYSICAL RECORD: n_fields 2; compact format; info bits 32 0: len 4; hex 80000007; asc ;; 1: len 8; hex 8000000000000008; asc ;;省略。。。 一些注释：LATEST DETECTED DEADLOCK：标示为最新发生的死锁；(1) TRANSACTION：此处表示事务1开始 ；MySQL thread id 16, OS thread handle 140548578129664, query id 3052 183.6.50.229 root update：此处为记录当前数据库线程id；insert into t_bitfly values(7,7)：表示事务1在执行的sql ，不过比较悲伤的事情是show engine innodb status 是查看不到完整的事务的sql 的，通常显示当前正在等待锁的sql；(1) WAITING FOR THIS LOCK TO BE GRANTED：此处表示当前事务1等待获取行锁；(2) TRANSACTION：此处表示事务2开始 ；insert into t_bitfly values(5,5)：表示事务2在执行的sql(2) HOLDS THE LOCK(S)：此处表示当前事务2持有的行锁；(2) WAITING FOR THIS LOCK TO BE GRANTED：此处表示当前事务2等待获取行锁；]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>数据库死锁日志</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下typora图床（附带阿里云教程）]]></title>
    <url>%2Fblog%2F2019%2F12%2F10%2Fwindows%E4%B8%8Btypora%E5%9B%BE%E5%BA%8A%2F</url>
    <content type="text"><![CDATA[typoraTypora是大家写博客、记笔记、写文档等日常使用场景下都会使用的一个MarkDown语法的软件，对于熟悉markdown语法和喜欢markdown简洁性的朋友来说，typora是不可或缺的工具。但是，对于图片处理，我们需要图床去将我们的本地图片（截图、流程图之类的）上传到第三方的对象存储上（当然自己的服务器也是可以的）。 本文基于一个typora在windows下的小插件windows下typora图床 来实现实时的粘贴图片到typora即将你的图片上传到阿里云OSS上，并且替换的实现过程，github上已经其实写的比较明白了，但是还是想把自己接入的过程和踩得坑记录一下。 对于typora的使用、阿里云OSS的使用（我记得一年只要个位数的钱）、markdown语法等网上有很多介绍和例子，下面是几个传送门： 阿里云对象存储 typora.io markdown语法 typora图床怎么发现的如果你没有图床，那么在你写博客的过程中如果要使用阿里云图片的外链，得是这样的操作。 将图片上传到阿里云OSS对象存储上 复制该图片的外链 将外链用markdown语法粘贴到正文中 这真的真的相当麻烦。(:з」∠) 所以图床就是用来解决这个问题，但是之前用的图床（之前用过chrome的一个图床插件）都是也只是省去了你登录对象存储在页面上上传的这一步，最后就还是要复制生成的url然后到typora的文章中。 这里就在网上搜了下typora的图床，看看有没有符合自己偷懒的想法的做法，一键截图之后复制到typora中然后就可以了。 然后就是Google搜了下，发现第一条就是日常学（划）习（水）的网站——知乎。 于是就点开之后看到了typora的这个插件，进而有了这个文章 手工教程首先贴一下这个插件的地址：github 这个文档中和知乎的回答差不多，我们可以直接来到使用这里开始： 下载插件的代码到本地，这里不熟悉的github的同学可以直接点击图中的download zip即可。 解压之后可以看到有这些文件，和github上的目录对应 然后按照文档上的教程手工替换（复制plugins目录、替换window.html）到对应的typora安装目录下的resource\app目录下。替换完成之后的目录长这个样子。 然后就是对里面的代码进行自己OSS的配置了。 打开plugins–&gt;image–&gt;upload.js文件（这里可以直接用记事本打开js文件，当然程序员自动sublime或者vscode。），拉到底就可以看到文档中说的init相关的代码。 然后复制文档上的阿里云这段配置，把整个473行替换掉。 接下来就按照注释（“//“后面的东西）来操作即可。 首先插件作者建议你添加一个子账号来单独操作你的OSS，这里简单理解下就是在你的阿里云上你可以建立多个用户组，而每个用户组中可以建立多个子用户，通过对用户组或者用户来设置权限达到一定操作。插件其实是用代码去调用阿里云提供的API来操作上传图片的，所以你肯定要在本地的typora的配置代码里填上一个关联你OSS的账号并且配置对应的权限才能成功上传图片；同时，出于安全考虑，你的这个账号应该只对你的OSS有写入和读的操作权限，所以建议来个子账号专门搞这个事情。 作者其实这里写的也很明白了，包括申请子用户的地址：https://ram.console.aliyun.com/users 打开之后点击新建用户，即可看到让你填用户账户信息。 当然这里也可以直接添加用户组，然后在组下面添加用户。 填写你想要的登录名称（复杂点也没关系，在后面配置一般是关键字搜索选择的）、显示名称和勾选上编程访问。这里的编程访问我们也可以清楚看到是通过assess信息来支持开发者调用API访问的用户。 点击确定，这时要收一个验证码： 填写完之后这步很关键，可以看到会在页面上告知你一个accessKeyId和accessKeySercret，但是坑的地方是这个授权key的值只有在创建的这个页面才能看到，之后就看不到了，所以这里一定要进行复制或者保存这两个值。可以看到页面上也提供了对这两个值的复制功能。 这时候就要给这个用户去设置权限。刚创建的子用户是没有任何权限的，这里你既然要对OSS进行上传写入的操作，肯定要给这个账户权限，可以看到在用户界面有拟刚才创建的子用户，并且可以配置权限 这里在权限搜索框内输入OSS，就可以看到我们要配置的权限（管理OSS的权限），点击确定即可。 到这里，init代码中需要的前三项就都有了。 还剩个bucketDomain，这里作者其实也说的很明白了，在你的OSS概览页面上，会有对应的bucket域名，这里挑选一个即可，我挑选的是外网访问的这个bucket域名 其实文档到这步也就没有了，我就以为是OK了，然后就很有自信的去保存了修改后的js文件去试了下。果然，不行(:з」∠)。 一直在提示我服务响应解析失败，错误。之后又对了遍文档发现也没漏啥。最后还是选择去看了upload代码。 首先这个错误肯定是作者定义的，然后就在upload.js中看到了这个错误。 这里可以看到其实是调用接口异常了，所以返回的这个错误，所以大概率是刚才配置的问题，再往上翻才看到原来除了文件底部的init方法之外，还要去配置下代码中的setting信息，这里稍微吐槽下为啥文档里没有写（没有，就是我前端不熟就没看代码，菜是原罪= =）。 在setting配置里有这段代码： 可以看到请求的域名和API访问是从这里解析的，所以你不配置这里，默认会用作者写死的jiebianjin去访问接口，当然调不通了（因为阿里云上并没有这个子用户）。这里还是刚才的accessKey信息和bucket信息。这里看到了设置了过期时间和上传的大小限制，进而手工改了下大小限制，默认是512k，我这里放大到了5M。 之后保存之后，又信心满满的去试了下，卧槽，居然还是报错。然后又对了下文档中的配置。 无奈打开了调试，typora其实就是个浏览器= =。windows下shift+f12是调试工具。这里没有把当时报错的接口的截图贴上，当时忘记截图了，这里是我在写这篇博客的时候调试截图的。通过一顿分析发现自己在配置BucketDomain的时候，忘记在最后加上一个反斜杠。这里大家在配置的过程中也可以注意下。 给插件的建议首先感谢插件作者 @Thobian 大佬写了这个插件，真的在typora下很方便，应该以后会比较重度使用。这里提几个建议。 完善各个厂商对象存储的配置教程，其实腾讯云OSS我也用过，和阿里云大同小异。 点击再上传这个功能可以加个弹窗之类的中间过程（不能省这个我get到，因为可能失败要重传），在写文过程中点击到图片会自动再上传一次，OSS上会有比较多的重复文件，不好管理 接第二条，我在写文过程中喜欢ctrl+s保存，这个好像也会把我的图片再重新上传一次（不太确定），也会造成大量的相同文件。 性能上有时会卡顿，如果是阿里云接口的锅当我没说哈(:з」∠)。 继续开发更好的功能给大家用，会一如既往的支持的~]]></content>
      <categories>
        <category>日常工具</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>typora</tag>
        <tag>图床</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[方法引用拾遗]]></title>
    <url>%2Fblog%2F2019%2F12%2F09%2F%E6%96%B9%E6%B3%95%E5%BC%95%E7%94%A8%E6%8B%BE%E9%81%97%2F</url>
    <content type="text"><![CDATA[方法引用方法引用是java8引入lambda表达式之后的一个特性，在日常的开发工作中经常使用，可以看做是一种函数指针（function pointer），也可以简单看做lambda表达式的一种语法糖。 方法引用的分类可以将方法引用分为四种： 类::static方法 对象::实例方法 类::实例方法 构造函数引用 前两种应该很好理解，现在来简单记录下第三种。 类::实例方法的方法引用这个看起来并不符合常规，类应该去调用static方法才对，为什么可以直接调用实例方法呢？以代码方式演示会比较好理解。 准备一个java bean。 1234567891011121314151617181920212223242526272829303132333435363738394041public class Student &#123; String name; Integer score; public Student(String name, Integer score) &#123; this.name = name; this.score = score; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getScore() &#123; return score; &#125; public void setScore(Integer score) &#123; this.score = score; &#125; public int compareByName(final Student s2) &#123; return this.getName().compareToIgnoreCase(s2.getName()); &#125; public int compareByScore(final Student s2) &#123; return this.getScore() - s2.getScore(); &#125; @Override public String toString() &#123; return "Student&#123;" + "name='" + name + '\'' + ", score=" + score + '&#125;'; &#125; 可以看到这个Student类中有根据name和score进行比较的方法。 测试类::static方法这种方法引用方式： 1234567891011121314private static void 类实例方法() &#123; Student s1 = new Student("张三", 21); Student s2 = new Student("李四", 22); Student s3 = new Student("王五", 23); List&lt;Student&gt; students = Arrays.asList(s1, s2, s3); students.sort(Student::compareByName); // 在实例方法内部compareByName处打了断点，会被调用两次（list中只有三个元素）， // List.sort方法接收参数是一个Comparator，而这个实例方法是返回int的。 // 这个说明 类::实例方法 这种方法引用是肯定要有一个对象去调用这个实例方法的。这个对象就是传入lambda表达式的list中的第一个Student对象 而后一个student对象会作为compareByName的参数传入 students.forEach(System.out::println); &#125; 可以看到这里students.sort(Student::compareByName);是可以直接传入compareByName方法的。List.sort方法的参数是个Comparator接口，而compareByName返回的是int类型，这里编译器也没有报错。 其实这行就是相当于另外一种写法的 students.sort((st1, st2) -&gt; st1.compareByName(st2)); 这里鼠标点击箭头，可以直接跳转到Comparator接口的，这里的类::实例方法，其实并不是指的是这个实例方法，而是对这个students数组中的每个元素去应用这个compareByName方法，而进行比较的是从数组的第一个元素调用compareByName方法，这里的参数就是第二个元素。 这里也可以打个断点去看到compareByName是被调用两次的。(list中有三个元素) default方法彩蛋默认方法也是java8新增的一个特性，用来解决接口向下（或者老版本）兼容的问题。 这里看到一个有趣的点：当两个接口中有同名默认方法，子类同时实现这两个接口，会出现什么情况呢？ 123456public interface MyInterface1 &#123; default void myMethod() &#123; System.out.println("MyInterface1"); &#125;&#125; 123456public interface MyInterface2 &#123; default void myMethod() &#123; System.out.println("MyInterface2"); &#125;&#125; 123456789101112131415public class 同名默认方法测试 implements MyInterface1, MyInterface2 &#123; @Override public void myMethod() &#123; // 指定实现MyInterface1中的myMethod方法 MyInterface1.super.myMethod(); &#125; public static void main(String[] args) &#123; 同名默认方法测试 m = new 同名默认方法测试(); m.myMethod(); &#125;&#125; 这里当没有指定实现的到底是哪个接口中的默认方法会编译不通过。（这类是用的一种特殊的写法：MyInterface1.super.myMethod()去指定对应的默认方法）。]]></content>
      <categories>
        <category>Java语法</category>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>方法引用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用@DependsOn解决一个spring启动问题]]></title>
    <url>%2Fblog%2F2019%2F08%2F04%2F%E4%BD%BF%E7%94%A8-DependsOn%E8%A7%A3%E5%86%B3%E4%B8%80%E4%B8%AAspring%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言最近遇到了一个启动失败的问题，原因是在bean初始化完成之后的钩子方法中使用获取容器中bean的工具类，(对应工具类之前的一篇博客 获取springbean)）。 分析这里具体的场景是我想实现一个bean在钩子方法中往一个策略map中注册自己作为一个策略使用，但是在启动的时候报错: 第33行代码如下： 123public static &lt;T&gt; T getBean(@NotNull Class&lt;T&gt; tClass) &#123; return context.getBean(tClass); &#125; 可以看到可能为空的是context，这个是通过在项目中启动时注入到ApplicationContextUtil中的静态变量context，很明显是在当前这个bean启动的时候，其钩子方法去调用这个变量还没实现context的注入。 1234@Override public void afterPropertiesSet() throws Exception &#123; // 策略工厂中注册 自身 的代码 &#125; 解决这里主要是一个场景，其实在bean启动的时候是依赖ApplicationContextUtil这个bean的，但是因为getBean方法都static方法，在平常业务代码中调用都是容器启动完毕的时候，所以没有问题，但是这里是想实现在bean初始化时自动通过钩子往一个map工厂中注册bean实例，且该bean没有显示的@Resource依赖ApplicationContextUtil，所以在注册的时候applicationContextutil这个bean还没初始化好，这里在这些具体策略的类上加了@DependsOn(“applicationContextUtil”) 12345@Service@Slf4j@DependsOn(value = "applicationContextUtil")public class AStrategy extends AbstractStrategy &#123;&#125; 这表示这个bean的初始化是依赖 applicationContextUtil 这个bean初始化完成之后(也就是静态变量上下文被注入)才去初始化的，这样启动就不会报NPE了。]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>DependsOn注解</tag>
        <tag>spring启动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lock接口用法和与synchronized的比较]]></title>
    <url>%2Fblog%2F2019%2F07%2F22%2FLock%E6%8E%A5%E5%8F%A3%E7%94%A8%E6%B3%95%E5%92%8C%E4%B8%8Esynchronized%E7%9A%84%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[关于锁的一些知识 可重入锁 如果锁具有可重入性，则称为可重入锁。synchronized和ReentrantLock都是可重入锁。其实可重入锁实际上表明了锁的分配机制：是基于线程的分配还是基于方法调用的分配。比如代码： 123456789class MyClass &#123; public synchronized void method1() &#123; method2(); &#125; public synchronized void method2() &#123; &#125;&#125; 在method1获取到锁之后，再去调用method2是可以重入的，此时锁的对象也是当前调用方法的对象。如果是不可重入的，那么这里在调用method2时，还要去申请获取自身持有的锁。 可中断锁 java中，synchronized是不可中断锁，而Lock是可中断锁。synchronized在未获取到锁时，只能等待，不能响应中断；而Lock接口的lockInterruptibly()方法即体现了lock的可中断性。 公平锁和非公平锁 公平锁即尽量以请求锁的顺序来获取锁，比如多个线程等待锁，这个锁释放时，等待时间最久的线程（最先请求的线程）会获取到锁，这就是公平锁。而非公平锁因为线程的竞争和被调度是不公平的。比如synchronized是非公平锁，无法保证获取锁的顺序。 而ReentrantLock和ReentrantReadWriteLock是可以根据构造函数的参数来设置是公平锁还是非公平锁。1ReentrantLock lock = new ReentrantLock(true); // true代表是公平锁 ReentrantLock也实现了isFair等判断是否为公平锁的方法。 当然公平锁的性能因为要排序所以会在高并发下比非公平锁差。 读写锁 这里指的是维护了两个锁，一个读锁和一个行锁。java中提供了ReentrantReadWriteLock。和数据库的S和X锁一样，读锁和读锁可以共享，不阻塞读操作。而写锁是独占的，不共享。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class ReadWriteLockTest &#123; public static void main(String[] args) &#123; ReadWriteLockDemo readWriteLockDemo = new ReadWriteLockDemo(); for (int i = 0; i &lt; 100; i++) &#123; try &#123; Thread.sleep(200); &#125;catch (Exception e) &#123; &#125; if (i %3 == 0) &#123; // 写线程： new Thread(new Runnable() &#123; @Override public void run() &#123; readWriteLockDemo.setNumber(999); &#125; &#125;).start(); &#125; else &#123; // 读线程 new Thread(new Runnable() &#123; @Override public void run() &#123; readWriteLockDemo.get(); &#125; &#125;).start(); &#125; &#125; &#125;&#125;class ReadWriteLockDemo &#123; private int number; private ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); public void setNumber(int number) &#123; // 获取写锁 Lock lock = readWriteLock.writeLock(); lock.lock(); try &#123; this.number = number; System.out.println(Thread.currentThread().getName() + "修改了number"); &#125; finally &#123; lock.unlock(); &#125; &#125; public void get() &#123; Lock lock = readWriteLock.readLock(); // 这里注意因为lock也可能报错 应该放在try的外边，防止没有加锁而去调用unlock方法导致异常 lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + "读取到了number：" + number); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 关于使用Lock接口的写法 123456789Lock lock = ...;lock.lock();try&#123; //处理任务&#125;catch(Exception ex)&#123; &#125;finally&#123; lock.unlock(); //释放锁&#125; Lock接口需要手动解锁，finally中执行unlock不多说 lock.lock这个方法要放在try之外，这里原因是如果lock失败，执行finally中unlock方法时会报错。当然也可以finally中判断下再unlock。 Lock接口使用Lock接口的出现是因为synchronized的一些缺陷： 被synchronized阻塞的线程等待时无法中断或者超时释放，如果占用锁的线程因为io很耗时，就会很影响性能。 synchronized不支持读写锁这种场景隔离。任何线程的操作都会等待独占锁，也牺牲了性能。 synchronized并不能知道当前线程是否获取到了独占锁，而Lock接口也提供了API去做判断。 当然Lock接口需要用户自己手动执行unlock，否则容易造成死锁。而synchronized关键字是不需要手动释放锁的。 Lock中的方法 lock方法：同步获取锁，如果其他线程已经获取锁，则进行等待。 trylock方法：有返回值，会尝试获取锁，不会阻塞，会立即返回，拿不到锁会返回false。 tryLock(long time, TimeUnit timeunit) ：和tryLock一样，但会阻塞对应的时间。 lockInterruptibly()方法：当通过这个方法获取锁时，如果线程正在等待获取锁，那么这个线程能响应中断。比如线程A获取到了锁，线程B在等待，对线程B调用interrupt能中断B的等待过程，抛出InterruptException。 这里注意因为lockInterruptibly()方法抛出了异常，调用端要处理抛出的异常中断：123456789public void method() throws InterruptedException &#123; lock.lockInterruptibly(); try &#123; //..... &#125; finally &#123; lock.unlock(); &#125; &#125; 这里如果对获取到锁的线程interrupt，是否抛出异常，取决于业务逻辑，比如在Thread.sleep就会响应中断（这个中断和lockInterruptibly没关系，是sleep自身的响应），而LockSupport并不会响应中断。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 private Lock lock = new ReentrantLock(); public static void main(String[] args) throws InterruptedException&#123; LockInterruptiblyTest lockInterruptiblyTest = new LockInterruptiblyTest(); MyTask task1 = new MyTask(lockInterruptiblyTest, &quot;task1线程&quot;); MyTask task2 = new MyTask(lockInterruptiblyTest, &quot;task2线程&quot;); task1.start(); Thread.sleep(200); // 保证让task1线程先获取到锁 task2.start(); task1.interrupt(); // task1获取到锁在执行业务逻辑并不会响应interrupt中断抛出异常 task2.interrupt();// task2在等待锁时抛出异常 &#125; public void biz() throws InterruptedException&#123; lock.lockInterruptibly(); try &#123; for (int i = 0; i &lt; 10 ; i++) &#123; System.out.println(Thread.currentThread().getName() + &quot;第&quot; + i + &quot;次执行业务逻辑&quot;);// TimeUnit.MILLISECONDS.sleep(2000); //注意本身线程休眠就是响应interrupt中断方法的 这里模拟不要用sleep 会影响观察 LockSupport.parkNanos(1000 * 1000 * 200); // LockSupport不响应中断 &#125; &#125; finally &#123; lock.unlock(); System.out.println(Thread.currentThread().getName() + &quot;finally释放了锁&quot;); &#125; &#125; static class MyTask extends Thread &#123; private LockInterruptiblyTest lockInterruptiblyTest; public MyTask(LockInterruptiblyTest test, String name) &#123; setName(name); this.lockInterruptiblyTest = test; &#125; @Override public void run() &#123; try &#123; lockInterruptiblyTest.biz(); &#125; catch (InterruptedException e) &#123; System.out.println(Arrays.toString(e.getStackTrace())); System.out.println(Thread.currentThread().getName() + &quot;等待锁时被interrupt中断&quot;); &#125; &#125; &#125; newCondition()方法：Lock接口还提供了条件Condition，对线程等待、唤醒更加详细和灵活。 在Lock创建出的condition会配合await() 和 signal/signalAll()等方法来实现线程通讯，且Lock能拥有多个condition，即多个等待队列，对比在配合synchronized使用的wait()、nnotify()/notifyAll()只能等待一个条件，非常灵活。 同时在生产者/消费者模型中，Conditio也可与避免synchronized和wait/notify产生的虚假唤醒问题。这里不做过多篇幅。 synchronized和Lock的比较这里总结下他们之间的区别： synchronized是内置的语言实现，是一个关键字，而Lock是一个接口，提供了多种实现。（Lock接口虽然只有ReentrantLock一个实现，但是接口更加灵活，且读写锁虽然没有直接实现Lock接口，但也算Lock的实现。 Lock需要手动去调用unlock方法解锁，而synchronized发生异常或执行完自动释放锁。 Lock功能更加丰富，提供了非阻塞的获取锁、带有时间的获取锁、等待锁线程响应中断、判断是否占有锁、condition条件锁、公平锁等功能。 Lock接口提供了读写锁，对锁使用分了场景，提高了性能。]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>Lock</tag>
        <tag>ReentrantLock</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一道题的思考]]></title>
    <url>%2Fblog%2F2019%2F07%2F22%2F%E6%AF%8F%E6%97%A5%E4%B8%80%E9%97%AE%E4%B8%80%E9%81%93%E9%A2%98%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[题目在小马哥的每日一问中看到了一道这个题：输出什么？。当时看错了在static块中的代码，就毫不意外的答错了= =，这个题其实没有看起来那么简单，这里去记录下这个题。小马哥这个每日一题的系列有很多比较”坑”的题，一般第一遍都比较难答对，推荐每天没事的时候可以去思否上看看这个题，也算拾遗一些基础~ 再来看看这个问题的代码： 123456789101112131415161718public class Lazy &#123; private static boolean initialized = false; static &#123; Thread t = new Thread(() -&gt; initialized = true); t.start(); try &#123; t.join(); &#125; catch (InterruptedException e) &#123; throw new AssertionError(e); &#125; &#125; public static void main(String[] args) &#123; System.out.println(initialized); &#125;&#125; 这个题问的是最后输出的什么。一开始很想当然的就去想输出什么，但是最后在ide中试了下运行，发现启动就卡在了那里_(:з」∠)… 后面就去用jstack看了下线程的情况： 1234567891011121314151617182019-08-03 20:23:45Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.171-b11 mixed mode):"Thread-0" #10 prio=5 os_prio=31 tid=0x00007fece71eb800 nid=0x3d03 in Object.wait() [0x0000700005bbb000] java.lang.Thread.State: RUNNABLE at 函数式设计.设计.Lazy$$Lambda$1/495053715.run(Unknown Source) at java.lang.Thread.run(Thread.java:748)"main" #1 prio=5 os_prio=31 tid=0x00007fece6803800 nid=0x1703 in Object.wait() [0x0000700004c8e000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000007956ffb30&gt; (a java.lang.Thread) at java.lang.Thread.join(Thread.java:1252) - locked &lt;0x00000007956ffb30&gt; (a java.lang.Thread) at java.lang.Thread.join(Thread.java:1326) at 函数式设计.设计.Lazy.&lt;clinit&gt;(Lazy.java:11)JNI global references: 320 发现Thread-0是Runnable状态的，但是是in object.wait() 这里还是卡住了没有执行。 思考这个题里有几个点： （1）static块也是main线程去加载的 （2）匿名内置类和lambda是有区别的 这里去简单说明下，如果在线程中用的是new Runnable的匿名内置类的方式： 1234567891011121314151617181920static &#123; println("static模块加载了"); Thread t = new Thread( // new Runnable 匿名内置类是 通过 Lazy$1.class来实现的 new Runnable() &#123; @Override public void run() &#123; &#125; &#125; ); t.start(); try &#123; t.join(); &#125; catch (InterruptedException e) &#123; throw new AssertionError(e); &#125; &#125; 也就是在看编译生成的字节码目录中会多一个Lazy$1.class文件: 并且在反编译Lazy中看到static块中，依赖这个Lazy$1.class的init方法。 而如果是使用的是像题目中的lambda表达式方式，可以看到字节码文件中并没有Lazy$1.class，而是在反编译class文件中的字节码中多了invokeDynamic指令来实现的lambda表达式： 如果是匿名内之类的方式我们先看如果是换成Runnable匿名内置类方式，而实现的run方法是个空方法体，即代码为： 123456789101112131415161718192021222324252627282930313233private static boolean initialized = false; // static也是由main线程去初始化的 static &#123; println("static模块加载了"); Thread t = new Thread( // new Runnable 匿名内置类是 通过 Lazy$1.class来实现的 new Runnable() &#123; @Override public void run() &#123; System.out.println("匿名内置类执行"); &#125; &#125; ); t.start(); try &#123; t.join(); &#125; catch (InterruptedException e) &#123; throw new AssertionError(e); &#125; &#125; public static void main(String[] args) &#123; println("main线程执行了"); System.out.println(initialized); &#125; private static void println(Object o) &#123; System.out.printf("线程[%s]- %s\n", Thread.currentThread().getName(), o); &#125; 这时启动并不会hang住；将run方法中加入了对static变量initialized的修改或者调用private static方法println，即代码为： 1234567 @Override public void run() &#123; System.out.println("匿名内置类执行"); // 调用 static变量赋值或者static方法就会发生类似于死锁的现象 因为静态变量算这个类的一部分 initialized = true;// println("static方法 打印线程名称执行"); &#125; 再次启动，会发现也hang住出现死锁现象。 其实从上面三点就可以分析出，因为在static模块执行时(Lazy类是不完全初始化的)，这时Runnable类也随之初始化，如果在Runnable类(也就是Lazy$1.class)初始化的时候，还依赖了Lazy的静态变量或者静态方法，那么就会产生字节码直接的循环依赖。 可以在下图中看到字节码中invokestatic指令代表依赖了Lazy的静态内容初始化完成： 再看回这道题如果是lambda表达式，即使run方法中是空实现(即不在run方法中引用static变量或者static方法)，启动也会hang住，这说明lambda来初始化线程并不受是否引用了static内容影响。 这里是因为 invokedDynamic指令是Lazy字节码的一部分，不需要因为引用static方法或者变量来执行，它需要等待Lazy类初始化的完成，而本身初始化完成又依赖invokedDynamaic指令的执行，同时执行的是字节码方法符为run:()Ljava/lang/Runnable，是执行自己的run方法，所以在字节码上也是一个循环依赖。(类加载器loadClass是同步的)。 这里注意下：这里不是只要用了invokeDynamic指令就会发生这个问题，比如方法引用也是通过invokeDynamic指令实现的如果在run方法中使用的是代码： 123456789101112131415static &#123; println("static模块加载了"); Thread t = new Thread( // 方法引用 System.out::println ); t.start(); try &#123; t.join(); &#125; catch (InterruptedException e) &#123; throw new AssertionError(e); &#125; &#125; 但是启动就不会有问题，因为这个等待的是java.io.PrintStream这和类初始化，而这个类初始化是BootStrap类加载器初始化的，早于Lazy类初始化加载，所以能正常运行。 也就是说，在static代码块中： 当使用匿名内置类的时候，注意不要依赖外部类的静态变量或者方法 当使用lambda表达式或者方法引用，注意类的加载的先后顺序，如果依赖不当，会造成启动死锁的情况。]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java枚举拾遗]]></title>
    <url>%2Fblog%2F2019%2F06%2F12%2Fjava%E6%9E%9A%E4%B8%BE%E6%8B%BE%E9%81%97%2F</url>
    <content type="text"><![CDATA[前言java枚举是在开发过程中用的最多的类，这里对java之前的枚举常量类和枚举做了一个分析，并且对枚举相关知识拾遗。 枚举类在出现枚举之前，通常是一个final类去表示”可枚举”这个概念，比如下面这个列举数字的枚举类 1234567891011121314151617181920212223242526/** * 模拟枚举类 (枚举类：在enum出现之前的表达 可枚举的含义的类) * 通常 private 构造函数 * final class * private static final 本类型 成员 * */final class EnumClass&#123; public static final EnumClass ONE = new EnumClass(1); public static final EnumClass TWO = new EnumClass(2); public static final EnumClass THREE = new EnumClass(3); public static final EnumClass FOUR = new EnumClass(4); @Getter @Setter private int value; private EnumClass(int value) &#123; this.value = value; &#125; public void print() &#123; System.out.println(this.toString()); &#125;&#125; 可以看到枚举类的特点： 成员用常量来表示，并且类型为当前类型(当前类型) 常被设置为final类 非public构造器(自己内部来创建实例) 这样有些缺点，比如： 枚举的输出打印的时候要怎么做？每个成员是第几个定义的？要想达到这些操作就必须要写一些方法，而每个枚举类去这样写这样的方法是比较蛋疼的，因为他不具有枚举的values方法。 枚举这里写出对应的java枚举 12345678910111213enum CountingEnum &#123; ONE(1), TWO(2), THREE(3), FOUR(4) ; @Getter private int value; /** private */ CountingEnum (int value) &#123; this.value = value; &#125;&#125; 这里如果想要输出对应的名字和顺序，那么就十分方便了。 1234// 输出 枚举 中的名字、位置、输出所有枚举 Arrays.stream(CountingEnum.values()).forEach(e -&gt; &#123; System.out.println("输出枚举中的顺序: " + e.ordinal() + "名字：" + e.name() + "value：" + e.getValue()); &#125;); 可以看到输出： 这是因为java所有的枚举都是继承Enum抽象类的，而valueOf()方法、ordinal()方法、name()方法都是定义在其中的，可以看下Eunm抽象类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public abstract class Enum&lt;E extends Enum&lt;E&gt;&gt; implements Comparable&lt;E&gt;, Serializable &#123; private final String name; public final String name() &#123; return name; &#125; private final int ordinal; public final int ordinal() &#123; return ordinal; &#125; protected Enum(String name, int ordinal) &#123; this.name = name; this.ordinal = ordinal; &#125; public String toString() &#123; return name; &#125; public final boolean equals(Object other) &#123; return this==other; &#125; public final int hashCode() &#123; return super.hashCode(); &#125; protected final Object clone() throws CloneNotSupportedException &#123; throw new CloneNotSupportedException(); &#125; public final int compareTo(E o) &#123; Enum&lt;?&gt; other = (Enum&lt;?&gt;)o; Enum&lt;E&gt; self = this; if (self.getClass() != other.getClass() &amp;&amp; // optimization self.getDeclaringClass() != other.getDeclaringClass()) throw new ClassCastException(); return self.ordinal - other.ordinal; &#125; @SuppressWarnings("unchecked") public final Class&lt;E&gt; getDeclaringClass() &#123; Class&lt;?&gt; clazz = getClass(); Class&lt;?&gt; zuper = clazz.getSuperclass(); return (zuper == Enum.class) ? (Class&lt;E&gt;)clazz : (Class&lt;E&gt;)zuper; &#125; public static &lt;T extends Enum&lt;T&gt;&gt; T valueOf(Class&lt;T&gt; enumType, String name) &#123; T result = enumType.enumConstantDirectory().get(name); if (result != null) return result; if (name == null) throw new NullPointerException("Name is null"); throw new IllegalArgumentException( "No enum constant " + enumType.getCanonicalName() + "." + name); &#125; /** * enum classes cannot have finalize methods. */ protected final void finalize() &#123; &#125; /** * prevent default deserialization */ private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException &#123; throw new InvalidObjectException("can't deserialize enum"); &#125; private void readObjectNoData() throws ObjectStreamException &#123; throw new InvalidObjectException("can't deserialize enum"); &#125;&#125; 仔细看也许你会有两个疑问： 没看到显示的定义CountingEnum时继承Enum类的？ values方法也没有看到在父类中定义？ 对这两个疑问我们可以去看这个类对应的字节码： 可以看到： enum其实也是final class， 虽然没有显示继承，但是其实是继承了Enum&lt;T&gt;类的，所以可以访问到对应的name，ordinal字段，这个设计让我们输出枚举一些信息的时候很便捷。也提供了valueOf方法，也可以在动态判断枚举的时候使用。 在来看下边的字节码： 可以看到是有values方法，其实这个是jvm通过字节码提升的方式去为枚举做的优化。所以使用枚举可以快速遍历并且一些输出之类的操作。 可以总结下枚举的特点： 枚举其实就是final class，并且继承java.lang.Enum抽象类。 枚举可以实现接口。 枚举不能显示的继承和被继承 留个坑：既然是final class，那么枚举里可以定义抽象方法吗？ 枚举中抽象方法的设计我们在看基础语法的时候，总是说final 和 abstract是互斥的，所以想当然的认为枚举中不能定义抽象方法，但结论其实是可以的。 我们先看一个枚举来实现加减操作的例子： 123456789101112131415enum Opration &#123; PLUS, DIVIDE ; public double apply(double x, double y) &#123; switch (this) &#123; case PLUS: return x + y; case DIVIDE: return x - y; &#125; throw new AssertionError("unknown"); &#125;&#125; 这个实现其实是通过在枚举中加入了非枚举含义的方法和域来实现的操作的一个类型枚举。但是有个问题，当拓展新的操作符时，需要破坏switch中的逻辑，这个不太符合开闭原则，这时候就可以通过把apply作为抽象方法，使得拓展时只需要实现符合自己的抽象逻辑。 123456789101112131415161718192021222324252627/** * 通过抽象方法， 来实现加入新的操作的时候 能符合开闭原则，只关心自己操作符抽象的实现 */enum OperationOptimize &#123; PLUS("+")&#123; @Override public int apply(int x, int y) &#123; return x + y; &#125; &#125;, DIVIDE("-") &#123; @Override public int apply(int x, int y) &#123; return x - y; &#125; &#125; ; @Getter private String str; private OperationOptimize(String str) &#123; this.str = str; &#125; // 抽象方法 public abstract int apply(int x, int y);&#125; 所以枚举是可以定义抽象方法的。 jdk中其实也有对应的例子，可以看下TimeUnit这个时间单位枚举，枚举类型都是通过实现抽象方法(其实是返回异常的普通方法，思想是一样的)来实现不同时间单位的转化。 彩蛋如何给上边的枚举类实现一个values方法？ 因为需要遍历所有的字段，所以很自然的想到了反射去实现。这里需要注意，因为枚举类定义的枚举都是public static final，而作为val变量是int的一个修饰符，需要将除了枚举外的val变量排除~ 示例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950final class EnumClass &#123; public static final EnumClass ONE = new EnumClass(1); public static final EnumClass TWO = new EnumClass(2); public static final EnumClass THREE = new EnumClass(3); public static final EnumClass FOUR = new EnumClass(4); @Getter @Setter private int value; private EnumClass(int value) &#123; this.value = value; &#125; @Override public String toString() &#123; return "EnumClass&#123;" + "value=" + value + '&#125;'; &#125; public void print() &#123; System.out.println(this.toString()); &#125; /** * 为枚举类实现一个values方法 */ public static EnumClass[] values() &#123; // 获取枚举类中所有字段 return Stream.of(EnumClass.class.getDeclaredFields()) // 过滤出 public static final的 .filter(field -&gt; &#123; // 修饰符 int modifiers = field.getModifiers(); return Modifier.isPublic(modifiers) &amp;&amp; Modifier.isStatic(modifiers) &amp;&amp; Modifier.isFinal(modifiers); &#125;) // 取出对应的字段值 .map(field -&gt; &#123; try &#123; return field.get(null); &#125; catch (IllegalAccessException e) &#123; throw new RuntimeException(e); &#125; &#125;).toArray(EnumClass[]::new); &#125;&#125;]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>枚举</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM调优工具及优化原则]]></title>
    <url>%2Fblog%2F2019%2F05%2F30%2FJVM%E8%B0%83%E4%BC%98%E5%B7%A5%E5%85%B7%E5%8F%8A%E4%BC%98%E5%8C%96%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[一些调优工具jmapjmap可以查看内存信息、对象实例个数和占用内存大小。 比如下面命令可以查看堆内存中存活的对象实例个数和占用内存大小1jmap -histo:live pid &gt; ./log.txt 还可以通过命令dump JVM的内存1jmap -dump:format=b,file=CMSdemo.hprof pid jstackjstack可以查看线程堆栈，比如命令：1jstack -e pid jstack能自动输出存在线程死锁的详细信息。（线程状态，互相等待的线程，等待的对象等） 另外在机器CPU飙高之后，可以使用jstack来排查占用cpu时间最多的线程。 top -p pid：此命令是显示pid对应java进程的CPU统计情况。 按H输出当前进程的线程情况（也可以直接top -H -p pid) 找到内存和CPU占用最高的线程tid，转为16进制（在jstack线程堆栈中线程id都是16进制的） 执行jstack -e pid |grep -A 10 16进制线程id即可查看对应的线程堆栈。 当然用Arthas更方便的排查CPU利用率高的线程。 Jinfo查看正在运行的Java程序的扩展参数。 比如查看jvm参数，可以用命令：1jinfo -flags pid 可以用命令，查看jvm的系统参数1info -sysprops 75453 jstatjstast可以看JVM堆内存各部分的使用量，以及加载类的数量。 垃圾回收对象统计（最常用）1jstat -gc pid 2000 100 -- 代表输出进程=pid的java堆gc信息，每2000ms打印一次，打印100次 S0C：Survivor0的大小，单位kb S1C：S1区的大小 S0U：S0的使用大小 S1U：S1的使用大小 EC：Eden区的大小 EU：Eden使用大小 OC：Old区当前空间大小 OU：Old区使用空间大小 CCSC：压缩指针类当前空间大小 CCSU：压缩指针类空间使用大小 YGC：年轻代GC次数 YGCT：年轻代GC消耗时间，单位S FGC：FullGC次数 FGCT：FullGC时间 GCT：垃圾回收消耗的总时间 堆内存统计下面的命令可以查看堆内存统计1jstat -gccapacity pid 新生代垃圾回收统计1jstat -gcnew 75453 1000 10 新生代内存空间1jstat -gcnewcapacity 75453 1000 10 老年代内存统计1jstat -gcoldcapacity 75453 1000 10 老年代垃圾回收统计1jstat -gcold 75453 1000 10 元空间统计1jstat -gcmetacapacity 75453 1000 10 JVM运行情况预估jstat -gc pid命令可以观察到堆内存使用情况和GC情况，则可以通过jstast观测的结果来了解和预估JVM的运行情况。 年轻代对象增长的速率观察EU（Eden区的使用）来估算每秒eden大概新增多少对象，可以根据负载去调整观察频率。注意系统的高峰期和日长期，在不同时间去观测对象增长速率。 YoungGC触发频率和每次耗时知道了年轻代对象增长速率，再根据Eden区的大小就可以知道YoungGC的触发频率，还可以根据YGC / YGCT 来计算出YoungGC每次的耗时。 每次YoungGC后有多少对象存活进入到老年代如果知道了YoungGC没过多久触发了一次，比如1s触发一次，可以用命令jstat -gc pid 1000 10 来打印最近10次的内存和GC情况，观察SU和OU的增长，因为每次YoungGC之后，存活对象会移动到Survivor区或者晋升到老年代中，所以可以看到每次多少对象进入到老年代。 FullGC触发频率和每次耗时FullGC的触发频率可以根据每次YoungGC多少对象进入老年代和老年代的大小来大致估算下。（当然老年代有很多参数，比如CMS垃圾回收器中触发CMS回收的阈值；比如元空间不足、手动调用System.gc()也都会触发FullGC） FullGC的每次耗时可以根据FGCT / FGC来计算得出。 整体优化思路优化思路主要是根据JVM内存分配和对象流转策略的几个点来的，主要是去优化FullGC的频率。同时要对JVM的内存有一个划分： 优化的原则比如： 选择合适的垃圾回收器 CPU单核：Serial可能是最好的选择，单线程进行回收 CPU多核：关注吞吐量，那么要选择Parallal Scavenge和Parallel Old的组合 CPU多核：关注用户停顿时间，内存不大时可以选择CMS+ParNew CPU多核：关注用户停顿时间，内存大于6G时可以选择G1。 增加内存大小 可能是最有效的减少GC频次的方法，但要注意副作用 内存变大，意味着每次GC都要清除更多的对象，要关注GC时间，且选择停顿时间低的垃圾回收器，比如CMS。 在GC之后回收的很少，且占用率逐步上升，可能是存在内存泄漏，这里要进行排查，不能无脑加内存。 设置合理的内存区域大小和分代晋升年龄 如果Eden区太小，可能会频发触发MinorGC，大多数对象都是在年轻代被GC掉，Eden区域不要太小。 如果Survivor太小，可能会提早晋升到老年代。 分代晋升年龄太小，也会导致本应在年轻代被GC的对象晋升到老年代。 根据对象的动态年龄判定来优化对象的动态年龄判定是指的年轻代的对象不一定要在GC分代年龄到达设置阈值才晋升到老年代，如果Survivor空间中相同年龄的所有对象大小总和大于Survivor空间的一半，年龄大于此年龄的对象可以直接老年代，不需要等待达到晋升老年代的阈值。 这里尽量减少YoungGC之后对象在Survivor区占用的比例，可以适当调大Survivor区的大小，避免生命周期短的对象对象提前晋升到老年代，来增加老年代的压力和FullGC的频率。 大对象直接进入到老年代新生代采用复制算法来GC，所以大于阈值的对象会为了避免复制则直接晋升到老年代中。应该尽量避免大对象的产生，因为大多数都是要在年轻代GC掉的对象，进入老年代可能会增加FullGC的频率。（比如未淘汰的缓存对象、未分页查询的SQL结果对象等） 老年代空间分配担保机制可能出现的现象：频繁FullGC、FullGC次数比YoungGC次数都多 在MinorGC之前，只要老年代的连续空间小于新生代对象总和或历次晋升的对象平均大小，就会进行一次FullGC。这时如果可预测有生命周期长的对象，那么可以设置较小的晋升阈值或者较小的晋升年龄提前进入老年代；如果对象都是快速被GC的，可以设置较大的老年代空间。都是为了避免因为空间担保机制频繁FullGC。 元空间不足触发FullGC可能出现的现象：启动过程频繁fullGC，启动时间变长。 如果不设置元空间的JVM相关参数，那么MetaSpace默认大小21m，在启动过程中会不断FullGC，（自动扩大元空间大小，再次触发FullGC的一个过程） CMS垃圾收集器的concurrent mode failure如果在CMS GC的并发标记或者并发清理阶段，再次因为大对象晋升、老年代空间担保机制等原因触发了FullGC，那么会触发Concurrent mode failure，会退化为单线程的Serial Old进行GC，效率低且整个系统会STW。 在确定了大体原因之后，如果要定位有问题的代码，可以使用jmap去看存活对象或者导出内存快照，也可以jstack看占用CPU比较高的线程（一般不断的创建对象代表线程的活跃）区定位具体的问题代码。]]></content>
      <categories>
        <category>Java基础</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[innodb的bufferPool]]></title>
    <url>%2Fblog%2F2019%2F05%2F29%2Finnodb%E7%9A%84bufferPool%2F</url>
    <content type="text"><![CDATA[Buffer Pool的内存结构 由多个缓存数据页和管理这些数据页的链表数据组成。 每个缓存页都有对应的描述数据，各个链表中除了基础节点存储头尾节点、节点个数之外，链表上的都是缓存页的描述数据。 LRU链表在缓存页不足时采用一些列冷热数据分离、热区数据分段移动到链表头部等算法来链表尾部的缓存页刷入磁盘。 这三个链表组合使用，BufferPool中的缓存页的使用、刷盘、空闲管理是一个动态的过程。 多个Buffer Pool和Buffer Pool中的chunk划分 Buffer Pool可以有多个，这样可以降低线程并发操作链表维护的成本，加大并发能力 Buffer Pool中有多个chunk划分，每个chunk是一系列的描述数据块和缓存页，多个chunk共享free、flush、lru链表。这样划分在Buffer Pool动态扩容时可以申请的内存小一点，避免申请多个连续的内存，降低碎片的产生。 show Engine innodb status。可以查看Buffer Pool和管理的链表使用情况。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>BufferPool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis命令拾遗(哈希表)]]></title>
    <url>%2Fblog%2F2019%2F05%2F21%2Fredis%E5%91%BD%E4%BB%A4%E6%8B%BE%E9%81%97-%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[redis哈希表操作命令redis也支持对hash表结构进行操作，体现在下边几个命令。 hset hash field value这个命令]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis哈希表命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM参数分类]]></title>
    <url>%2Fblog%2F2019%2F05%2F20%2Fjvm%E5%8F%82%E6%95%B0%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[JVM参数的分类常用的JVM参数可以大致的分为三类，下边简单的将JVM的参数做一个分类，作为一个JVM参数的简单总结。 JVM标准参数JVM的标准参数是指的在各个JDK版本中都比较稳定的，不会变动的参数，一般是针对jdk全局的参数。 比如 -help -server -client -version -showversion -cp -classpath 这种参数称之为标准参数。 JVM非标准化参数JVM非标准化参数指的是在各个jdk版本中会有略微变化的参数，这里可以分为 -X参数 和 -XX参数 -X参数比如-X参数有： -Xint : 解释执行 -Xcomp ：第一次使用就编译成本地代码（java不是严格的解释性或者编译性执行的语言） -Xmixed : 上面两种混合模式，有编译器自己去优化选择。 可以看到当前电脑上jdk的version中可以显示这个参数的值： 这里是mix模式，我们可以通过命令java -Xint -version来将mode调整为int模式： -XX参数-XX参数使我们平常使用最多的JVM参数，主要用于调优和Debug。 常见的-XX参数有 Boolean类型 格式：-XX:[+/-]&lt;name&gt; 表示启动或者禁用了name参数 比如：-XX:+UseConcMarkSweepGC表示启用了CMS垃圾回收器 ​ -XX:+UseG1GC 表示启用了G1垃圾回收器 非Boolean(Key-Value类型) 格式：-XX:&lt;name&gt;=&lt;value&gt; 表示name参数的值是value 比如：-XX:MaxGCPauseMills=500表示GC最大的停顿时间是500 ​ -XX:GCTimeRatio=19 -XX参数常见的参数还有一种缩写模式（虽然是-X开头，但是其实是-XX参数） 比如： -Xms 等价于-XX:InitialHeapSize 表示初始化堆大小 -Xmx 等价于-XX:MaxHeapSize 表示最大堆的大小 -Xss 等价于 -XX:ThreadStackSize 线程堆栈的大小 可以使用 jinfo -falg &lt;参数名称&gt; 命令行工具去查看当前java进程的JVM参数。 比如： jinfo -flag MaxHeapSize pid 之后会介绍命令行工具去查看对应的JVM参数。]]></content>
      <categories>
        <category>Java基础</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM参数分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis命令拾遗(字符串操作)]]></title>
    <url>%2Fblog%2F2019%2F05%2F19%2Fredis%E5%91%BD%E4%BB%A4%E6%8B%BE%E9%81%97(%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C)%2F</url>
    <content type="text"><![CDATA[前言前一段时间一直在忙，拉下了一些知识的学习，现在努力追赶修补中。= = 当然也有一些新的知识的学习，但其实更多的是关于一些知识的拾遗。之前在工作当中发现对redis命令掌握的还不是很完善，所以想花比较少的碎片时间去写一下redis常用命令的拾遗。 redis命令对这些命令的拾遗记录是在网站：http://redisdoc.com上进行学习的，很简单明了，推荐给大家进行学习拾遗。 这里只是把日常会被忽略或者遗忘的点进行一下梳理，并不是每个知识点的一个总结。 字符串操作####set命令 set可以通过一系列参数进行修改： ex seconds ：将键的过期时间设置为seconds秒，具体的命令是set key value EX seconds。等同于执行setex key second value。 px milliseconds：和ex一样，只不过单位是毫秒，具体的命令是 set key value PX milliseconds。等同于执行psetex key milliseconds value。 nx / xx： set key value nx等价于 setnx key value；set key value xx是当键存在才设置值，没有setxx这个吗命令，这两个设置值失败的时候，set命令会返回nil，而直接使用setnx命令，则返回的是0和1。 setex命令setex命令效果等价于执行下边两个命令： 12set key valueexpire key seconds 但是不同的是，setex是一个原子的操作，它是在同一时间完成设置值和过期时间的操作，经常用在存储缓存时候。 setex设置成功时候 返回ok。 同样psetex只是单位是毫秒而已。 get命令get命令不用多说，但是注意get命令只是用在字符串操作，如果key对应的值不是字符串类型，那么返回一个错误。 getset命令此命令的作用是：将key设置为value，并且返回key在被设置之前的值。如果key之前不存在，则返回nil。当键key存在但不是字符串时，会报错。 strlen命令返回字符串key的长度，当key不是字符串时，返回一个错误。如果key不存在，返回0。 append命令append命令：如果已经存在key并且它的值是一个字符串，append命令将value追加到key对应值的末尾。如果key不存在，append命令会像执行set key value一样将值设置为对应的key的值。 append命令的返回值是值字符串的长度。 注意append的时间复杂度是平摊o(1) setrange key offset value指从偏移量offset开始，用value参数覆写value值。这个命令会确保字符串足够长以便于设置value到对应的偏移量。比如字符串只有5个字符长，但设置的offset是10，那么会在原来字符串值到偏移量之间设置零字节(“\x00”)进行填充。 这个命令的返回值是被修改之后字符串值的长度 getrange key start end这个命令指的是返回键key对应的字符串值的指定部分，字符串的截取范围由start end两个参数决定(包括start和end在内)。start和end支持负数偏移量，-1代表最后一个字符，-2代表倒数第二个字符。但是注意只能按照字符串顺序获取，不能倒序获取(比如 getrange key -1 -3) incr keyincr虽然是自增的含义命令，但其实是一个属于字符串的操作，redis并未提供一个专用的整数类型，所以键key存储的值在执行incr命令的时候会被翻译解释为十进制64位有符号整数。 如果incr操作的key值对应不存在，那么先会初始化为0，然后再执行incr命令。 如果key值不能被解释为数字，那么会返回一个错误。 incrby key increment和incr一样的含义，只不过有递增量为increment。同样的递减是有对应的decr key和decrby key decrement。 incrbyfloat key increment这个就是针对浮点数的增加计算。注意incrbyfloat命令计算的结果最多只保留小数点后面17位。 mset key value [key value …]同时为多个键设置值，这个命令是一个原子操作，所有给定键会在同一时间内被设置，并且具有set的特性，会覆盖key对应原来的值。如果仅是在不存在的情况下设置值，可以用msetnx，msetnx也是一个原子操作，如果多个key中有一个key没有设置上，那么所有的key都不会设置对应的值。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis中Mapper接口的代理逻辑]]></title>
    <url>%2Fblog%2F2019%2F04%2F08%2Fmybatis%E4%B8%ADMapper%E6%8E%A5%E5%8F%A3%E7%9A%84%E4%BB%A3%E7%90%86%E9%80%BB%E8%BE%91%2F</url>
    <content type="text"><![CDATA[特点 （1）Mapper文件中有Mapper接口映射关系的唯一标识，比如findById在接口中定义此方法，那么在mapper.xml肯定也有findById标签对应的sql模板，如果写错会在Mybatis启动的时候报错，达到提前校验的目的。 （2）Mapper接口在使用时不用为其实现接口，就可以自动绑定映射其对应的sql模板执行方法。在spring环境中也可以接口注入直接使用。这里注入的是Mapper接口的代理类。 这些功能就在mybatis框架的binding包下。 binging的核心组件及关系如下： MapperRegistry MapperRegistry是Mybatis初始化过程中构造的一个对象，主要作用是维护Mapper接口和其对应的MapperProxyFactory。 核心字段： 12345678public class MapperRegistry &#123; // 全局唯一的configuration对象，存放解析之后的全部Mybatis配置信息 private final Configuration config; // 已知的所有映射 // key:mapperInterface,即dao的数据库接口 // value:MapperProxyFactory,即映射器代理工厂 private final Map&lt;Class&lt;?&gt;, MapperProxyFactory&lt;?&gt;&gt; knownMappers = new HashMap&lt;&gt;();&#125; ​ addMapper和getMapper方法 addMapper是为Mapper接口添加对应的代理工厂到kownsMapper中。 123456789101112131415161718192021222324public &lt;T&gt; void addMapper(Class&lt;T&gt; type) &#123; // 要加入的肯定是接口，否则不添加 if (type.isInterface()) &#123; // 加入的是接口 if (hasMapper(type)) &#123; // 如果添加重复 throw new BindingException("Type " + type + " is already known to the MapperRegistry."); &#125; boolean loadCompleted = false; try &#123; knownMappers.put(type, new MapperProxyFactory&lt;&gt;(type)); // It's important that the type is added before the parser is run // otherwise the binding may automatically be attempted by the // mapper parser. If the type is already known, it won't try. MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type); parser.parse(); loadCompleted = true; &#125; finally &#123; if (!loadCompleted) &#123; knownMappers.remove(type); &#125; &#125; &#125;&#125; ​ getMapper是获取Mapper接口的一个代理对象，也是通过获取到knownMappers map中的MapperFactoryProxy，然后通过newInstance方法来获取新的代理对象 12345678910111213public &lt;T&gt; T getMapper(Class&lt;T&gt; type, SqlSession sqlSession) &#123; // 找出指定映射接口的代理工厂 final MapperProxyFactory&lt;T&gt; mapperProxyFactory = (MapperProxyFactory&lt;T&gt;) knownMappers.get(type); if (mapperProxyFactory == null) &#123; throw new BindingException("Type " + type + " is not known to the MapperRegistry."); &#125; try &#123; // 通过mapperProxyFactory给出对应代理器的实例 return mapperProxyFactory.newInstance(sqlSession); &#125; catch (Exception e) &#123; throw new BindingException("Error getting mapper instance. Cause: " + e, e); &#125;&#125; MapperProxyFactory MapperProxyFactory逻辑很简单，就是生成代理类的工厂。 其中核心字段为： mapperInterface 即要代理的 Mapper接口 methodCache 用来存放Method和MapperMethod对象的键值对。 这里MapperMethod就是最终在MethodProxy中用于执行sql的地方。 1234567891011121314151617181920212223242526272829303132333435public class MapperProxyFactory&lt;T&gt; &#123; // 对应SQL的java接口类 private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethod&gt; methodCache = new ConcurrentHashMap&lt;&gt;(); /** * MapperProxyFactory构造方法 * @param mapperInterface 映射接口 */ public MapperProxyFactory(Class&lt;T&gt; mapperInterface) &#123; this.mapperInterface = mapperInterface; &#125; public Class&lt;T&gt; getMapperInterface() &#123; return mapperInterface; &#125; public Map&lt;Method, MapperMethod&gt; getMethodCache() &#123; return methodCache; &#125; @SuppressWarnings("unchecked") protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; // 三个参数分别是： // 创建代理对象的类加载器、要代理的接口、代理类的处理器（即具体的实现）。 return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy); &#125; public T newInstance(SqlSession sqlSession) &#123; final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); &#125;&#125; MapperProxy MapperProxy实现了InvocationHandler接口，用于拦截生成代理类。 代理逻辑是利用Method对应的MapperMethod去执行对应execut方法。 123456789101112131415161718192021222324252627282930313233343536373839public class MapperProxy&lt;T&gt; implements InvocationHandler, Serializable &#123; private static final long serialVersionUID = -6424540398559729838L; // 与当前mapperProxy关联的sqlSession对象 用于访问数据库 private final SqlSession sqlSession; // 被代理的mapper接口 private final Class&lt;T&gt; mapperInterface; // 该Map的键为方法，值为MapperMethod对象。通过该属性，完成了MapperProxy内（即映射接口内）方法和MapperMethod的绑定 private final Map&lt;Method, MapperMethod&gt; methodCache; public MapperProxy(SqlSession sqlSession, Class&lt;T&gt; mapperInterface, Map&lt;Method, MapperMethod&gt; methodCache) &#123; this.sqlSession = sqlSession; this.mapperInterface = mapperInterface; this.methodCache = methodCache; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; if (Object.class.equals(method.getDeclaringClass())) &#123; // 继承自Object的方法 // 直接执行原有方法 return method.invoke(this, args); &#125; else if (method.isDefault()) &#123; // 默认方法 // 执行默认方法 return invokeDefaultMethod(proxy, method, args); &#125; &#125; catch (Throwable t) &#123; throw ExceptionUtil.unwrapThrowable(t); &#125; // 找对对应的MapperMethod对象 去执行 final MapperMethod mapperMethod = cachedMapperMethod(method); // 调用MapperMethod中的execute方法 return mapperMethod.execute(sqlSession, args); &#125; private MapperMethod cachedMapperMethod(Method method) &#123; return methodCache.computeIfAbsent(method, k -&gt; new MapperMethod(mapperInterface, method, sqlSession.getConfiguration())); &#125;&#125; MapperMethod MapperMethod是最终执行sql的地方，也是存储了当前执行Mapper接口方法的Method对象。其中包含两个核心字段 sqlCommond、methodSignature。这两个都是其中的静态内部类。 SqlCommand sqlCommand变量维护了关联sql语句的相关信息。 name 即唯一标识 type 标识是哪种类型的sql语句 其在构造函数中根据传入的Mapper接口和method方法来初始化SqlCommond。逻辑其实就是从传入接口或其父类中解析出MapperStatement对象，其能标识mapper.xml中的完整的一个sql模板。再从中解析出name和commandType。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public static class SqlCommand &#123; // SQL语句的名称 即唯一标识 private final String name; // SQL语句的种类，一共分为以下六种：增、删、改、查、清缓存、未知 private final SqlCommandType type; /** * 根据传入接口和方法封装sql信息 * @param configuration * @param mapperInterface * @param method */ public SqlCommand(Configuration configuration, Class&lt;?&gt; mapperInterface, Method method) &#123; // 方法名称 final String methodName = method.getName(); // 方法所在的类。可能是mapperInterface，也可能是mapperInterface的子类 final Class&lt;?&gt; declaringClass = method.getDeclaringClass(); // mapper接口名称、方法名称拼起来做唯一标识 // 到configuration全局配置对象中查找sql语句 // mappedStatement就是Mapper.xml配置文件中一条SQL解析之后得到的对象 MappedStatement ms = resolveMappedStatement(mapperInterface, methodName, declaringClass, configuration); if (ms == null) &#123; if (method.getAnnotation(Flush.class) != null) &#123; // 标记flush注解 name = null; type = SqlCommandType.FLUSH; &#125; else &#123; throw new BindingException("Invalid bound statement (not found): " + mapperInterface.getName() + "." + methodName); &#125; &#125; else &#123; name = ms.getId(); type = ms.getSqlCommandType(); if (type == SqlCommandType.UNKNOWN) &#123; throw new BindingException("Unknown execution method for: " + name); &#125; &#125; &#125; public String getName() &#123; return name; &#125; public SqlCommandType getType() &#123; return type; &#125; /** * 找出指定接口指定方法对应的MappedStatement对象 * @param mapperInterface 映射接口 * @param methodName 映射接口中具体操作方法名 * @param declaringClass 操作方法所在的类。一般是映射接口本身，也可能是映射接口的子类 * @param configuration 配置信息 * @return MappedStatement对象 */ private MappedStatement resolveMappedStatement(Class&lt;?&gt; mapperInterface, String methodName, Class&lt;?&gt; declaringClass, Configuration configuration) &#123; // 数据库操作语句的编号是：接口名.方法名 String statementId = mapperInterface.getName() + "." + methodName; // configuration保存了解析后的所有操作语句，去查找该语句 if (configuration.hasStatement(statementId)) &#123; // 从configuration中找到了对应的语句，返回 return configuration.getMappedStatement(statementId); &#125; else if (mapperInterface.equals(declaringClass)) &#123; // 说明递归调用已经到终点，但是仍然没有找到匹配的结果 return null; &#125; // 从方法的定义类开始，沿着父类向上寻找。找到接口类时停止 for (Class&lt;?&gt; superInterface : mapperInterface.getInterfaces()) &#123; if (declaringClass.isAssignableFrom(superInterface)) &#123; // 递归查找父类 MappedStatement ms = resolveMappedStatement(superInterface, methodName, declaringClass, configuration); if (ms != null) &#123; return ms; &#125; &#125; &#125; return null; &#125;&#125; ​ MethodSignature MethodSignature主要维护了当前接口方法的信息，如返回值类型、参数和实际入参的绑定关系（运用了ParamNameResolver工具类）等。 在methodSignature.convertArgsToSqlCommandParam方法中，也是处理了@Param注解与sql模板中的参数绑定关系。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126public static class MethodSignature &#123; // 返回类型是否为集合类型 private final boolean returnsMany; // 返回类型是否是map private final boolean returnsMap; // 返回类型是否是空 private final boolean returnsVoid; // 返回类型是否是cursor类型 private final boolean returnsCursor; // 返回类型是否是optional类型 private final boolean returnsOptional; // 返回类型 private final Class&lt;?&gt; returnType; // 如果返回为map,这里记录所有的map的key @MapKey注解 private final String mapKey; // resultHandler参数的位置 private final Integer resultHandlerIndex; // rowBounds参数的位置 private final Integer rowBoundsIndex; // 引用参数名称解析器 private final ParamNameResolver paramNameResolver; public MethodSignature(Configuration configuration, Class&lt;?&gt; mapperInterface, Method method) &#123; Type resolvedReturnType = TypeParameterResolver.resolveReturnType(method, mapperInterface); if (resolvedReturnType instanceof Class&lt;?&gt;) &#123; this.returnType = (Class&lt;?&gt;) resolvedReturnType; &#125; else if (resolvedReturnType instanceof ParameterizedType) &#123; this.returnType = (Class&lt;?&gt;) ((ParameterizedType) resolvedReturnType).getRawType(); &#125; else &#123; this.returnType = method.getReturnType(); &#125; this.returnsVoid = void.class.equals(this.returnType); this.returnsMany = configuration.getObjectFactory().isCollection(this.returnType) || this.returnType.isArray(); this.returnsCursor = Cursor.class.equals(this.returnType); this.returnsOptional = Optional.class.equals(this.returnType); this.mapKey = getMapKey(method); this.returnsMap = this.mapKey != null; // rowBoundsIndex和resultHandlerIndex this.rowBoundsIndex = getUniqueParamIndex(method, RowBounds.class); this.resultHandlerIndex = getUniqueParamIndex(method, ResultHandler.class); // 解析参数名称的工具类 this.paramNameResolver = new ParamNameResolver(configuration, method); &#125; public Object convertArgsToSqlCommandParam(Object[] args) &#123; // 通过传入实参获取 实参和参数名称的映射表 return paramNameResolver.getNamedParams(args); &#125; public boolean hasRowBounds() &#123; return rowBoundsIndex != null; &#125; public RowBounds extractRowBounds(Object[] args) &#123; return hasRowBounds() ? (RowBounds) args[rowBoundsIndex] : null; &#125; public boolean hasResultHandler() &#123; return resultHandlerIndex != null; &#125; public ResultHandler extractResultHandler(Object[] args) &#123; return hasResultHandler() ? (ResultHandler) args[resultHandlerIndex] : null; &#125; public String getMapKey() &#123; return mapKey; &#125; public Class&lt;?&gt; getReturnType() &#123; return returnType; &#125; public boolean returnsMany() &#123; return returnsMany; &#125; public boolean returnsMap() &#123; return returnsMap; &#125; public boolean returnsVoid() &#123; return returnsVoid; &#125; public boolean returnsCursor() &#123; return returnsCursor; &#125; /** * return whether return type is &#123;@code java.util.Optional&#125;. * @return return &#123;@code true&#125;, if return type is &#123;@code java.util.Optional&#125; * @since 3.5.0 */ public boolean returnsOptional() &#123; return returnsOptional; &#125; // 返回指定参数的index private Integer getUniqueParamIndex(Method method, Class&lt;?&gt; paramType) &#123; Integer index = null; final Class&lt;?&gt;[] argTypes = method.getParameterTypes(); for (int i = 0; i &lt; argTypes.length; i++) &#123; if (paramType.isAssignableFrom(argTypes[i])) &#123; if (index == null) &#123; index = i; &#125; else &#123; throw new BindingException(method.getName() + " cannot have multiple " + paramType.getSimpleName() + " parameters"); &#125; &#125; &#125; return index; &#125; private String getMapKey(Method method) &#123; String mapKey = null; if (Map.class.isAssignableFrom(method.getReturnType())) &#123; final MapKey mapKeyAnnotation = method.getAnnotation(MapKey.class); if (mapKeyAnnotation != null) &#123; mapKey = mapKeyAnnotation.value(); &#125; &#125; return mapKey; &#125;&#125; execute方法 最终sql的执行都是通过MapperMethod的execute方法执行，这里依赖了其中的sqlCommond和methodSignature两个变量。 execute核心逻辑就是根据具体的sqlCommondType来选择执行具体的方法。其中也处理了不同的返回值 对于Insert、Update、delete类型的sql执行，返回值回采用rowCountResult方法来处理，内部做了对影响行数的处理（可以直接返回boolean类型） 对应Select类型， 如果有executeWithResultHandler类型的参数，会按照resultHandler的回调来处理返回值。 如果方法返回值为集合类型或是数组类型，则会调用 executeForMany() 方法，底层依赖 SqlSession.selectList() 方法进行查询，并将得到的 List 转换成目标集合类型。 如果方法返回值为 Map 类型，则会调用 executeForMap() 方法，底层依赖 SqlSession.selectMap() 方法完成查询，并将结果集映射成 Map 集合。 针对 Cursor 以及 Optional返回值的处理，也是依赖的 SqlSession 的相关方法完成查询的，这里不再展开。 针对单条数据，也会依赖sqlSession.selectOne方法完成查询。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 执行映射接口中的方法 MapperMethod的核心方法 * execute方法会根据要执行的sql语句的具体类型执行sqlsession的具体方法完成数据库操作 * @param sqlSession sqlSession接口的实例，通过它可以进行数据库的操作 * @param args 执行接口方法时传入的参数 * @return 数据库操作结果 */public Object execute(SqlSession sqlSession, Object[] args) &#123; Object result; switch (command.getType()) &#123; // 根据SQL语句类型，执行不同操作 case INSERT: &#123; // 如果是插入语句 // 将参数顺序与实参对应好 Object param = method.convertArgsToSqlCommandParam(args); // 执行操作并返回结果 // rowCounntResult方法会根据方法的返回值类型对结果进行转换 result = rowCountResult(sqlSession.insert(command.getName(), param)); break; &#125; case UPDATE: &#123; // 如果是更新语句 // 将参数顺序与实参对应好 Object param = method.convertArgsToSqlCommandParam(args); // 执行操作并返回结果 result = rowCountResult(sqlSession.update(command.getName(), param)); break; &#125; case DELETE: &#123; // 如果是删除语句MappedStatement // 将参数顺序与实参对应好 Object param = method.convertArgsToSqlCommandParam(args); // 执行操作并返回结果 result = rowCountResult(sqlSession.delete(command.getName(), param)); break; &#125; case SELECT: // 如果是查询语句 /** * 如果在方法参数列表中有 ResultHandler 类型的参数存在，则会使用 executeWithResultHandler() 方法完成查询，底层依赖的是 SqlSession.select() 方法，结果集将会交由传入的 ResultHandler 对象进行处理。 * 如果方法返回值为集合类型或是数组类型，则会调用 executeForMany() 方法，底层依赖 SqlSession.selectList() 方法进行查询，并将得到的 List 转换成目标集合类型。 * 如果方法返回值为 Map 类型，则会调用 executeForMap() 方法，底层依赖 SqlSession.selectMap() 方法完成查询，并将结果集映射成 Map 集合。 * 针对 Cursor 以及 Optional返回值的处理，也是依赖的 SqlSession 的相关方法完成查询的，这里不再展开。 */ if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) &#123; // 方法返回值为void，且有结果处理器 // 使用结果处理器执行查询 executeWithResultHandler(sqlSession, args); result = null; &#125; else if (method.returnsMany()) &#123; // 多条结果查询 result = executeForMany(sqlSession, args); &#125; else if (method.returnsMap()) &#123; // Map结果查询 result = executeForMap(sqlSession, args); &#125; else if (method.returnsCursor()) &#123; // 游标类型结果查询 result = executeForCursor(sqlSession, args); &#125; else &#123; // 单条结果查询 Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); if (method.returnsOptional() &amp;&amp; (result == null || !method.getReturnType().equals(result.getClass()))) &#123; result = Optional.ofNullable(result); &#125; &#125; break; case FLUSH: // 清空缓存语句 result = sqlSession.flushStatements(); break; default: // 未知语句类型，抛出异常 throw new BindingException("Unknown execution method for: " + command.getName()); &#125; if (result == null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) &#123; // 查询结果为null,但返回类型为基本类型。因此返回变量无法接收查询结果，抛出异常。 throw new BindingException("Mapper method '" + command.getName() + " attempted to return null from a method with a primitive return type (" + method.getReturnType() + ")."); &#125; return result;&#125; ​ 总结 重点介绍了 MyBatis 中的 binding 模块，正是该模块实现了 Mapper 接口与 Mapper.xml 配置文件的映射功能。 首先，介绍了 MapperRegistry 这个注册中心，其中维护了 Mapper 接口与代理工厂对象之间的映射关系。 然后，分析了 MapperProxy 和 MapperProxyFactory，其中 MapperProxyFactory 使用 JDK 动态代理方式为相应的 Mapper 接口创建了代理对象，MapperProxy 则封装了核心的代理逻辑，将拦截到的目标方法委托给对应的 MapperMethod 处理。 最后，详细讲解了 MapperMethod，分析了它是如何根据方法签名执行相应的 SQL 语句。]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>Mybatis的Mapper代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo基础（五）——dubbo接口的特性设置]]></title>
    <url>%2Fblog%2F2019%2F03%2F31%2Fdubbo%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%94%EF%BC%89%E2%80%94%E2%80%94dubbo%E6%8E%A5%E5%8F%A3%E7%9A%84%E7%89%B9%E6%80%A7%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[dubbo的一些配置之前的文章中写了dubbo的初步使用和dubbo和springboot的使用整合，这里来总结下dubbo框架暴露接口常用的配置项。 启动时检查dubbo提供了在服务启动时的一些检查机制，这个机制包括consumer端对服务提供者的检查、dubbo对注册中心的检查。可以看下官方文档中dubbo:reference标签中关于check属性的配置。 可以看到是默认在启动时检查服务提供者是否存在，不可用时会抛出异常，组织spring初始化。 我们这里可以将之前demo中的userService服务的消费者上配置check=true(默认就是true)，来看看在不启动服务提供者时的消费者控制台的报错： 是经典的No Provider错误。这里官方文档也对这个配置做了说明： 如果在配置文件中设置了dubbo.reference.check=false，则是强制改变所有reference的check值，就算配置中部分接口有声明，也会被覆盖。 同时，也可以在配置文件中设置dubbo.consumer.check=false，这个是设置所有的reference的check属性默认值，当配置文件中有对单个reference的check的显式设置，会被覆盖掉 这里也可以去看下对注册中心不存在时的错误： 如果我们在reference标签和register标签中显示配置check=”false”，这些错误只有在调用时才会报错。 超时在dubbo的service和reference标签中都有timeout设置，service标签中timeout是1000ms，代表远程服务调用时间；而reference标签中的timeout设置默认值是继承自dubbo:consumer标签中的默认值，也是1000ms。在前几篇dubbo配置中有提到，timeout的配置遵守一个规则： 更精准的优先。方法级别配置会覆盖接口级别的配置，接口级别的配置会覆盖全局的provider、consumer配置。 如果精准级别相同，消费方优先。当配置的颗粒度是相同的，如果消费方设置了超时属性，会覆盖服务提供方超时属性的配置。 比如这里设置服务提供者的超时是5000ms： 123&lt;dubbo:service interface="service.user.UserService" ref="userService" timeout="5000" version="0.0.1" stub="service.user.UserServiceStub"&gt;&lt;/dubbo:service&gt; 而在消费者配置超时是2000ms： 1&lt;dubbo:reference id="userService" interface="service.user.UserService" version="0.0.1" timeout="2000"/&gt; 在服务提供者加入一个3000ms的sleep，这里按照配置的优先级会是消费者的2000ms生效。这里会报dubbo接口调用超时的错误：(错误信息中接口调用花费了2004ms，超时生效的是2000ms配置，报错是waiting server-side response timeout) 重试次数在dubbo:service和dubbo:reference标签中可以设置retries属性来设置接口调用失败时的重试次数，这里重试此时指的是第一次调用之后的重试次数，并且这里的重试会遵守负载均衡的策略。 service标签中的重试次数默认是2，而reference标签的默认重试次数继承consumer标签的默认次数也是2。这里去简单模拟下有三个服务提给者，调用失败的场景是依靠上边的超时设置，重试次数也尊属消费者优先的规则。 在不同的端口暴露userService服务，每个服务会打印第几个服务，并且有对应的睡眠时间模拟调用： 端口是20880，睡眠时间是5000ms 12345&lt;dubbo:protocol name="dubbo" port="20880" /&gt;&lt;dubbo:service interface="service.user.UserService" ref="userService" version="0.0.1" stub="service.user.UserServiceStub"&gt;&lt;/dubbo:service&gt; 端口是20881，睡眠时间是5000ms 12345&lt;dubbo:protocol name="dubbo" port="20881" /&gt;&lt;dubbo:service interface="service.user.UserService" ref="userService" version="0.0.1" stub="service.user.UserServiceStub"&gt;&lt;/dubbo:service&gt; 端口是20882，睡眠时间是2000ms 12345&lt;dubbo:protocol name="dubbo" port="20882" /&gt;&lt;dubbo:service interface="service.user.UserService" ref="userService" version="0.0.1" stub="service.user.UserServiceStub"&gt;&lt;/dubbo:service&gt; 而在消费者端配置超时时间是3000ms，所以只有端口是20882提供的服务可以不会因为超时调用失败。 消费端的配置： 1&lt;dubbo:reference id="userService" interface="service.user.UserService" version="0.0.1" timeout="3000"/&gt; 服务启动之后，发起调用可以看到consumer端的日志： 对20880端口暴露的服务调用超时之后，因为默认的负载均衡策略是加权随机调用，这里重试调用了第三个服务接口调用成功。 这里要知道重试应该配置在幂等的接口上，比如查询、更新等，因为会进行重试进行请求。 多版本功能同一个服务要升级时可能出现服务不稳定的情况，可以使用版本号进行过度，不同的版本号之间是隔离的。 在service标签上配置的version属性就是服务提供者的版本；而reference标签上的version则代表引用服务的版本。 我们也在上边使用了0.0.1版本，而和spring boot整合的dubbo接口版本是boot-1.0.0。 如果配置version=”*” 如下，则表示的是随机引用版本。 &lt;dubbo:reference id=&quot;barService&quot; interface=&quot;com.foo.BarService&quot; version=&quot;*&quot; /&gt; 负载均衡dubbo框架对集群环境下提供了多种负载均衡机制： 基于权重的随机负载均衡 Random LoadBalance (dubbo的默认的负载均衡策略)。 基于权重的轮询负载均衡机制。 最小活跃数负载均衡机制。 一致性hash负载均衡(注意这里是一致性hash，因为集群中的服务节点可以增加、减少)。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo接口属性</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条sql执行的过程]]></title>
    <url>%2Fblog%2F2019%2F03%2F22%2F%E4%B8%80%E6%9D%A1sql%E6%89%A7%E8%A1%8C%E7%9A%84%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[MySql内部组件的结构 如图所示：客户端发来一条SQL语句之后，Mysql内部组件会： 连接器：管理客户端发来的连接，对其中的用户校验权限，管理MySql内部的连接池。 词法分析器：对Sql语法、词法进行校验、分析。解析出一个语法树让MySql理解要去做什么事情。 优化器：会对SQL可选的索引等条件计算成本，生成执行计划给执行器去执行。 执行器：按照执行计划，去调用存储引擎的接口，来获取SQL语句的结果。 一条sql交互的过程 binlog是什么binlog是MySqlServer层实现的二进制逻辑日志，和redo log不同，redo log记录的是物理日志（表空间 + 区号 + 数据页 + 偏移量 + 修改内容），binlog的内容大概是（user表id = 1的记录name更新为xxx）是一个逻辑日志。同时redo log是innodb存储引擎实现事务中的持久性特性而存在的，在其他存储引擎不存在，而binlog是mysql都有的。 Mysql Server层逻辑日志。（所有引擎共享） Binlog是逻辑日志，记录的是语句的原始逻辑。 Binlog是追加写的，不限制大小，不会像redo log覆盖几个文件循环写。 在事务Commit时，会写binlog，这个过程存在于redo log的二阶段提交过程。因为binlog常用于数据恢复和主从同步，所以要保证redo和binlog的一致性采用了两阶段。 Innodb存储引擎执行sql的过程（以更新语句为例） 从磁盘加载数据到Buffer Pool中，innodb中的sql操作都需要加载数据到Buffer Pool中。加载数据都是以数据页的形式加载到内存中，Buffer Pool中也是数据页的形式存在的。 写undo log文件，一次更新语句需要写undolog保证事务的原子性（回滚时找到历史版本的数据）。同时undo log形成版本链，和ReadView来完成MVCC的并发控制。 更新Buffer Pool内存中记录的数据，更新完之后内存数据和磁盘数据不一致，数据页为脏页。 写redo log到内存中的redo log buffer，为后面redo log刷盘做准备。 准备提交事务的阶段，redo log刷新到磁盘，有几种策略（立即刷入、刷入OS缓存、不刷入）。这个阶段也可以理解为redo log的prepare阶段。（两阶段来保证redo log和binlog的一致性） 准备提交事务的阶段，在mysql server层的binlog写入磁盘，也有几种刷盘策略。 提交事务阶段，redolog写入commit标志，redolog的二阶段，此阶段之后事务才算真正提交。 之后的Buffer Pool会根据lru、flush链表的刷盘策略将脏页刷入磁盘。（此步不在事务阶段，线程异步刷入） 为什么在事务阶段写这么多日志？因为innodb引擎要实现事务，undo log其实保证了事务回滚时的原子性，回滚到undo log版本链上的历史数据。 而redo log用于实现持久性，只要redo log和binlog的两阶段完成，就能保证这次变更是crash safe的，不会丢失。 binlog也会用于数据恢复和主从同步，是server层面的二进制逻辑日志，记录了语句信息。 为什么不直接写入磁盘数据文件？ 写那么多的日志都是在文件末尾追加写，相当于是追加写，是顺序IO；而因为更新数据要维护不同的索引树，数据的分布在磁盘上访问是随机IO，效率不是一个数量级的，这样innodb选择去写这些日志，异步线程去刷新内存中的脏页到磁盘上，来提高事务的效率。 在压测数据库可以关注哪些指标？ QPS和TPS IOPS：机器的随机IO能力。每秒可以执行多少个随机IO请求。这个指标很关键，访问磁盘中的数据就是随机IO，压测时候可以观察这个性能。 吞吐量：机器的磁盘每秒可以读写的字节数据量。事务过程中会写redo binlog等日志文件，这个指标决定了大量redo log刷盘的性能。 latency：每写入一条数据的延迟。越低越好。 CPU负载：肯定是重要指标 网络负载：如果带宽打满，肯定也是瓶颈。 内存负载：内存吃紧肯定也是瓶颈。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>sql执行过程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo基础（四）——dubbo的配置加载]]></title>
    <url>%2Fblog%2F2019%2F03%2F20%2Fdubbo%E5%9F%BA%E7%A1%80%EF%BC%88%E5%9B%9B%EF%BC%89%E2%80%94%E2%80%94dubbo%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[dubbo的配置在之前的文章中配置了spring boot和dubbo框架的使用（传送门：springboot使用dubbo框架），看到了把dubbo相关的配置配置在了配置文件中。这里官方文档中也去讲解了对应的dubbo配置的加载。 dubbo的配置加载流程首先要知道dubbo的配置是在应用启动阶段，并且这里的配置包括应用配置、注册中心配置、服务配置等。 dubbo的配置来源 Jvm System Properties，-D参数 Externalized Coniguration， 外部化配置，这里在文档中提到的有zk、apollo ServiceConfig、ReferenceConfig等编程接口采集的配置 项目本地的配置文件 dubbo.properties 除了外部化配置，dubbo的配置读取在总体上遵循了以下几个原则： dubbo支持了多层级的配置，并且按预定优先级自动实现配置间的覆盖，最终所有配置汇总到数据总线URL后，驱动后续的服务暴露、引用等流程。 ApplicationConfig、ServiceConfig、ReferenceConfig也可以理解为配置来源的一种，是直接面向用户编程的配置采集方式 配置格式以Properties为主，在配置上支持path-based的命名规范。 dubbo配置的覆盖策略dubbo的配置覆盖策略如下图： 这里看到dubbo预先设置的配置覆盖加载顺序是jvm设置的启动参数 —&gt; 外部配置 —&gt; spring的xml或者api配置 —&gt; 项目中的本地文件。 这里以上一篇中的spring boot和dubbo整合的demo来测试这个覆盖顺序。这里外部的配置先不做演示，只去比较启动参数、spring配置、本地dubbo.properties配置三个覆盖顺序。 我们以dubbo.protocol.port这个配置作为示例： jvm启动参数 首先配置启动时的vm参数： 启动之后，可以看到此时服务在dubbo-admin上显示的端口为我们在启动参数中设置的20881。(配置文件中配置的是20880，这里优先读取的是启动参数中的配置) spring配置 这里因为使用的是spring boot，所以在application.properties中配置即可。 这里启动provider项目，可以看到之前的接口下线，然后暴露服务的端口变为了20880。 本地的dubbo.properties配置 在本地建立一个dubbo.properties文件，这里写成暴露服务的端口是20882。(这里要主要将spring配置中配置注释掉。) 重新启动provider，可以看到对应的dubbo接口暴露为了20882。 以xml配置说明不同粒度的覆盖和优先级由下图可以知道这些配置标签分为provider侧、consumer侧、应用共享的配置(这里包括application、注册中心、monitor的配置)、还有一些子配置(方法、参数级别的配置)不同的粒度。 这些不同的粒度的配置也有对应的覆盖关系，以timeout为例，其他retries，loadbalance，actives等类似，都遵守粒度之间的覆盖规则： 方法级别优先，接口级别次之，全局配置再次之。 如果级别一样，则消费方优先，提供方次之。 其中，服务提供方配置，通过 URL 经由注册中心传递给消费方。具体如下图所示： 这里就timeout这个超时参数，建议由服务提供方设置超时，因为一个方法需要执行多长时间，服务提供方更清楚，如果一个消费方同时引用多个服务，就不需要关心每个服务的超时设置。 这里的一个小tips： 引用缺省是延迟初始化的，只有引用被注入到其它 Bean，或被 getBean() 获取，才会初始化。如果需要饥饿加载，即没有人引用也立即生成动态代理，可以配置：&lt;dubbo:reference ... init=&quot;true&quot; /&gt;]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo基础（三）——spring boot调用dubbo]]></title>
    <url>%2Fblog%2F2019%2F03%2F20%2Fdubbo%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94spring-boot%E8%B0%83%E7%94%A8dubbo%2F</url>
    <content type="text"><![CDATA[dubbo集成spring bootspring boot肯定是现在用的做多的开发框架，而dubbo框架是最流行的rpc框架之一，整合springboot和dubbo的使用很有必要。本篇博客还是根据上一篇中的dubbo简单demo的简单示例来整合spring boot。(上一篇传送门：dubbo-demo) 依赖因为是springboot项目，dubbo官方也提供了dubbo的starter。 123456&lt;!-- dubbo starter --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt;&lt;/dependency&gt; 这里注意下spring boot的版本和dubbo-starter的版本映射关系，这里使用的是spring boot 2.1.3版本，用的是0.2版本的dubbo依赖，而1.x版本的spring boot框架则对应0.1.x版本的dubbo starter。 这里引入了starter之后也引入了之前在上一篇中的zk客户端依赖。 这里也遇到了idea新建maven module之后的一些坑，一直没办法加载对应的类，这里提示下可以尝试查看idea的maven配置，是不是把新加入的module勾选了ignore： boot-provider这里也是去先构造对应的服务提供者，提供一个用户地址的简单查询服务。但是spring boot多采用注解驱动和避免了很多繁琐的xml配置，所以这里我们去将dubbo的全局配置配置在application.properties文件中，而关于服务的暴露也是用注解暴露。 dubbo应用配置12345678# 应用方信息dubbo.application.name=boot-dubbo-demo# 注册中心地址dubbo.registry.address=zookeeper://127.0.0.1:2181# 协议名称dubbo.protocol.name=dubbo# 协议端口dubbo.protocol.port=20800 可以看到这里其实就是对应的之前普通spring项目中使用dubbo的provider.xml的标签配置。 这里要注意是在启动类上要加入@EnableDubbo注解开启spring boot对dubbo的支持。 服务的暴露这里是用的@Service注解暴露服务，其实也是对应着dubbo-provider.xml中的dubbo:service标签，这里要注意是不要引入是spring的@Service注解。可以看到这个service注解中也有dubbo:service中对应的属性，比如这里写入的version版本信息。 12345678910111213141516171819202122232425262728293031323334353637383940import com.alibaba.dubbo.config.annotation.Service;import javabean.UserAddress;import org.springframework.stereotype.Component;import service.user.UserService;import java.util.ArrayList;import java.util.List;/** * @author 夸克 * @date 2019/3/18 00:15 */@Component@Service(version = "boot-1.0.0")public class UserServiceImpl implements UserService &#123; @Override public List&lt;UserAddress&gt; getUserAddressList(String userId) &#123; System.out.println(Thread.currentThread().getName() + " 调用到了消费者"); final UserAddress userAddress1 = new UserAddress() .setUserId(1L) .setAddressId(1L) .setAddressNo("123") .setAddressStr("庆丰大街") .setUserName("小张"); final UserAddress userAddress2 = new UserAddress() .setUserId(1L) .setAddressId(2L) .setAddressNo("456") .setAddressStr("西湖") .setUserName("小王"); return new ArrayList&lt;UserAddress&gt;()&#123;&#123; add(userAddress1); add(userAddress2); &#125;&#125;; &#125;&#125; 启动provider项目，就可以在dubbo-admin上看到注册到注册中心的服务。 boot-consumer消费者端要配置大体和服务提供者端是一样的，也是用@Refernce注解来代替对应的dubbo:refernce标签。这里也要在启动类上去加入@EnableDubbo注解。 dubbo的配置1234567# 应用方信息dubbo.application.name=boot-dubbo-demo# 注册中心地址dubbo.registry.address=zookeeper://127.0.0.1:2181# 启动端口server.port=8081 这里的端口是8081是因为provider和consumer是两个spring bootmodule 都是启动类去启动的，这里测试在一台电脑上要是不同的端口。 引用暴露的服务123456789101112131415161718192021222324@Servicepublic class OrderServiceImpl implements OrderService &#123; @Reference(version = "boot-1.0.0", timeout = 50000) private UserService userService; /** * 生成订单过程： * 调用远程接口 查询用户信息 * 将用户信息去生成订单 * @return */ @Override public List&lt;UserAddress&gt; initOrder() &#123; List&lt;UserAddress&gt; userAddressList = userService.getUserAddressList("1"); if (null != userAddressList &amp;&amp; userAddressList.size() &gt; 0) &#123; System.out.println("调用远程接口完成"); Optional.of(userAddressList).ifPresent(System.out::println); &#125; return userAddressList; &#125;&#125; 可以看到@Reference注解中也可对应dubbo:reference标签的属性，这里设置的超时时间和对应的版本。 简单controller测试这里去写了一个简单的controller去测试spring-boot使用dubbo这个框架： 1234567891011@RestControllerpublic class TestController &#123; @Resource private OrderService orderService; @GetMapping(path = "/initOrder") public List&lt;UserAddress&gt; initOrder(@RequestParam("userId") Integer userId) &#123; return orderService.initOrder(); &#125;&#125; github地址https://github.com/zhanglijun1217/dubbo-demo]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo基础</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo基础（二）——一个简单调用demo]]></title>
    <url>%2Fblog%2F2019%2F03%2F18%2Fdubbo%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E8%B0%83%E7%94%A8demo%2F</url>
    <content type="text"><![CDATA[get start在上一篇中介绍了dubbo诞生的背景和框架的特性：dubbo概念和基本概念，这里就来一个dubbo的简单使用小体验。 dubbo注册中心安装dubbo中的官方文档的快速启动使用的是multicast广播注册中心暴露服务地址，这里选择的是使用zk作为注册中心，因为zk是很多公司作为dubbo注册中心，并且zk也是dubbo官方文档中推荐使用的注册中心。 本次是使用的mac上安装的zk，安装步骤：mac下安装zk dubbo-demo建立此次是采用maven构建整个项目。整个项目中的module结构如下： 其中： common-interface是抽离出的公用接口，其实这里就是将provider中的接口和consumer中的接口抽离出来 order-service-consumer是此次设置的dubbo-consumer，模拟的是一个订单服务，里面有一个初始化订单的方法。 user-service-provider是此次设置的dubbo服务提供者，模拟的是一个用户服务，在初始化订单时肯定要查询用户服务的接口。 maven依赖此次在项目中的依赖用到了dubbo的依赖，因为使用的是zk注册中心，所以这里要引入zk的客户端依赖，这里要注意的是dubbo2.6版本之后是要引入zk的curator客户端。 dubbo依赖： 123456&lt;!-- https://mvnrepository.com/artifact/com.alibaba/dubbo 这个dubbo版本是集成了spring的 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt;&lt;/dependency&gt; zk client依赖 1234567&lt;!-- curator-framework --&gt;&lt;!-- dubbo2.6 以上版本要引入curator 的 zk客户端 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;2.12.0&lt;/version&gt;&lt;/dependency&gt; provider由dubbo的架构图可知 provider提供的服务要先注册在注册中心上，这里就要去配置provider相关的配置。 可以看到provider要提供的接口服务： 1234public interface UserService &#123; List&lt;UserAddress&gt; getUserAddressList(String userId);&#125; 这里可以给一个简单实现： 1234567891011121314151617181920212223242526public class UserServiceImpl implements UserService &#123; @Override public List&lt;UserAddress&gt; getUserAddressList(String userId) &#123; System.out.println("调用到了消费者"); final UserAddress userAddress1 = new UserAddress() .setUserId(1L) .setAddressId(1L) .setAddressNo("123") .setAddressStr("庆丰大街") .setUserName("小张"); final UserAddress userAddress2 = new UserAddress() .setUserId(1L) .setAddressId(2L) .setAddressNo("456") .setAddressStr("西湖") .setUserName("小王"); return new ArrayList&lt;UserAddress&gt;()&#123;&#123; add(userAddress1); add(userAddress2); &#125;&#125;; &#125;&#125; 这里的关键是provider的xml配置： 12345678910111213141516171819202122232425&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd"&gt; &lt;!-- 提供方应用信息，用于计算依赖关系 --&gt; &lt;dubbo:application name="dubbo-demo" /&gt; &lt;!-- 注册的地址 --&gt; &lt;dubbo:registry address="zookeeper://127.0.0.1:2181" /&gt; &lt;!-- 使用dubbo协议在20880端口暴露服务 --&gt; &lt;dubbo:protocol name="dubbo" port="20880" /&gt; &lt;!-- 生命要暴露的服务接口 --&gt; &lt;dubbo:service interface="service.user.UserService" ref="userService" /&gt; &lt;!-- 实现服务 --&gt; &lt;bean id="userService" class="service.impl.UserServiceImpl" /&gt;&lt;/beans&gt; 其中的配置： application标签，提供了服务提供方应用信息，用于计算依赖关系和dubbo-admin中的界面信息 registry标签，这个标签标明了注册中心和注册中心的地址，这里可以支持多种协议和多种写法，具体见官方文档。 protocol标签，这里是暴露在20880端口的服务，这里的端口可以修改。 service标签，这里是声明要暴露的用户服务接口，ref为spring环境中的bean的id。 bean标签，这里是spring的操作，将实现作为spring bean管理。 这里可以简单的测试一下服务是否注册成功： 123456789101112public class TestProvider &#123; public static void main(String[] args) throws Exception &#123; // 加载spring配置 ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext("provider.xml"); // 启动spring环境 applicationContext.start(); // 使程序hang住 不退出 System.in.read(); &#125;&#125; 启动这个测试程序后可以用telnet测试一下服务是否在本地的zk上注册上去 zk也可以看到多了一个dubbo节点： consumer同样consumer也要先实现order服务的接口 12345678910111213141516171819202122232425@Servicepublic class OrderServiceImpl implements OrderService &#123; @Resource private UserService userService; /** * 生成订单过程： * 调用远程接口 查询用户信息 * 将用户信息去生成订单 * @return */ @Override public boolean initOrder() &#123; List&lt;UserAddress&gt; userAddressList = userService.getUserAddressList("1"); if (null != userAddressList &amp;&amp; userAddressList.size() &gt; 0) &#123; System.out.println("调用远程接口完成"); Optional.of(userAddressList).ifPresent(System.out::println); &#125; return true; &#125;&#125; 配置对应的consumer.xml文件 1234567891011121314151617181920&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;dubbo:application name="dubbo-demo" /&gt; &lt;dubbo:registry address="zookeeper://127.0.0.1:2181" /&gt; &lt;!-- 引用远程接口 --&gt; &lt;dubbo:reference id="userService" interface="service.user.UserService" /&gt; &lt;!-- 启动包扫描 --&gt; &lt;context:component-scan base-package="service.impl" /&gt;&lt;/beans&gt; 这里多了一个dubbo的reference标签，这里就是要引用刚才暴露的接口。 在实现里看到了将实现类作为spring bean管理，这里在xml中开启了包扫描。 这里去写一个简单的test程序去测试写的consumer程序： 123456789101112131415161718public class TestConsumer &#123; @SneakyThrows public static void main(String[] args) &#123; ClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext("consumer.xml"); // 启动spring环境 applicationContext.start(); // 拿到orderService这个bean OrderService bean = applicationContext.getBean(OrderService.class); // 调用 测试是否调用了远程接口 bean.initOrder(); System.in.read(); &#125;&#125; 同时这里也启动这个test类，看控制台中的输出观察是否调用了远程接口。 可以看到在TestConsumer控制台输出 12调用远程接口完成[UserAddress(addressId=1, addressNo=123, addressStr=庆丰大街, userName=小张, userId=1), UserAddress(addressId=2, addressNo=456, addressStr=西湖, userName=小王, userId=1)] 在TestProvider控制台输出 1调用到了消费者 可以看到调用成功。 github此次的代码已上传至github: github]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[synchronized锁对应的内存模型知识]]></title>
    <url>%2Fblog%2F2019%2F03%2F18%2Fsynchronized%E9%94%81%E5%AF%B9%E5%BA%94%E7%9A%84%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[背景synchronized锁在并发编程的学习和面试中都占有着比较重要的地位，而且]]></content>
      <categories>
        <category>并发编程</category>
      </categories>
      <tags>
        <tag>synchronized</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redolog的两提交过程]]></title>
    <url>%2Fblog%2F2019%2F03%2F11%2Fredolog%E7%9A%84%E4%B8%A4%E6%8F%90%E4%BA%A4%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[为什么redo log具有数据恢复能力Mysql的binlog只能用于归档，不足以实现崩溃恢复（Crash-Safe），需要借助引擎层的redo-log才能拥有崩溃恢复的能力。 所谓崩溃恢复，即指在数据库宕机恢复之后，也要保证事务的完整性，不能做一半操作，同时也要注意主从同步的一致性。 简单看下binlog和redo log的区别： 适用对象不同 redo log是针对innodb引擎特有的。 binlog是server层实现的，所有存储引擎可以使用 写入内容不同 binlog是逻辑日志，是语句的原始语义，比如“给id为1的记录的age字段+1”。 redo log是物理日志，记录的是“表空间号 + 数据页 + 偏移量 + 修改内容” 写入方式不同 binlog是追加写入的，在写入一定大小的文件之后会切换下一个文件进行写，不会覆盖和删除之前的文件。 redo log是循环写入的，空间固定会被用完。当redo log两阶段提交落盘之后，会在有限的空间删除这些redo log，是一种循环写。 其中第三点写入方式不同决定了redo log拥有恢复能力，因为追加写没办法看出binlog日志何时写入磁盘的，不能去做区分；而在宕机之后，可以根据redo log中的日志恢复到内存即可，也就是知道要恢复哪些数据。 redo log的两阶段提交一条sql执行的过程 MySql客户端和服务端建立连接，客户端发送一条语句到服务端。 服务端查询缓存，如果命中缓存，返回结果，否则进入下一步 服务端进行SQL解析、预处理、生成合法的语句树。 再由优化器生成对应的执行计划 执行器根据优化器生成的执行计划，调用存储引擎的API进行执行，把结果返回给客户端。 对于一条update的SQL，也是大致上边的步骤，但是在执行器和存储引擎中要多写redo log和binlog的过程。比如对于SQL：1update table set age = age + 1 where id = 1; 执行器：找到存储引擎取id=1的这条记录 存储引擎：在id聚簇索引上找到id=1的这行，加载其数据页到缓冲池。返回给执行器。 执行器：拿到记录之后，把age字段值+1，得到一个新的记录，调用存储引擎的接口写入这行新纪录。 存储引擎：将这行新纪录更新到内存，将这个更新操作写入redo log中，此时redo log为prepare状态。然后告诉执行器执行完成，可以提交事务。 执行器：生成binlog，将binlog写入磁盘。 执行器：调用存储引擎的接口提交事务。 存储引擎：将redo log状态置为commit，更新操作完成 流程图如下： 这里根据两阶段提交的流程，在Crash之后做崩溃恢复流程是这样的： 如果恢复时，redo log事务是完整的，即commit阶段，则直接提交。 如果恢复时，redo log事务是prepare阶段的，这时需要判断binlog的完整性。 a. 如果binlog存在且是完整的，则进行提交事务的操作，再写binlog到磁盘，redo log置为commit。 b. 如果binlog是不完整的，则需要回滚事务。 这里主要是考虑了主备同步的问题，从库都是通过binlog进行主备同步。这里的binlog的完整取决于其自身的格式，比如row模式和statement模式下或者mixed模式下各自的校验，或者各自的checkSum之类的，这里不去纠结。 如果redo log在prepare阶段Crash，但是binlog不完整，那么此时如果去继续提交事务，那么因为崩溃之前的binlog没有生成或者不完整，所以从库是没有这条SQL结果的数据的。所以此时去回滚这个redo log。 反过来，如果是在写入binlog成功之后数据库Crash，那么此时因为 binlog已经写入成功，从库有了这条数据，那么处于prepare阶段的redo log要去commit操作，进行事务的提交来保证主从数据一致性。 所以，处于prepare阶段的redo log和完整的binlog就能保证数据库Crash-Safe了 redo log不用两阶段行不行？假设也是先写redo log，再写binlog，但是redo log是直接commit的，数据已经修改，这时如果写binlog crash了，那么redo log已经提交，主库中有数据但从库此时无法同步数据，就造成数据不一致。 先写binlog到磁盘 后写 redo log行不行主从的架构下，binlog到磁盘，从库同步数据，如果在写redo log时Crash，就会造成主从不一致。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>redo log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次排查$jacocoData的过程]]></title>
    <url>%2Fblog%2F2019%2F03%2F10%2F%E4%B8%80%E6%AC%A1%E6%8E%92%E6%9F%A5-jacocoData%E7%9A%84%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[起因最近在开发过程中，遇到了一个奇怪的现象，在测试环境去利用反射拿一个类的字段时，发现拿到的field数组中多了一个奇怪的变量：$jacocoData，是一个static的boolean数组： 很明显jacoco这种统计代码覆盖率不是我定义在一个业务含义的类中，这时考虑到可能是测试环境中对代码覆盖率在编译时对字节码进行了修改，于是去测试环境的机器上看这个jar包。 疑惑点在机器上对jar包解压（解压命令jar -xvf xxxxxxx.jar），并且在对应的目录下找到对应的calss文件，注意这里解压之后的要看的字节码文件都在BOOT-INF目录的lib下： 将反编译的字节码复制了下来，却发现对应的字节码中并没有这个变量。 这里就不是网上说的很多编译时修改字节码来实现测试覆盖率的功能。 解决后来问了部署jacoco服务的框架组人员，发现是用了java Agent在修改运行时字节码实现的，拉了dump发现确实在运行时进行字节码修改的，而在jacoco官方的github也曾经有过这个问题的issue： jacocoData 关于java Agent技术可以参考博客:agent博客，这里提到了asm技术和agent探针参数。 所以在反射取字段时候遇到这个坑比较难排查，记录一下。这里的解决办法参考了博客:$jacocoData问题的解决，即使用了是否为复合字段的field方法解决。]]></content>
      <categories>
        <category>bug记录</category>
      </categories>
      <tags>
        <tag>反射</tag>
        <tag>字节码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo基础（一）——概念及基本框架]]></title>
    <url>%2Fblog%2F2019%2F03%2F06%2Fdubbo%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[入门dubbo是公司选择rpc框架时首先会去选择的框架，好好了解dubbo框架是一个合格程序员的必经之路。这里作为dubbo的入门篇，把一些概念和官方文档搞清楚一定是最应该开始的步骤。 dubbo框架诞生的背景随着互联网的发展，网站应用的规模不断扩大，常规的垂直应用架构已无法应对，分布式服务架构以及流动计算架构势在必行，亟需一个治理系统确保架构有条不紊的演进。 单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架（ORM）是关键 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需要增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心（SOA）是关键dubbo框架满足的需求在大规模服务化之前，应用只能通过RMI或者Hessian等工具，简单的暴露和引用远程服务，通过配置服务的URL地址进行调用，通过F5等硬件进行负载均衡。此时的诉求是，当服务越来越多时，服务的url配置就会变得非常复杂，负载均衡越来越难做。此时就需要一个服务注册中心，动态的注册和发现服务，使服务的位置透明。并且通过在消费方获取服务提供方地址列表，实现软负载均衡和FailOver（服务提供方挂掉之后的容灾）。这也是后来我们常见的注册中心zk、etcd等。当进一步发展，服务间的依赖关系变得错综复杂，架构关系很难去理清楚，这时候就有一个dubbo治理的需求。当服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑？什么时候该加机器？为了解决这些问题，第一步，将服务每天的调用量，响应时间，都统计出来，作为容量规划的参考指标。其次，可以动态调整权重，把某台机器的权重一直加大，并在加大的过程中记录响应时间的变化，直到响应时间到达阈值，记录此时的访问量，再以此时访问量乘以机器数反推可以承受的总容量。 dubbo框架的整体架构节点角色说明 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册和发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 调用关系说明 服务容器负责启动、加载、运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 dubbo架构的特性可以看到dubbo架构具有以下几个特点：连通性、健壮性、伸缩性、以及向未来架构的扩展升级性。 连通性 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。 监控中心负责统计个服务调用次数、调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以可视化方式展示。（在架构图中可以看到这个是异步发送的） 服务提供者向注册中心注册其提供的服务，并汇报调用时间到监控中心，此事件不包含网络开销 服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心，此时间包含网络开销。 注册中心、服务提供者、服务消费者三者之前均为长连接 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心立即推送事件通知消费者。 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者本地缓存了提供者列表。 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者。健壮性 监控中心宕机不影响使用，只是丢失部分采样数据 数据库宕机后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕机后，将自动切换至另一台。 注册中心全部宕机，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕机后，不影响使用 服务提供者全部宕机后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 伸缩性 注册中心对等集群，可动态增加机器部署实例，所有客户端将自动发现新的注册中心。 服务提供者无状态，可动态增加机器部署实例，注册中心将推送新的服务提供者信息给消费者。升级性当服务集群规模进一步扩大，带动IT治理结构进一步升级，需要实现动态部署，进行流动计算，现有分布式服务架构不会带来阻力。下图是未来可能的一种架构：节点说明：| 节点 | 角色说明 || ———- | ————————————– || Deployer | 自动部署服务的本地代理 || Repository | 仓库用于存储服务应用发布包 || Scheduler | 调度中心基于访问压力自动增减服务提供者 || Admin | 统一管理控制台 || Registry | 服务注册和发现的注册中心 || Monitor | 统计服务的调用次数和调用时间的监控中心 |]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式——装饰者模式]]></title>
    <url>%2Fblog%2F2019%2F03%2F03%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E2%80%94%E2%80%94%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[装饰者模式装饰者模式是java IO流中使用的一个经典模式，本文会简单介绍装饰者模式的原理和解决的问题，并且以一个咖啡demo来演示装饰者模式。 装饰者模式的定义装饰者模式遵守的设计原则：开闭原则（类应该支持扩展，而拒绝修改） 装饰者模式通过组合的方式扩展对象的特性，这种方式允许我们在任何时候对对象的功能进行扩展甚至是运行时扩展，而若我们用继承来完成对类的扩展只能在编译时实现，可以比较动态地对对象进行扩展，某些情况下，装饰者模式比继承更加灵活。 装饰者模式的一些特征 装饰者（decorator）和被装饰（扩展）的对象有着相同的超类。 可以用多个装饰者装饰一个对象。 可以用装饰过的对象替换代码中的原对象，（因为有相同的超类） 装饰者可以在委托（delegate，即调用被装饰的类的成员完成一些工作）被装饰者的行为完成之前或者之后加上他自己的行为 一个对象在任何时候都能被装饰，甚至是运行时。 装饰者模式的结构 Component：一般是一个抽象类，表示一组有着某种用途的基类，包含着这些类最基本的特性。 ConcreteComponent:继承自Component，一般是一个有实际用途的类，这个就是直接的被装饰者。 Decorator：继承自Component，装饰者需要实现的接口（可以是抽象类），用来保证装饰者和被装饰者用共同超类，并保证每一个装饰者有一些必须具有的性质。 ConcreteDecorator：继承自Decorator，用来装饰Component类型的类（ConcreteComponent），但是不能装饰抽象类，为其添加新的特性，可以在委托被装饰者的行为完成之前或之后的任意时候。 装饰者模式的实例一个比较经典的咖啡案例：咖啡馆提供不同种口味的咖啡和咖啡的调料，我们需要一个程序来描述并且计算出任意一种咖啡和任意几种调料搭配在一起的价格。 最初继承的设计最容易想到的是用继承去实现这个需求，定义一个超类，其中只对cost做一个抽象，而让所有的咖啡和调料品之间的组合都去继承这个超类，这样一个设计的坏处是类会爆炸，并且当一种咖啡中调料要变化的时候，要修改原来的类，并且会造成大量的冗余。 好一点的调料放入超类方案也可以想到的是将调料作为一个Boolean变量放入超类中，这样真正的咖啡在继承超类的同时也可以设置想配合的不同调料品。但是这样做的一个缺陷点是如果要引入一个新的调料或者丢弃一个原来的调料，则要对超类进行修改，灵活性受到了很大的挑战。 用装饰者模式去做这个需求 这就是装饰者模式解决这个咖啡馆项目的设计图。 可以看到有一个超类Drink,这个就相当于我们装饰者模式中的Component，可以看到在这个超类中有描述变量、价格变量和cost方法。同时cost方法是一个抽象方法，而子类去实例化时去初始化对应的描述和价格变量。 对应的代码： 123456789101112131415161718192021222324252627/** * 装饰者模式：动态的将新功能附加到对象上，在对象扩展方面，它比继承更有弹性 * 咖啡和调料这么一个项目 * 饮料作为装饰者模式的一个主体 定义一些饮料都有的方法 同时主体也要作为变量在装饰者中定义，装饰者也要继承主体 * * Drink是一个超类 * @date 2019/3/2 17:53 */@Setterpublic abstract class Drink &#123; private String desc; @Getter private float price=0f; public String getDesc() &#123; return desc + "-" + this.getPrice(); &#125; /** * 供子类去实现计算价格的方法 抽象方法 * @return */ public abstract float cost();&#125; 左边的ShortBlack、Decaf等单体咖啡是装饰者模式中的ConcreteComponent，这些子类中都去实现了cost方法，这里注意这个cost方法的实现，因为每个子类都要去实现cost方法，所以可以做一个中间层去实现这个cost方法，对于这里的cost即为返回子类实例化时在构造函数中填充的父类中的price变量值。 123456789101112131415/** * 单体的一个中间层，这里实现了cost方法 * 这里coffee单体 继承了Drink超类，其实cost的计算就是计算他们给price赋值的价格 * * 引入新的单体咖啡时，不会修改之前的单体咖啡代码，只需要继承这个中间层类即可 * * Created by zlj on 2019/3/3. */public class Coffee extends Drink &#123; @Override public float cost() &#123; return super.getPrice(); &#125;&#125; 一个单体咖啡子类的写法： 1234567891011/** * Created by zlj on 2019/3/3. * 一种单体咖啡 */public class LongBlack extends Coffee &#123; public LongBlack() &#123; super.setDesc("LongBlack"); super.setPrice(5.0f); &#125;&#125; Decorator类即装饰者模式中的Decorator，即为装饰者类的一个抽象，这个类由装饰者模式的定义结构我们可以知道也是继承了Component也就是Drink类的，同时，它也定义了一个Drink类型的变量，这里即用父类定义了要装饰的单体咖啡。因为它继承自Drink类，所以它也要去实现抽象方法cost方法，这里注意因为要在调用时展示一个咖啡的描述和价格时，作为装饰者需要将被装饰者和其他组合在一起的装饰者价格和描述一起展示出来，所以实现是将装饰者的描述和价格 同 被装饰者的描述和价格一起递归显示，所以代码如下，这里的Decorator也相当于真正装饰者的一个抽象或者中间层。这里也要注意装饰者可以装饰一个单体咖啡，也可以装饰一个被装饰过的装饰者。 12345678910111213141516171819202122232425262728293031323334353637/** * 装饰者的一个中间层 其实就是装饰者类 这里装饰的是主体咖啡， 而继承装饰者的都是咖啡中的调料 * * 1. 装饰者类是要继承主体类 * 2. 装饰者可以直接装饰主体，也可以是已经被装饰者包装过的，就是多个调料 * 3. 装饰者要定义一个主体变量在其中 * * 当要拓展 一个新的装饰者时，直接继承装饰者中间层类即可。 * Created by zlj on 2019/3/3. */public class Decorator extends Drink &#123; /** * 被装饰的主体变量 */ private Drink obj; public Decorator(Drink obj) &#123; this.obj = obj; &#125; /** * 这里也去实现cost方法，因为本身装饰者也是有价格的 * 这里的obj.cost() 如果obj是一个被装饰过的类，这里会进行递归计算 * @return */ @Override public float cost() &#123; return super.getPrice() + obj.cost(); &#125; @Override public String getDesc() &#123; return super.getDesc() + "&amp;&amp;" + obj.getDesc(); &#125;&#125; 图中的Soy、Milk类就是其中的调料类，这个要继承Decorator，作用其实就是ConcreteDecorator。这里的作用其实比较简单，其实就是传入被装饰对象（上边也提到这里可以是单体咖啡和被装饰过的装饰者），还有就是把描述和价格填上。 1234567891011121314/** * 单个继承装饰者的调料类 * 要在构造函数中将装饰的对象 设置进去 * * Created by zlj on 2019/3/3. */public class Chocolate extends Decorator &#123; public Chocolate(Drink obj) &#123; super(obj); super.setDesc("Chocolate"); super.setPrice(1.0f); &#125;&#125; 简单测试做一个简单的程序测试一下装饰者模式： 1234567891011121314151617181920212223242526272829303132333435363738public class TestMain &#123; public static void main(String[] args) &#123; /** * 只是点了一个单体咖啡 */ Drink order1; order1 = new Espresso(); System.out.println(order1.getDesc()); System.out.println("*******************************"); /** * 点有调料的咖啡 */ Drink order2; // 先是一杯单体咖啡 order2 = new LongBlack(); // 加牛奶 order2 = new Milk(order2); // 加巧克力 order2 = new Chocolate(order2); // 加盐 order2 = new Soy(order2); System.out.println(order2.getDesc() + "总共钱： " + order2.cost()); &#125;&#125;// 运行结果：Espresso-2.0*******************************Soy-3.0&amp;&amp;Chocolate-1.0&amp;&amp;Milk-2.0&amp;&amp;LongBlack-5.0总共钱： 11.0 java中的装饰者模式文章一开始就提到了java中的IO流使用了装饰者模式，这里简单的去看一个FilterInputStream类，而它继承的InputStream类即装饰者中的Component类，该类其实就是装饰者的中间层，可以看到其中定义了InputStream的变量引用，而其子类BufferInputStream、DataInputStream、LineNumberInputStream等是ConcreteDecorator。结构如下： 也可以看到代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455publicclass FilterInputStream extends InputStream &#123; // 被装饰的变量 protected volatile InputStream in; protected FilterInputStream(InputStream in) &#123; this.in = in; &#125; public int read() throws IOException &#123; return in.read(); &#125; public int read(byte b[]) throws IOException &#123; return read(b, 0, b.length); &#125; public int read(byte b[], int off, int len) throws IOException &#123; return in.read(b, off, len); &#125; public long skip(long n) throws IOException &#123; return in.skip(n); &#125; public int available() throws IOException &#123; return in.available(); &#125; public void close() throws IOException &#123; in.close(); &#125; public synchronized void mark(int readlimit) &#123; in.mark(readlimit); &#125; public synchronized void reset() throws IOException &#123; in.reset(); &#125; public boolean markSupported() &#123; return in.markSupported(); &#125;&#125; 再唠叨几句装饰者和继承装饰者模式中用到了继承，但是这里装饰者模式用继承是为了保证装饰者和被装饰者有共同的超类，而不是为了给被装饰的类扩展新的特性，而装饰者模式中新的特性是通过类与类的组合（has-a的关系）而得到的，所以把装饰者模式和继承看做同一类设计思想是不恰当的。 装饰者模式的优点针对第一种继承的方案：装饰者模式减少了代码的类，只定义了基础的Component和Decorator，而是通过组合的方式实现具体的咖啡的。 针对第二种在超类中定义Boolean变量的方案，装饰者模式提高了灵活性，当你需要加入新的调料或者单体咖啡时，只需定义新的ConcreteComponent和ConcreteDecorator即可。而当你想对一个已有加过调料的咖啡进行再装饰时，也只需要将对应的调料装饰上去即可，价格和描述即可递归显示。 参考http://www.cnblogs.com/coffeeSS/p/5405787.html 《headFirst 设计模式》 代码在github: https://github.com/zhanglijun1217/design_mode/tree/master/src/main/java/decorator]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>装饰者模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用telnet测试dubbo接口初使用]]></title>
    <url>%2Fblog%2F2019%2F02%2F25%2F%E4%BD%BF%E7%94%A8telnet%E6%B5%8B%E8%AF%95dubbo%E6%8E%A5%E5%8F%A3%E5%88%9D%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[背景dubbo接口的测试不像controller的http接口那么容易测试，这里去了解了下使用telnet去测试参数没那么复杂的dubbo接口。 正题首先看看一个dubbo接口的代码： 12345678910111213141516public interface ShopAggregateRemoteService &#123; /** * 获取所有产品类型 聚合字段的产品类型范围[软件+有伴服务] * @param careDeleteFlag 是否关心软删 true：只返回生效的 false：全部返回 * @return */ ListResult&lt;AggregateProductTypeDTO&gt; getAllAggregateProductType(boolean careDeleteFlag); /** * 获取所有产品类型 聚合字段的产品类型范围[软件+有伴服务] * @return */ default ListResult&lt;AggregateProductTypeDTO&gt; getAllAggregateProductType() &#123; return getAllAggregateProductType(false); &#125;&#125; 里面的实现：(就是一个简单的查数据库操作) 123456789101112@Overridepublic ListResult&lt;AggregateProductTypeDTO&gt; getAllAggregateProductType(boolean careDeleteFlag) &#123; AggregateProductTypeCriteria example = new AggregateProductTypeCriteria(); AggregateProductTypeCriteria.Criteria criteria = example.createCriteria(); if (careDeleteFlag) &#123; // 关心软删 加入软删标志 criteria.andDeleteFlagEqualTo(0); &#125; return DTOWrapper.wrap(customerConverter.convertList(aggregateProductTypeMapper.selectByCondition(example), AggregateProductTypeDTO.class));&#125; 这里想本地启动项目去测试一下这个接口有没有问题，这个参数也比较简单（这里直接调用default方法就可以了）。 这里使用的mac系统，但是mac系统中高版本未默认安装telnet，所以还需要brew去安装一下。 brew install telnet 安装完成之后，就可以在本地将服务启动。 输入命令去telnet这个服务的dubbo接口： telnet localhost 7100 然后可以看到进入到了dubbo的命令行界面： 这里可以用dubbo的ls命令去查看有什么dubbo服务： 最后是我们的invoke调用这个命令 invoke com.youzan.xxx.getAllAggregateProductType() 可以看到结果是调通的： 总结这里使用的dubbo的简单命令去测试了工作中的一个接口，之后会对dubbo的使用和测试做更多的分析。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>telent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——线程状态（2）]]></title>
    <url>%2Fblog%2F2019%2F02%2F24%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E2%80%94%E2%80%94%E7%BA%BF%E7%A8%8B%E7%8A%B6%E6%80%81%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[线程的状态线程的状态很早之前就理解过了，最近翻《并发编程艺术》的书时候，看到有个点之前理解的不太对。 书中的线程状态分类 new 初始状态，线程被构建，但是还没有调用start()方法 runnable 运行状态，java中将操作系统中的就绪和运行两种状态笼统称作”运行中“（这里没有网上常见的Running状态） blocked 阻塞状态，表示线程阻塞于锁 waiting 等待状态，表示线程进入等待状态，进入该状态表示当前线程需要等待其他线程做出一些特定动作（通知或中断） time_waiting 超时等待状态，该状态不同于waiting，可以在指定的时间自行返回。（可以看到这里将waiting和time_waiting分开了，这样也很符合我们用jstack命令看到的线程状态信息） terminated 终止状态，表示线程已经执行完毕。 一个线程状态演示的例子在书中有这样的一个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * Created by zlj on 2019/2/23. */public class ThreadState &#123; public static void main(String[] args) &#123; // TimeWaiting 状态的线程 new Thread(new TimeWaiting(), "TimeWaitingThread").start(); // Waiting状态的线程 new Thread(new Waiting(), "WaitThread").start(); // 模拟同步锁 即未抢占到锁的线程为blocked状态的线程 new Thread(new Blocked(), "BlockedThread-1").start(); new Thread(new Blocked(), "BlockedThread-2").start(); &#125; /** * 不断的进行睡眠 模拟线程的超时等待状态 */ static class TimeWaiting implements Runnable &#123; @Override public void run() &#123; while (true) &#123; SleepUtils.second(1000); &#125; &#125; &#125; /** * 该线程在Waiting.class上进行等待 模拟线程的等待状态 */ static class Waiting implements Runnable &#123; @Override public void run() &#123; while (true) &#123; synchronized (Waiting.class) &#123; try &#123; // 进行等待 Waiting.class.wait(); &#125; catch (InterruptedException e) &#123; &#125; &#125; &#125; &#125; &#125; /** * 模拟线程阻塞状态，一个线程获取了Blocked.class的锁 并且不会释放该锁 */ static class Blocked implements Runnable &#123; @Override public void run() &#123; synchronized (Blocked.class) &#123; while (true) &#123; SleepUtils.second(1000); &#125; &#125; &#125; &#125;&#125; 这里用了jps和jstack命令去观察了线程状态： 12345678910111213141516171819202122232425262728293031323334// blockedThread-2线程阻塞在获取Blocked.class的锁上"BlockedThread-2" #15 prio=5 os_prio=0 tid=0x000000001a27a800 nid=0x31dc waiting for monitor entry [0x000000001b11e000] java.lang.Thread.State: BLOCKED (on object monitor) at lesson.wwj.juc.thread_status.ThreadState$Blocked.run(ThreadState.java:63) - waiting to lock &lt;0x00000000d5c3e6d0&gt; (a java.lang.Class for lesson.wwj.juc.thread_status.ThreadState$Blocked) at java.lang.Thread.run(Thread.java:745)// 获取到了锁 "BlockedThread-1" #14 prio=5 os_prio=0 tid=0x000000001a278800 nid=0x4538 waiting on condition [0x000000001b01e000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at java.lang.Thread.sleep(Thread.java:340) at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386) at lesson.wwj.juc.thread_status.SleepUtils.second(SleepUtils.java:12) at lesson.wwj.juc.thread_status.ThreadState$Blocked.run(ThreadState.java:63) - locked &lt;0x00000000d5c3e6d0&gt; (a java.lang.Class for lesson.wwj.juc.thread_status.ThreadState$Blocked) at java.lang.Thread.run(Thread.java:745)// 处于waiting状态的线程"WaitThread" #13 prio=5 os_prio=0 tid=0x000000001a274000 nid=0x4c84 in Object.wait() [0x000000001af1f000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000d5c3b5a0&gt; (a java.lang.Class for lesson.wwj.juc.thread_status.ThreadState$Waiting) at java.lang.Object.wait(Object.java:502) at lesson.wwj.juc.thread_status.ThreadState$Waiting.run(ThreadState.java:44) - locked &lt;0x00000000d5c3b5a0&gt; (a java.lang.Class for lesson.wwj.juc.thread_status.ThreadState$Waiting) at java.lang.Thread.run(Thread.java:745)// 处于超时等待的线程"TimeWaitingThread" #12 prio=5 os_prio=0 tid=0x000000001a25f000 nid=0x5b44 waiting on condition [0x000000001ae1f000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at java.lang.Thread.sleep(Thread.java:340) at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386) at lesson.wwj.juc.thread_status.SleepUtils.second(SleepUtils.java:12) at lesson.wwj.juc.thread_status.ThreadState$TimeWaiting.run(ThreadState.java:27) at java.lang.Thread.run(Thread.java:745) 书中的状态转换图 这里要注意的一点：阻塞状态是线程阻塞在进入synchronized关键字修饰的方法或者代码块（获取锁）时的状态，但是阻塞在java.concurrent包中Lock接口的线程状态却是等待状态，因为Lock接口对于阻塞的实现均使用了LockSupport类中相关的方法。]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>线程状态</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——Thread API]]></title>
    <url>%2Fblog%2F2019%2F02%2F21%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E2%80%94%E2%80%94Thread-API%2F</url>
    <content type="text"><![CDATA[这篇主要介绍Thread API，也是并发编程中的基础 Thread一些常用API守护线程 守护线程的概念和原理可以见：守护线程和非守护线程 守护线程的一个应用： 比如在做长连接的时候，需要一个心跳检查线程，这个线程就应该设置为后台线程，这样当整个连接关闭时，也会跟随连接线程消亡。 在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑，会有可能不去执行。 这里可以在一个线程中再创建一个后台线程，来验证上述的这个应用： 12345678910111213141516171819202122232425262728293031323334353637/** * 这里对后台线程提出一个问题： * （1）当在main函数中的一个Thread里再创建一个线程，设置为后台线程，那么外边线程结束之后 里面的线程是否也会退出？ 会的 这个就长连接中的健康检查 * * * @author 夸克 * @date 2019/2/19 00:21 */public class DaemonQuestionThread &#123; public static void main(String[] args) &#123; Thread outerThread = new Thread(() -&gt; &#123; Thread innerThread = new Thread(() -&gt; &#123; try &#123; while (true) &#123; System.out.println("do Something for health check"); Thread.sleep(1_000); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); // 设置一个守护线程 设置的过程必须在start方法之前 innerThread.setDaemon(true); innerThread.start(); &#125;); try &#123; Thread.sleep(1_000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; outerThread.start(); System.out.println("程序结束"); &#125;&#125; 线程id线程id是Thread类在构造函数初始化时赋值给的Thread类中的tid字段，而赋值时其实调用的是静态的加锁方法nextThreadId()。可以看下源码： 123private static synchronized long nextThreadID() &#123; return ++threadSeqNumber;&#125; 其实就是对Thread类中的静态字段threadSeqNumber自增。 12/* For generating thread ID */ private static long threadSeqNumber; 线程优先级概念： OS是采用分时间片的形式调度运行的线程，OS会分出一个个时间片，线程会分配到若干时间片，当线程的时间片用完了就会发生线程调度，并等待着下次分配。线程分配到的时间片多少也就决定了线程使用处理器资源的多少，而线程优先级就是决定线程需要多或者少分配一些处理器资源的线程属性。 是一个和操作系统有关的线程属性，有的操作系统直接回忽略用户设置线程的优先级。 IO密集型是要设置高线程优先级的，cpu密集型是要设置低优先级的。 join方法 一个线程调用join方法其实就是让被join的线程等待该线程执行完毕之后再执行。下面是java代码示例。可以看到join其实也是通过wait()方法去实现的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 哪个线程去调用join方法 就可以在加的线程内先执行该线程完毕之后才执行外部线程 * * join方法可以加时间控制 * join方法的底层实现其实是wait * * @author 夸克 * @date 2019/2/20 00:11 */public class ThreadJoinTest &#123; public static void main(String[] args) &#123; Thread t1 = new Thread(() -&gt; &#123; // t1线程 IntStream.range(1, 1000).forEach(i -&gt; System.out.println(Thread.currentThread().getName() + "--" + i)); &#125;); t1.start(); Thread t2 = new Thread(() -&gt; &#123; // t2线程 IntStream.range(1, 1000).forEach(i -&gt; System.out.println(Thread.currentThread().getName() + "--" + i)); &#125;); t2.start(); // try &#123; // 调用了t1.join 会先输出t1 再输出t2 t1.join(); t2.join();// 这样写了之后 对于main线程来说 必须等到t1 和 t2线程执行完毕 才能执行main线程 但是对于t1 和 t2 是交替执行的 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; System.out.println("==================t1 和 t2执行完毕================"); IntStream.range(1, 1000).forEach(i -&gt; System.out.println(Thread.currentThread().getName() + "--" + i)); // 如果这里调用 Thread.currentThread.join 则会出现程序无法关闭的问题。 main线程自己join了自己的情况// try &#123;//// Thread.currentThread().join();// &#125; catch (Exception e) &#123;// e.printStackTrace();// &#125; &#125;&#125; 这里再补充一个join的小demo：比如要每个线程去采集对应服务器的数据，现在有三台服务器，每台除了要记录对应服务器采集的时间外，还要在主线程中输出一共消耗了多少时间。这个就是一个join方法的简单应用。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Thread.join方法的一个小demo ： * 假设有四台服务器，每个线程要对每台服务器采集信息，比如不同的服务器采集需要不同的时间， * 这里要求主线程去记录时间的时候，必须等待每个线程采集信息完毕 * * @author 夸克 * @date 2019/2/20 00:29 */public class ThreadJoinDemo &#123; public static void main(String[] args) &#123; // 模拟三个服务器 long begin = System.currentTimeMillis(); Thread thread1 = new Thread(new CaptureRunnable("m1", 1_000)); Thread thread2 = new Thread(new CaptureRunnable("m2", 2_000)); Thread thread3 = new Thread(new CaptureRunnable("m3", 3_000)); thread1.start(); thread2.start(); thread3.start(); // 这里必须调用join 才能保证每个线程执行完毕 try &#123; thread1.join(); thread2.join(); thread3.join(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 因为是每个服务器 并行采集 所以这里的最长应该是t3的 System.out.println("采集信息完毕,最长花费时间：" + (System.currentTimeMillis() - begin)); &#125;&#125;class CaptureRunnable implements Runnable &#123; private String name; private long expiredTime; public CaptureRunnable(String name, long expiredTime) &#123; this.name = name; this.expiredTime = expiredTime; &#125; @Override public void run() &#123; try &#123; long beginTime = System.currentTimeMillis(); Thread.sleep(expiredTime); System.out.println(Thread.currentThread().getName() + "采集信息花了" + (System.currentTimeMillis() - beginTime)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; interrupt方法 interrupt方法是Thread API中一个重要的方法。interrupt方法是Thread类提供的一个线程中断机制，这个方法的原理是给线程设置一个为true的中断标志，设置之后，会根据线程的状态有不同的结果。如果当前线程处于阻塞状态，那么将中断标志设为true后，如果是由join、wait、sleep引起的阻塞状态，那么会将线程的中断标志设置回false，并且抛出一个InterruptedException；如果打断时当前线程处于非阻塞状态，那么仅仅会将线程中的中断 标志设置为true，在之后如果线程进入了阻塞状态，也会立马抛出一个InterruptedException，且中断标志被清除，重新设置为false。 所以对于interrupt方法，可以知道调用了interrupt方法之后线程不一定会中断，它只是将线程的中断标志位设置为true，而是否抛出InterruptedEx是根据线程的阻塞状态相关。它更像是线程的一个协作机制，线程A需要中断B线程，那么调用B.interrupt()进行线程的协作。 我们可以看看其中的源码： 在Thread类中，有一个变量blocker来表示线程的中断标志位，对于这个字段我们可以知道它的默认值是null，之后我们的理解其实可以简单的认为当中断标志位设置了，就是true，而中断标志位被清除了，就是false了。 1private volatile Interruptible blocker; 而关于中断有三个方法： public void interrupt(); 这个方法就不去赘述了，作用就是设置中断标志位为true，如果当前 public static boolean interrupted(); 和 public boolean isInterrupted(); 这两个方法前者是静态方法，后者是类实例方法，都返回了当前线程是否被中断。主要有两个区别： （1）静态方法提供了一种访问的方式，比如初始化Thread中传入lambda表达式作为Runnable接口的方式，这样无法在当前内部类中调用线程实例的判断方法，有了前者方法就可以直接调用静态方法，获取当前线程是否被中断。 123456789101112Thread thread = new Thread(() -&gt; &#123; while (true) &#123; try &#123; Thread.sleep(100);// 阻塞状态 清除中断标志位 抛出异常 &#125; catch (Exception e) &#123; System.out.println(&quot;收到打断信号&quot;); // 这里就是外边的interrupt方法打断了这里的sleep e.printStackTrace(); &#125; System.out.println(&quot;&gt;&gt;&quot; + Thread.currentThread().getName() + &quot;..&quot; + Thread.interrupted()); &#125;&#125;); （2）前者静态方法，会判断当前线程是否已经中断。线程的中断状态 由该方法清除。线程中断被忽略，因为在中断时不处于活动状态的线程将由此返回 false 的方法反映出来。而后者实例方法，判断线程是否已经中断。线程的中断状态 不受该方法的影响。线程中断被忽略，因为在中断时不处于活动状态的线程将由此返回 false 的方法反映出来。这里区别是静态方法会清除线程的中断标志，这里可以分析源码得到： 123456789public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125;public boolean isInterrupted() &#123; return isInterrupted(false);&#125;private native boolean isInterrupted(boolean ClearInterrupted); 可以看到其实两个方法都是调用的native方法isInterrupted方法，但是静态方法传入的参数是true，而实例方法传入的是false，这个参数的含义很清晰就是控制是否清楚当前中断标志位。 这里关于interrupt方法不能真正中断线程的实现，也可以在源码中得到，同时也能解释为什么当线程处于阻塞状态时，调用interrupt()方法，会抛出InterruptedException，并且将标志位清空。 123456789101112131415public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; // blocker为空时不会进入到if的判断，所以只会调用synchronized代码块之后的最后的interrupt0方法，而从注释来看这个native方法仅仅是设置interrupt标志位的 if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); // 真正执行中断线程的方法 return; &#125; &#125; interrupt0();&#125; 这个时候当线程中的中断标志位为空时，很明显不会进入if的判断，这时只是会设置当前线程的中断标志位。而且这时也没有进入的if中的interrupt真正中断线程的方法。 而当线程阻塞时，以sleep方法为例，在调用sleep时，就会调用native方法interrupt0，这个方法会将线程标志位设置为true，并且现在blocker标志肯定不为null。所以会进入到if的判断代码块中，这时会再调用一次interrupt0方法（调用这个方法会清除当前线程的标志位），并且使当前线程退出阻塞状态（调用了真正的中断线程的方法）并且抛出InterruptedException异常。见下图 InterruptedException异常的处理 这里要规范的处理方法有两种： （1） 把该类异常抛给上层调用者来处理（当然，抛出去后，接收者也要考虑这个同样的问题） （2）在 catch 该异常后，通过 interrupt() 方法恢复当前线程的中断状态，示例如下： 12345678910try &#123; Thread.sleep(100);// 阻塞状态 清除中断标志位 抛出异常&#125; catch (InterruptedException e) &#123; System.out.println("收到打断信号"); // 这里就是外边的interrupt方法打断了这里的sleep e.printStackTrace(); // 正确的处理InterruptException的一种方式 Thread.currentThread().interrupt();// 通过这个回复线程的中断标志位，给下面的操作处理&#125; 这样原因已经比较清楚了：出现了 InterruptedException，说明当前线程在 wait / sleep / join 时的阻塞（等待）状态下被打断，此时 JDK 的实现默认会退出阻塞，并且清除了中断状态。也即是讲，如果此时通过 isInterrupted() 去读取中断状态时，得到的是 false。而这与 interrupt() 的调用目的是违背的，因为 interrupt 的目的是请求和标记目标线程的中断。如果我们不去主动恢复中断状态，就会导致其他需要读取中断状态的地方 判断错误，导致一些意外情况的发生。 interrupt这里的参考 一个interrupt方法的详解 interrupt的用途 关于InterruptedException的思考 交流上述代码都能在github中找到:]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>Thread API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[索引优化和设计原则]]></title>
    <url>%2Fblog%2F2019%2F02%2F02%2F%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%E5%92%8C%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[索引的优化原则 联合索引尽量所有字段全值匹配。如果使用部分字段，注意最左前缀规则，按照顺序使用联合索引。 不在索引列上做计算、函数、类型转换等操作，会导致索引失效走全表扫描。 尽量使用索引覆盖，减少select * 带来的回表的成本。 mysql使用in、not in、is null、is not null、&lt;&gt; 、 !=，mysql的优化器会根据扫描的条数进行成本分析，可能直接会全表扫描，所以注意查询条件命中记录的条数。 like注意使用前缀通配匹配。like ‘abc%’。如果没办法可要优化到索引覆盖级别。 如果不符合预期，可以强制force index使用索引。 避免文件排序（explain extra中有Using filesort）。为排序字段建立索引，或者where条件字段和排序字段能使用联合索引。排序还要避免多个排序字段的升序、降序规则不同。 如果不能避免filesort，则可以调整mysql的参数max_length_for_sort_data，调小一点来触发双路排序。filesort是在内存维护一个sort buffer，当要排序字段总大小大于这个值就会在buffer中只加载主键和排序字段，回表拿最后的结果，而不是在buffer中加载所有符合条件的数据的所有字段排序后返回。 分页优化1select * from table where name ='xxx' limit 100000,10; 此语句的过程是，扫描二级索引name，查出符合条件的主键值，回表查询聚簇索引，扫描100010行，取最后的10行。 深度分页会造成回表扫描大量聚簇索引才能取到深度的那几条数据，造成效率低下。 解决办法： 能用主键先用一个标签来过滤的场景来用主键id过滤。改写为1select * from table where id &gt; 100000 limit 10; 这样能减少扫描聚簇索引的数量。但是有局限性，因为要求能找到这样一个标签值，且结果是按照主键这样排序的。 非主键排序的深度分页可以用延迟关联的方法来连接查询。1select * from table order by name limit 100000,5; 可以改写为 1select * from table a where inner join (select id from table b order by name limit 100000, 5) b on a.id = b.id; 这样的连接查询中，子查询先是索引覆盖去排序去筛选出对应的记录且去除分页记录，然后再作为驱动表连接查询该表，此时是根据主键id做关联查询，可以大量减少回表扫描记录数。 连接查询的优化 连接查询的字段建立索引，驱动表记录去和被驱动表连接查询可以使用索引。 小表驱动大表，本身连接查询也是这样去选择驱动表的。 连接字段没有索引，mysql本身会有一个基于块的嵌套循环查询算法，在内存中有join_buffer区域去加载一批驱动表的数据，去和被驱动表做关联查询，减少磁盘IO的次数。 count查询 count(字段) 不为统计null的记录数。count(id)、count(1)、count(*)会统计null的记录。 本身这几个效率差不多，在count(字段)中的字段有索引情况下，可以直接扫描二级索引来完成统计，效率更高。 索引的设计原则 代码先行，索引后上因为代码中要开发sql查询，可以根据业务中的sql来确定具体的sql来建立合适的索引。 联合索引进来覆盖条件 索引要能覆盖大量查询场景（where、order by、group by），顺序也要按照最左前缀原则来设计。 不要在小基数的字段上建立索引 越小的基数在等值或者范围查询场景下扫描更多的记录，可能造成优化器选择全表扫描。 不能发挥索引的优势。 where 和order by 冲突 尽量先满足where 索引字段的长度尽量小一点，相同大小空间的B+树能承载更多的数据，查找效率也更高。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>索引优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式——模板方法的应用]]></title>
    <url>%2Fblog%2F2019%2F02%2F01%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E2%80%94%E2%80%94%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[模板方法模板方法很多情况下代码中的业务都可以抽象出一个模板去解决，这时候经常需要用到模板方法。大家经常接触到的是一些业务方法的抽象模板，比如在计算优惠券的流程当中总是有一定的步骤： （1）先计算该商品是否可以拥有优惠券信息 （2）再为该商品绑定优惠券信息 （3）最后回调或者通知向下的流程 今天要记录的是一个通用服务层的模板方法，包含了前置校验、后置处理（是有点像拦截器= =）、finally操作。 业务processor 可以定义一个domainProcessor去代表业务操作的processor接口，这个接口可以承接泛型。 123456789public interface DomainProcessor&lt;T&gt; &#123; /** * 处理业务的方法 返回值现在是void的 也可以自己定义一个通用的类似于上下文的返回值 * * @param context */ void process(T context);&#125; 这里承接的泛型context是一个上下文的概念，指的是在一个业务处理processor中的上下文信息，其中可以有计算的参数，计算的结果和一些中间信息。 做一个抽象类，去将其中的前置操作、biz操作、后置操作、finally操作定义出来。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public abstract class AbstractProcessor&lt;T&gt; implements DomainProcessor&lt;T&gt; &#123; /** * 前置处理 主要做一些校验逻辑 编排的时候放在前边 * * @param context * @return */ protected boolean preHandle(T context)&#123; return true; &#125; /** * 业务处理 这个是抽象方法 子类去实现具体的逻辑 * @param context */ protected abstract void bizHandle(T context); /** * 后置处理 * * @param context */ protected void postHandle(T context) &#123; &#125; /** * last 操作 * @param context */ protected void finallyHandle(T context) &#123; &#125; /** * 异常处理 * * @param context * @param e */ protected void exceptionHandle(T context, Exception e) &#123; // TODO 打印error或者warn级别日志 throw new BizException(e.getMessage(), e); &#125; @Override public void process(T context) &#123; /** * 编排流程 */ try &#123; if (!preHandle(context)) &#123;// log.warn("处理器前置处理失败，退出。context:&#123;&#125;", context); return; &#125; // 业务方法 bizHandle(context); // 后置操作 postHandle(context); &#125; catch (Exception e) &#123; exceptionHandle(context, e); &#125; finally &#123; finallyHandle(context); &#125; &#125;&#125; 可以看到，这里定义了前置操作，这里可以去对biz要用的参数进行一个校验或者一些前置操作，同时将biz定义为了抽象方法，意图在为了让子类去继承时一定要去实现bizHandle这个方法。而exceptionHandle和finallyHandle方法则定义了异常的处理和最终要做的（比如线程快照的清除）。而整个process方法其实就是整个处理器的入口，即对这整个流程的一个编排。 可以看一个这个模板processor的的具体实现和测试类： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class AbstractProcessorTest &#123; private static final ThreadLocal&lt;String&gt; contextHolder = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; // TODO test TestContext testContext = new TestContext(); contextHolder.set("aaa"); TestProcessor testProcessor = new TestProcessor(); // 调用process的编排 testProcessor.process(testContext); TestContext exContext = new TestContext(); // test bizHandle testProcessor.process(exContext); &#125; private static class TestProcessor extends AbstractProcessor&lt;TestContext&gt; &#123; @Override protected boolean preHandle(TestContext context) &#123; // 前置的一个check或者一个前置操作 String s = contextHolder.get(); if (s == null) &#123; throw new RuntimeException("holder中字符串为空"); &#125; context.setMessage(s); return true; &#125; @Override protected void postHandle(TestContext context) &#123; context.setPostStr("post"); System.out.println("processor 结果是："+ context.isResult()); &#125; @Override protected void finallyHandle(TestContext context) &#123; if (context.isResult()) &#123; contextHolder.remove(); &#125; &#125; @Override protected void bizHandle(TestContext context) &#123; context.setResult(context.getMessage().length()&gt;0); &#125; &#125;&#125; 总结这个模板可以作为之后一个处理器的抽象模板，能让代码逻辑很清晰的展现出来，也能解耦了各个处理模块，这里可以总结下。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——基础拾遗]]></title>
    <url>%2Fblog%2F2018%2F12%2F31%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E6%8B%BE%E9%81%97%2F</url>
    <content type="text"><![CDATA[并发编程基础拾遗在看并发的书或者看并发相关的博客时，会发现一些知识点会遗漏或者之前没有看到，这里去总结一下。 创建线程相关 main线程我们通常会通过写main方法去创建多个线程，main线程是非守护线程，代表方法的入口。这个时候如果用jconsole去看的话，会发现这时也会启用很多后台线程：比如GC线程、计算引用线程、JMX线程。 线程生命周期 新建状态 可运行状态 运行状态 阻塞状态 死亡状态 这些个状态之间的转换每次都记得不是很牢固。 当新建一个线程之后，这个线程处于新建状态，这时调用t.start()方法，线程就会进入可运行状态，这个状态下线程还没有真正的执行。当os分配给当前线程时间片的时候，线程会进入运行状态。当线程处于运行状态时，时间片用完或者调用Thread.yield()方法，线程则变回为可运行状态，同样若再获得os的时间片，线程将会再次进入运行状态。当正在运行状态的线程遇到Thread.sleep或者其他线程join的时候，会进入阻塞状态，直到sleep结束或者join线程结束之后线程进入可运行状态等待时间片的分配。（这里不能直接由阻塞状态直接变为运行状态）。当线程处于运行状态的时候，对象调用了o.wait方法，那么线程就会进入等待队列，等待被其他线程唤醒进入锁池，还有如果线程未持有对象的锁而被阻塞的时候也会进入到锁池，直到拿到对象锁标志之后，才能到可运行状态，再次等待时间片的分配。当线程的run方法结束，或者线程是一个守护线程是main线程结束，或者异常退出之后，线程进入到死亡状态。 start()方法和run方法的一个设计start方法和run()方法是Thread类中的两个方法，我们启动一个线程的时候要调用start方法。 123456789101112131415161718192021public synchronized void start() &#123; if (threadStatus != 0) throw new IllegalThreadStateException(); group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125;&#125; 在start方法里是调用了一个start0的native方法，这个方法其实是调用的run方法。 123456@Overridepublic void run() &#123; if (target != null) &#123; target.run(); &#125;&#125; 这样的设计是体现的设计模式中的一个简单模板方法，在调用start方法的同时其实是调用你重写的run方法，其他的start0和一些线程状态的判断都在start方法中作为不变的部分，而变的run方法则供使用者去实现run方法。 Runnable接口相关runnable接口的作用Runnable接口创建线程，这种创建线程的方式可以将run方法的逻辑从new thread的控制中抽离出来，这里如果耦合在new Thread中，没有体现业务可执行逻辑和线程控制分离开，体现了面向对象的思想。其实这种方式体现的是一种策略模式，Runnable接口就像策略模式中的策略接口，实现Runnable接口的run方法的类都是一个线程运行的策略，同时Runnable设计成了函数式接口，也更加灵活。 Thread API相关new Thread构造函数Thread构造函数有多个参数的重载，这里去记录下相关的构造函数，并且每个构造函数中参数的含义。 new Thread()创建线程对象Thread，默认有一个线程名，以Thread-开头，从0开始数。 new Thread(Runnable target)如果在构造Thread的时候没有传递Runnable或者没有复写Thread的run方法，该Thread将不会调用任何东西，如果传递了Runnable接口的实例，或者复写了Thread的run方法，则会执行该方法的逻辑单元（逻辑代码）。 new Thread(ThreadGroup g)在创建线程的时候，如果构造函数中未传入threadGroup，则thread会默认获取父线程的threadGroup作为该线程的threadGroup。此时子线程和父线程将会在同一个ThreadGropu中。这时候可以通过threadGroup提供的api去获取一些信息。 比如threadGroup.getActiveCount 获取活跃线程数。 比如threadGroup.enumerate(new Thread[]) 是将所有活跃线程枚举到一个数组中。 new Thread(long stackSize)stackSize是平常用的比较少的，这里要清楚jvm的一个结构：（这里简单用图去展示了jmm和虚拟机栈中栈帧结构）。jmm的结构： 关于变量的存储，这里可以看一下下面代码中的注释： 12345678910111213141516171819202122232425public class NewThreadWithStackSize &#123; /** * 注意这里不是局部变量，所以这里的int i是放入方法区的 */ private int i = 0; /** * bytes这个变量存放在方法区中，里面存放的是对象在堆内存中的地址 */ private byte[] bytes = new byte[1024]; private static int count = 0; public static void main(String[] args) &#123; // main函数也是一个线程 // create a thread by jvm named 'main' and 创建一个虚拟机栈 // 局部变量 放入局部变量表中 int j = 0; // arr在局部变量表中，对象还是放入堆内存中 int[] arr = new int[1024]; &#125;&#125; 栈帧的结构： 这里可以做一个栈溢出的的例子，来为一个线程分配固定栈空间的大小，看看大概操作栈的次数（这里只是一个大概的数值） 1234567891011121314151617181920212223242526272829public class NewThreadWithStackSize2 &#123; private static int count = 0; public static void main(String[] args) &#123; /** * 指定stackSize的new Thread构造函数 */ Thread thread = new Thread(null, new Runnable() &#123; @Override public void run() &#123; try &#123; add(1); &#125; catch (Error r) &#123; r.printStackTrace(); System.out.println(count); &#125; &#125; private void add(int i) &#123; count ++; add(i+1); &#125; &#125;, "test-stackSize", 1&lt;&lt;23); // 1 右移23位 大概是8mb 给当前线程thread分配了8m的stackSize 进行递归栈溢出 thread.start(); &#125;&#125; 可以看到count的数值是： 我们可以知道：构造Thread的时候传入stacksize表示该线程占用的stack大小，如果没有指定stacksize的大小，默认是0，0代表着会忽略该参数，该参数会被JNI函数去调用。需要注意：该参数有一些平台有效，有些平台则无效。可以通过jvm参数 -Xss10m 来设置stacksize的大小]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式——观察者模式]]></title>
    <url>%2Fblog%2F2018%2F12%2F24%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E2%80%94%E2%80%94%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[观察者模式观察者模式也是我们经常会用到的设计模式之一，这里用一个气象站的一些数据变化通知气象板为例去记录一下观察者设计模式，值得一提的是java中提供了观察者模式的接口和类。 demo一个气象站通知气象板的小demo，气象站提供温度、气压、湿度的数据给一些气象板提供数据，当气象站发生变化了之后，要通知订阅气象站数据的气象板数据变更。 一般方案在气温变化的气象站中加入气象板对象，在数据变化时，去调用气象板对象的update方法。 气象站WeatherData类中，定义了看板类成员变量，在dataChange方法中调用了该气象板的update方法来动态的变更气象板的数据。 123456789101112131415161718192021222324252627282930313233343536373839404142@Getter@Setterpublic class WeatherData &#123; /** * 温度 */ private float temperature; /** * 湿度 */ private float humidity; /** * 气压 */ private float pressure; // 定义进来模拟的当天的看板 在构造函数中进行初始化 private CurrentConditions currentConditions; public WeatherData(CurrentConditions currentConditions) &#123; this.currentConditions = currentConditions; &#125; private void dataChange() &#123; currentConditions.update(getTemperature(), getPressure(), getHumidity()); &#125; /** * 模拟数据变化过程 */ public void setData(float temperature, float pressure, float humidity) &#123; setTemperature(temperature); setPressure(pressure); setHumidity(humidity); dataChange(); &#125;&#125; 模拟的气象站CurrentConditions类中，提供了update方法去修改了自己类中的成员变量。 1234567891011121314151617181920212223242526272829@ToStringpublic class CurrentConditions &#123; private float currentTemperature; private float currentPressure; private float currentHumidity; /** * 进行更新看板中的数据 并且打印 * @param currentTemperature * @param currentPressure * @param currentHumidity */ public void update(float currentTemperature, float currentPressure, float currentHumidity) &#123; this.currentTemperature = currentTemperature; this.currentPressure = currentPressure; this.currentHumidity = currentHumidity; display(); &#125; public void display() &#123; System.out.println("*** Today ***" + toString()); &#125;&#125; 运行主方法： 1234567891011121314151617181920212223public static void main(String[] args) &#123; // 新建一个 当天天气的公告板 CurrentConditions currentConditions = new CurrentConditions(); WeatherData weatherData = new WeatherData(currentConditions); // 模拟气象变化 weatherData.setData(300f, 100f, 22f); // 再次变化 weatherData.setData(200f, 111f, 222f); /** * 但是可以想到这种方式是只有一种当天天气的公告板 * 但是如果有很多公告板 再接入的时候就要在WeatherData中定义这个公告板，在changeData中调用update * 扩展性很差 * * 要用观察值模式去增强这个demo的观察者，把变得部分做抽象和接口设计 * 看use_observer package */&#125; 可以看到运行结果： 12*** Today ***CurrentConditions(currentTemperature=300.0, currentPressure=100.0, currentHumidity=22.0)*** Today ***CurrentConditions(currentTemperature=200.0, currentPressure=111.0, currentHumidity=222.0) 这种方案的弊端： 拓展性比较差，这只是一个气象看板的情况，如果是多个气象看板，那么需要耦合在WeatherData类中。 必须要知道每个订阅气象站的看板对象的update方法参数，这个也要去耦合多个逻辑。 观察者模式改进这里简单描述下观察者模式的定义： 当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。观察者模式属于行为型模式。被依赖的对象为Subject，依赖的对象为Observer，Subject通知Observer变化 。 那么在我们的demo场景中，我们要去分析不变的部分和变的部分。 变的部分：是一对多关系的维护，气象板可能增加并且减少，并且每个订阅气象站数据的气象板对象去更新的参数和方式也不一样。 不变的部分：是气象站中对观察者们的注册和移除方法，并且要对每个观察者去通知更新其数据的接口；观察者们要去提供update接口。这些不变的部分是可以抽象成接口和其中的成员方法的。 方案定义主题接口和观察者接口： 订阅的主题Subject接口，这个接口中有观察者的订阅和观察者的移除方法，同时也有通知所有的观察者的方法。 12345678910111213141516171819public interface Subject &#123; /** * 观察者订阅接口 * @param observer */ void registerObserver(Observer observer); /** * 移除观察者方法 * @param observer */ void removerObserver(Observer observer); /** * 通知所有的观察者方法 */ void notifyObservers();&#125; 观察者要实现的接口Observer： 这个接口中定义了update的行为方法，这里是简单写了去主动传入关心的三个数据 1234public interface Observer &#123; void update(Float currentTemperature, Float humidity, Float pressure);&#125; 再去看相应的气象站和气象看板的实现，这里气象站应该去实现主题Subject接口，而气象看板应该实现Observer接口提供自己的更新逻辑。 气象站WeatherData类，里面使用ArrayList&lt;Observer&gt;去定义了观察者的集合，此外也有实现了循环去通知观察者和删除观察者的接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Datapublic class WeatherData implements Subject &#123; /** * 温度 */ private float temperature; /** * 湿度 */ private float humidity; /** * 气压 */ private float pressure; /** * 观察者集合 */ private ArrayList&lt;Observer&gt; observerArrayList; @Override public void registerObserver(Observer observer) &#123; observerArrayList.add(observer); &#125; @Override public void removerObserver(Observer observer) &#123; if (observerArrayList.contains(observer)) &#123; observerArrayList.remove(observer); &#125; &#125; @Override public void notifyObservers() &#123; // 这里的通知 参数是写死的三个 拓展性较差 // java内置的观察者中有拓展参数 动态去取的 // 循环观察者去更新 observerArrayList.forEach(e -&gt; e.update(getTemperature(), getHumidity(), getPressure())); &#125; /** * 模拟参数的变化 */ public void dataChange(float temperature, float humidity, float pressure) &#123; setTemperature(temperature); setHumidity(humidity); setPressure(pressure); // 这里去通知所有的观察者即可 notifyObservers(); &#125;&#125; 这里去定义两个气象看板实现Observer接口，一个是CurrentConditions，一个是ForcastConditions，这里简单模拟就是在update中输出的字符串不相同来模拟不同的任务。 CurrentConditions： 1234567891011121314151617181920212223@Data@ToStringpublic class CurrentConditions implements Observer&#123; private float currentTemperature; private float currentPressure; private float currentHumidity; @Override public void update(Float currentTemperature, Float humidity, Float pressure) &#123; this.currentHumidity = humidity; this.currentPressure = pressure; this.currentTemperature = currentTemperature; display(); &#125; public void display() &#123; System.out.println("*** Today ***" + toString()); &#125;&#125; ForcastConditions: 1234567891011121314151617181920212223@Data@ToStringpublic class ForcastConditions implements Observer &#123; private float currentTemperature; private float currentPressure; private float currentHumidity; @Override public void update(Float temperature, Float humidity, Float pressure) &#123; this.currentHumidity = humidity; this.currentPressure = pressure; this.currentTemperature = temperature; display(); &#125; public void display() &#123; System.out.println("*** Forcast ***" + toString()); &#125;&#125; 在main中运行一下看一下效果： 1234567891011121314151617181920212223242526272829303132333435363738/** * 使用观察者模式增强扩展性的demo * 观察者模式：对象之间多对一依赖的一种设计方案，被依赖的对象为subject，依赖的对象为Observer，Subject通知Observer变化。拥有比较强的拓展性 * * * @author 夸克 * @date 2018/12/11 01:02 */public class Main &#123; public static void main(String[] args) &#123; // 两个气象看板 去 订阅天气的变化 Observer cuerrent = new CurrentConditions(); Observer forcast = new ForcastConditions(); // 两个观察者订阅主题 WeatherData weatherData = new WeatherData(); ArrayList&lt;Observer&gt; list = new ArrayList&lt;Observer&gt;()&#123;&#123; add(cuerrent); add(forcast); &#125;&#125;; weatherData.setObserverArrayList(list); // 模拟天气变化 System.out.println("=============天气变化1============="); weatherData.dataChange(111f, 222f, 333f); System.out.println("=============天气变化2============="); weatherData.dataChange(333f, 444f, 555f); System.out.println("=============添加一个观察者============="); weatherData.registerObserver(new ForcastConditions()); // 再次模拟天气变化 weatherData.dataChange(555f, 666f, 777f); &#125;&#125; 结果如下 12345678910=============天气变化1=============*** Today ***CurrentConditions(currentTemperature=111.0, currentPressure=333.0, currentHumidity=222.0)*** Forcast ***ForcastConditions(currentTemperature=111.0, currentPressure=333.0, currentHumidity=222.0)=============天气变化2=============*** Today ***CurrentConditions(currentTemperature=333.0, currentPressure=555.0, currentHumidity=444.0)*** Forcast ***ForcastConditions(currentTemperature=333.0, currentPressure=555.0, currentHumidity=444.0)=============添加一个观察者=============*** Today ***CurrentConditions(currentTemperature=555.0, currentPressure=777.0, currentHumidity=666.0)*** Forcast ***ForcastConditions(currentTemperature=555.0, currentPressure=777.0, currentHumidity=666.0)*** Forcast ***ForcastConditions(currentTemperature=555.0, currentPressure=777.0, currentHumidity=666.0) java内置的观察者模式jdk中有内置的观察者模式，是java.util.Observable类，这个要注意并不是对应着上边自定义观察者模式中的Observer接口，而是对应着我们自定义的主题Subject接口，并且这里去设计成了类。而java中也是用Observer接口去对应观察者模式的Observer接口。 可以先看下java内置的Observer接口：可以看到也是去定义了一个update方法，然后传入了Observable类，也就是订阅的主题，还有就是一个Object的参数，其实就是update的更新数据，这里要求封装成一个Object。 123456789101112131415161718192021/** * A class can implement the &lt;code&gt;Observer&lt;/code&gt; interface when it * wants to be informed of changes in observable objects. * * @author Chris Warth * @see java.util.Observable * @since JDK1.0 */public interface Observer &#123; /** * This method is called whenever the observed object is changed. An * application calls an &lt;tt&gt;Observable&lt;/tt&gt; object's * &lt;code&gt;notifyObservers&lt;/code&gt; method to have all the object's * observers notified of the change. * * @param o the observable object. * @param arg an argument passed to the &lt;code&gt;notifyObservers&lt;/code&gt; * method. */ void update(Observable o, Object arg);&#125; 再来看下Subject也就有Observable类： 定义了两个成员变量，changed是标识一个观察者们是否变化的状态，而obs指的就是观察者的集合，这里是用Vector存储的。 12private boolean changed = false;private Vector&lt;Observer&gt; obs; 定义的添加和删除观察者都是同步的：（addElement和removeElement就是对Vector的操作添加元素和删除元素的操作，这里不去赘述） 1234567891011public synchronized void addObserver(Observer o) &#123; if (o == null) throw new NullPointerException(); if (!obs.contains(o)) &#123; obs.addElement(o); &#125;&#125;public synchronized void deleteObserver(Observer o) &#123; obs.removeElement(o); &#125; 通知观察者的方法也可以看到分析了存在竞争条件的情况： 一种是新注册的观察者会错过主题进行的通知，另一种是未注册的观察者会被误通知，所以这里同步代码块中，判断了changed这个状态，只有当changed为true才将vector中存储的观察者赋给arrLocal变量；并且在clearChanged方法中也同步清除了changed为true这个状态，这个是为了表示该对象已不可改。这样去广播通知的时候就不用去同步代码块中执行。 1234567891011121314151617181920212223242526272829303132333435public void notifyObservers(Object arg) &#123; /* * a temporary array buffer, used as a snapshot of the state of * current Observers. */ Object[] arrLocal; synchronized (this) &#123; /* We don't want the Observer doing callbacks into * arbitrary code while holding its own Monitor. * The code where we extract each Observable from * the Vector and store the state of the Observer * needs synchronization, but notifying observers * does not (should not). The worst result of any * potential race-condition here is that: * 1) a newly-added Observer will miss a * notification in progress * 2) a recently unregistered Observer will be * wrongly notified when it doesn't care */ if (!changed) return; arrLocal = obs.toArray(); clearChanged(); &#125; for (int i = arrLocal.length-1; i&gt;=0; i--) ((Observer)arrLocal[i]).update(this, arg);&#125; protected synchronized void clearChanged() &#123; changed = false; &#125; 在这个类中提供了一些同步方法，比如判断changed是否发生变化、比如返回当前观察者的数量。这里不去赘述。 jdk内置的Observable类中还提供了没有参数的notifyObservers方法，这里相当于调用了notifyObservers(null)，这里就可以理解为有些场景是关心Subject变化但是不需要Subject传入参数，所有提供了这个方法。 使用java内置观察者模式改造气象站demo这里就可以让气象站去继承Observable类，即观察者模式中的主题，去重写Observable类中的一些方法。 同时各个气象看板要去实现jdk内置的Observer接口和实现其中的update方法。这里要把要更新的数据做成一个类，来适配update接口。 气象站WeatherData类，可以看到这里调用了父类的setChanged方法和notifyObservers方法，这里由刚才看到notify通知的源码知道要注意设置changed参数为true，这样才会去广播数据变更给观察者们。所以这里在数据变更的方法之前调用了设置changed参数为true的方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Datapublic class WeatherData extends Observable &#123; /** * 温度 */ private float temperature; /** * 湿度 */ private float humidity; /** * 气压 */ private float pressure; private void dataChange() &#123; // 使用jdk内置Subject 即Observable类，通知观察者们的时候要设置changed为true super.setChanged(); super.notifyObservers(new Data(getTemperature(), getHumidity(), getPressure())); &#125; /** * 模拟数据变化过程 */ public void setData(float temperature, float pressure, float humidity) &#123; setTemperature(temperature); setPressure(pressure); setHumidity(humidity); dataChange(); &#125; /** * 通知观察者的数据类 这里要是适配JDK内置的Observer接口中的update方法 */ @AllArgsConstructor public class Data&#123; /** * 温度 */ private float temperature; /** * 湿度 */ private float humidity; /** * 气压 */ private float pressure; &#125;&#125; 再来看两个气象看板的类，这里是实现了update方法，转为了数据类去更新数据。 12345678910111213141516171819202122232425262728293031323334@Data@ToStringpublic class CurrentConditions implements Observer &#123; /** * 温度 */ private float temperature; /** * 湿度 */ private float humidity; /** * 气压 */ private float pressure; @Override public void update(Observable o, Object arg) &#123; WeatherData.Data data = (WeatherData.Data) arg; setTemperature(data.getTemperature()); setHumidity(data.getHumidity()); setPressure(data.getPressure()); display(); &#125; public void display() &#123; System.out.println("*** Today ***" + toString()); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536@Data@ToStringpublic class ForcastConditions implements Observer &#123; /** * 温度 */ private float temperature; /** * 湿度 */ private float humidity; /** * 气压 */ private float pressure; @Override public void update(Observable o, Object arg) &#123; // TODO 这里还可以对Observable 进行一个筛选和过滤的逻辑 WeatherData.Data data = (WeatherData.Data) arg; setTemperature(data.getTemperature()); setHumidity(data.getHumidity()); setPressure(data.getPressure()); display(); &#125; public void display() &#123; System.out.println("*** forcast ***" + toString()); &#125;&#125; 这里去测试下： 123456789101112131415161718public static void main(String[] args) &#123; WeatherData weatherData = new WeatherData(); // 两个观察者，使用java内置观察者模式 通知顺序和注册顺序是相反的 CurrentConditions currentConditions = new CurrentConditions(); weatherData.addObserver(currentConditions); weatherData.addObserver(new ForcastConditions()); // 这时模拟气候变动 weatherData.setData(111f, 222f, 333f); System.out.println("移除一个观察者"); // 移除一个观察者 注意这里要将上面同一个观察者对象 作为移除参数 weatherData.deleteObserver(currentConditions); weatherData.setData(444f, 555f, 666f);&#125; 可以看到输出结果： 123456*** forcast ***ForcastConditions(temperature=111.0, humidity=333.0, pressure=222.0)*** Today ***CurrentConditions(temperature=111.0, humidity=333.0, pressure=222.0)移除一个观察者*** forcast ***ForcastConditions(temperature=444.0, humidity=666.0, pressure=555.0)Process finished with exit code 0]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>观察者模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test多个设备使用hexo]]></title>
    <url>%2Fblog%2F2018%2F12%2F24%2Ftest%E5%A4%9A%E4%B8%AA%E8%AE%BE%E5%A4%87%E4%BD%BF%E7%94%A8hexo%2F</url>
    <content type="text"><![CDATA[背景hexo是支持多设备去写你的博客的，这里参考的是这个资料： hexo多设备]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集合中的深拷贝]]></title>
    <url>%2Fblog%2F2018%2F12%2F19%2F%E9%9B%86%E5%90%88%E4%B8%AD%E7%9A%84%E6%B7%B1%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[深拷贝与浅拷贝对对象的深拷贝和浅拷贝我们并不陌生。 （1）浅拷贝： 在浅拷贝中，如果原型对象的成员变量是值类型，将复制一份给克隆对象；如果原型对象的成员变量是引用类型，则将引用对象的地址复制一份给克隆对象，也就是说原型对象和克隆对象的成员变量指向相同的内存地址。简单来说，在浅拷贝中，当对象被复制时只复制它本身和其他包含的值类型的成员变量，而引用类型的成员对象并没有复制。在Java中，通过覆盖Object类的clone()方法可以实现浅拷贝。 （2）深拷贝: 在深拷贝中，无论原型对象的成员变量是值对象还是引用类型，都将复制一份给克隆对象，深拷贝将原型对象的所有引用对象也复制一份给克隆对象。简单来说，在深克隆中，除了对象本身被复制外，对象所包含的所有成员变量也将复制。在Java中，如果要实现深拷贝，可以通过覆盖Object类的clone()方法实现，也可以通过序列化(Serialization)等方式来实现。（当然如果引用类型里面还包含很多引用类型，或者内层引用类型的类里面又包含引用类型，使用clone方法就会很麻烦。这时我们可以用序列化的方式实现对象的深克隆。） 这里肯定有些朋友注意到Cloneable接口和Serializable接口，这两个接口其实都是java中的标记型接口，其中都没有任何方法的定义，其实就是告诉jre这些接口的实现类是否有某个功能，是否支持克隆，是否支持序列化。 Object类中的clone()方法可以看到这个方法在Object类中是protected的。可与看看它的方法的源码注释： 12345678910111213/** * Creates and returns a copy of this object. The precise meaning * of "copy" may depend on the class of the object. The general * intent is that, for any object &#123;@code x&#125;, the expression: * x.clone() != x * will be true, and that the expression: * x.clone().getClass() == x.getClass() * will be &#123;@code true&#125;, but these are not absolute requirements. * While it is typically the case that: * x.clone().equals(x) * will be &#123;@code true&#125;, this is not an absolute requirement. * &lt;p&gt; */ 从注释中我们可以看到： 因为x.clone != x，所以我们可以知道克隆对象将有单独的内存地址分配。 原始对象和克隆对象有相同的类型，但它不是强制的。 原始和克隆对象应该是平等的，equal()方法返回true，但它不是强制的。 同时，可以猜想到clone()方法设置为protected的原因：每个类都去继承了Object方法，但是clone提供的是浅拷贝的功能，我们并不知道要拷贝的原始类中成员都是值对象还是引用对象，如果要在一个类B中使用一个非同包类A的clone对象，并且B没有继承A，那么这时是不能调用A.clone()，因为Object的clone方法是protected的，如果要调用则需要A去重写clone方法，并且把clone方法的可见范围变为B类能访问的可见范围。这样A就可以对自身的成员变量是否存在引用对象去判断到底是提供深拷贝还是浅拷贝克隆方法，从而不会把自己的引用成员变量暴露出去被修改。关于这个详细的解释有篇博客：clone方法为什么是protected的？ 浅拷贝和深拷贝的实现浅拷贝浅拷贝的实现： 实现Cloneable接口（不实现的话在clone方法会抛出ClassNotSupportedException异常）,虽然该接口只是一个标记型接口。 覆盖clone方法，访问修饰符是public。方法中调用supers.clone()方法得到需要的复制对象。(native本地方法，效率还是很高的)。 demo来一个无论学习什么知识都会有的User类，让其实现Cloneable接口且覆盖父类的clone方法 12345678910111213141516171819202122232425262728293031323334public class User implements Cloneable&#123; // Cloneable是一个标记型接口，不实现在进行拷贝的时候会报错 private int number; public int getNumber() &#123; return number; &#125; public void setNumber(int number) &#123; this.number = number; &#125; @Override public String toString() &#123; return "User&#123;" + "number=" + number + '&#125;'; &#125; /** * 重写Object的clone方法 * * 改修饰权限符为public * * 直接调用super.clone() 这个native方法进行拷贝对象 */ @Override public User clone() throws CloneNotSupportedException &#123; return (User) super.clone(); &#125;&#125; 测试一下浅拷贝 12345678910111213141516171819202122232425/** * test方法 */public static void main(String[] args) &#123; User user1 = new User(); user1.setNumber(1111); User user2 = null; // 实现浅拷贝 try &#123; user2 = user1.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; if (null != user2) &#123; // 变更浅拷贝出的对象中的成员变量 user2.setNumber(2222);- &#125; System.out.println(user1);// 输出1111，浅拷贝 System.out.println(user2);&#125; 深拷贝在User类中加入一个引用对象Address类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class User implements Cloneable&#123; // Cloneable是一个标记型接口，不实现在进行拷贝的时候会报错 private int number; /** * 引用对象 */ private Address address; public int getNumber() &#123; return number; &#125; public void setNumber(int number) &#123; this.number = number; &#125; public Address getAddress() &#123; return address; &#125; public void setAddress(Address address) &#123; this.address = address; &#125; public User(int number, Address address) &#123; this.number = number; this.address = address; &#125; @Override public String toString() &#123; return "User&#123;" + "number=" + number + ", address=" + address + '&#125;'; &#125; /** * 覆盖clone方法 * 改为public * * 返回值为user * * @return */ @Override public User clone() &#123; User user = null; try &#123; user = (User) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return user; &#125;&#125; Address类中并没有实现Cloneable接口和覆盖clone方法 123456789101112131415161718192021222324public class Address &#123; private String add; public String getAdd() &#123; return add; &#125; public void setAdd(String add) &#123; this.add = add; &#125; public Address(String add) &#123; this.add = add; &#125; @Override public String toString() &#123; return "Address&#123;" + "add='" + add + '\'' + '&#125;'; &#125;&#125; 这时候测试 12345678910111213141516171819202122public static void main(String[] args) &#123; Address address = new Address("1111"); User user1 = new User(1111, address); User user2 = user1.clone(); System.out.println(user1); // 都输出1111 System.out.println(user2); // 修改user2的成员变量 address.setAdd("2222"); // 因为现在还是浅拷贝 user1 和 user2中的值还指向堆内存中同一个address对象 System.out.println(user1); System.out.println(user2);&#125;// 返回结果User&#123;number=1111, address=Address&#123;add='2222'&#125;&#125;User&#123;number=1111, address=Address&#123;add='2222'&#125;&#125; 可以看到当改变add的值的时候，user1和user2中的address对象都发生了改变，这里是因为user2只是user1的浅拷贝生成的对象，user2对象中的address成员变量只是复制了user1中的address变量的引用， 并没有将user2中的address对象指向另一块Address对象。 这里可以去改造下，Address对象要实现Cloneable接口并且覆盖clone方法，并且User对象中的clone方法也要定义user对象中的address成员变量的拷贝 Address中的clone 123456789101112@Overridepublic Address clone() &#123; // 实现clone方法 Address address = null; try &#123; address = (Address) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return address;&#125; User中的clone： 123456789101112131415 */@Overridepublic User clone() &#123; User user = null; try &#123; user = (User) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; // 对引用类型的成员变量进行clone user.address = address.clone(); return user;&#125; 这样发现运行的结果： 12345User&#123;number=1111, address=Address&#123;add=&apos;1111&apos;&#125;&#125;User&#123;number=1111, address=Address&#123;add=&apos;1111&apos;&#125;&#125;====================变更了address====================User&#123;number=1111, address=Address&#123;add=&apos;2222&apos;&#125;&#125;User&#123;number=1111, address=Address&#123;add=&apos;1111&apos;&#125;&#125; 实现深拷贝的方法重写clone方法在上面的测试中，我们可以知道重写clone可以实现深拷贝，但是当我们遇到成员变量是一个嵌套对象的时候也要求内层嵌套对象也要重写clone方法，这样比较麻烦，我们可以采用序列化的方式去做深拷贝 序列化方式序列化方式下，我们需要让需要拷贝的对象类实现序列化接口，然后通过流或者Json等序列化工具去深拷贝一个对象。 一个基于流的深拷贝工具类这里提供一个基于流的深拷贝工具类，这里场景是根据 1234567891011121314151617181920212223242526272829303132333435363738@SuppressWarnings("unchecked")@Slf4jpublic class DeepCopyUtil &#123; /** * 对集合进行深拷贝 * * @param src * @param &lt;T&gt; * @return */ public @Nullable static &lt;T&gt; Collection&lt;T&gt; deepCopy(Collection&lt;T&gt; src) &#123; if (CollectionUtils.isEmpty(src)) &#123; return null; &#125; try &#123; ByteArrayOutputStream byteOut = new ByteArrayOutputStream(); ObjectOutputStream out = new ObjectOutputStream(byteOut); out.writeObject(src); ByteArrayInputStream byteIn = new ByteArrayInputStream(byteOut.toByteArray()); ObjectInputStream in = new ObjectInputStream(byteIn); return (Collection&lt;T&gt;) in.readObject(); &#125; catch (ClassNotFoundException | IOException e) &#123; log.warn("deep copy fail, src=&#123;&#125; ", JSON.toJSONString(src), e); return null; &#125; &#125;&#125; 其实还是比较建议直接用FastJson或者其他序列化工具去序列化来实现深拷贝。 github该部分代码：https://github.com/zhanglijun1217/java8/tree/master/src/deep_and_shallow_copy]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>深拷贝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java集合类的一些总结——Arrays.asList和Guava操作集合]]></title>
    <url>%2Fblog%2F2018%2F12%2F15%2Fjava%E9%9B%86%E5%90%88%E7%B1%BB%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93%E2%80%94%E2%80%94Arrays-asList%E5%92%8C%E5%B9%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[使用背景总结一下最近项目中使用到集合的两个点，一个是Arrays.asList这个方法使用的坑，另一个是利用Guava的Sets工具类去求并交集。 使用总结Arrays.asList的坑先上结论： Arrays.asList这个方法不适用于基本类型：byte,short,int,long,float,double,boolean 该方法将数组和列表动态链接起来，当其中一个更新后，另一个也会更新 不支持add和remove方法（1）首先对于第一点，这个方法不适用于基本类型，并不是不能用于基本类型，看一下下面的代码：1234567public static void main(String[] args) &#123; int[] int_array = &#123;1, 2, 3&#125;; List&lt;int[]&gt; ints = Arrays.asList(int_array); // 对于基本类型，会打印一个元素结果 System.out.println(ints);&#125; 输出：1[[I@610455d6] 从Arrays.asList返回List的泛型可知，这个工具方法将基本类型的数组只转成了一个元素的列表。因为在Arrays.asList中，该方法接受一个变长参数，一般可以看做数组参数，但是因为int[]本身就是一个类型，所以data变量作为参数传递时，编译器认为只传了一个变量，这个变量的类型是int数组，所以会这样。基本类型是不能作为泛型的参数，按道理应该使用包装类型，这里没有报错是因为数组是可以泛型化的，所以在转换后list中有一个类型为int[]的数组。值得注意的是，如果这时打印出其中的class，会返回class[I。因为jvm不可能输出int[]对应的array类型，array属于java.lang.reflect包，通过反射访问数组这个类，编译时生成的这个class值。（2）对于第二点，我们可以尝试改变数组中的值时，可以看看对应list中的值：1234567891011121314String[] string_array = &#123;"aa", "bb", "cc"&#125;; List&lt;String&gt; stringList = Arrays.asList(string_array); System.out.println("============改变数组前============"); System.out.println(stringList); // 改变数组 string_array[1] = "change"; System.out.println("============改变数组后============"); System.out.println(stringList); // 改变list stringList.set(1, "change2"); System.out.println("============改变list后============"); System.out.println(string_array[1]); 输出结果：123456============改变数组前============[aa, bb, cc]============改变数组后============[aa, change, cc]============改变list后============change2 这个通过源码很好理解，可以看一下asList的源码：12345@SafeVarargs @SuppressWarnings("varargs") public static &lt;T&gt; List&lt;T&gt; asList(T... a) &#123; return new ArrayList&lt;&gt;(a); &#125; new ArrayList构造函数代码：1234private final E[] a; ArrayList(E[] array) &#123; a = Objects.requireNonNull(array); &#125; 从源码中得到asList就是将数组作为返回的List的实际引用，即List中的数组就是要转成list的数组，所以当更改数组中的值时，list中的值也会变化，list列表中的值变更也相当于变更了其中的数组，也就是源数据数组。这就是所谓的链接在一起。Arrays.asList体现的是适配器模式，只是转换接口（array到list），后台的数据仍是数组。（3）第三点是数组转换之后的List是不支持remove和add方法的。这里说的不支持，是调用add或者remove方法之后会抛出一个异常 java.lang.UnsupportedOperationException。一开始感到奇怪，ArrayList为什么不支持add和remove方法？这里去看了返回的ArrayList的源码，这个ArrayList并不是常用的ArrayList，而是在Arrays中定义的一个内部类：1234567891011private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, java.io.Serializable &#123; private static final long serialVersionUID = -2764017481108945198L; private final E[] a; ArrayList(E[] array) &#123; a = Objects.requireNonNull(array); &#125; ... // 省略一些代码 &#125; 这个内部类虽然继承了AbstractList，但是没有去实现其中的add和remove方法，可以看下AbstractList中定义的默认add方法的实现：123456789101112131415/** * &#123;@inheritDoc&#125; * * &lt;p&gt;This implementation always throws an * &#123;@code UnsupportedOperationException&#125;. * * @throws UnsupportedOperationException &#123;@inheritDoc&#125; * @throws ClassCastException &#123;@inheritDoc&#125; * @throws NullPointerException &#123;@inheritDoc&#125; * @throws IllegalArgumentException &#123;@inheritDoc&#125; * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */ public void add(int index, E element) &#123; throw new UnsupportedOperationException(); &#125; 可以看到如果没有去实现add方法，那么会抛出这个异常，这里因为也叫做ArrayList，所以还是比较容易踩坑。clear、remove方法也是一样的原因。关于Arrays.asList的代码github地址：https://github.com/zhanglijun1217/java8/tree/master/src/Arrays_asList Guava中的Sets工具类求交集近期也去使用了一个求集合交集的场景。这里去简单记录下Guava中的这个用法。1Sets.intersection(set1, set2); 这个方法返回的是一个新的集合去承接传入两个集合的交集的。Guava也提供了对集合操作很多其他方法，比如并集、差集、全集等，这个后面会去总结。]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Arrays.asList</tag>
        <tag>集合操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ClassNotFoundException和NoClassDefFoundError]]></title>
    <url>%2Fblog%2F2018%2F12%2F06%2FClassNotFoundException%E5%92%8CNoClassDefFoundError%2F</url>
    <content type="text"><![CDATA[背景极客时间上《Java核心技术36讲》第二讲中提到了一个问题：ClassNotFoundException和NoClassDefFoundError有什么区别？看到这个问题的时候，第一时间想到的就是一个是受检的异常，而另一个是一个Error，但是其实在真正的项目开发中这两个错误都遇到过，都是关于类或者文件jar包找不到的错误，这里去总结下其中的不同。 两者的区别ClassNotFoundException可以看到，ClassNotFound是一个继承自ReflectiveOperationException的受检异常。JDK中的解释是：当应用程序视图使用以下方法通过字符串加载类时，（加载的方法）（1）Class类中的forName()方法（2）ClassLoader类中的findSystemClass方法（3）ClassLoader类中的loadClass方法。但是没有找到具体指定名称的类的定义。这里可以知道当应用程序运行的过程中尝试使用类加载器去加载Class文件的时候，如果没有在classpath中查找到执行的类，那么就会抛出ClassNotFoundException。一般情况下，当我们使用Class.forName()或者ClassLoader.loadClass()以及使用ClassLoader.findSystemClass()在运行时加载类的时候，如果类没有找到，那么就会导致JVM抛出ClassNotFoundException。最常见的，我们都写过jdbc连接数据库的代码，我们都会用到Class.forName()去加载JDBC的驱动，如果我们没有将驱动放到应用下的classPath，那么会导致抛出异常ClassNotFoundException。 12345678910public static void main(String[] args) &#123; try &#123; Class.forName("aaaaaa"); &#125; catch (ClassNotFoundException e) &#123; System.out.println("发生了ClassNotFoundException"); &#125; &#125; 还有根据类加载器的可见性机制，子类加载器可以看到父类加载器加载的类，而反之则不行。所以当一个类已经被Application类加载器加载过了，然后如果想要使用Extension类加载器加载这个类，将会抛出java.lang.ClassNotFoundException异常。 NoClassDefFoundError看名字这个是一个Error异常，看源码可以看到它是一个继承自LinkageError的Error异常，看官网中的解释是要找的类在编译时期还可以找到，但是在运行java应用的时候找不到了，这比较经常出现在静态块的初始化过程中。JDK官方的解释：当虚拟机或ClassLoader实例在类的定义中加载（作为通用方法调用的一部分或者作为new 表达式创建的新实例的一部分），但是无法找到该类的定义时，抛出此异常。当前执行的类被编译时，所搜索的类定义存在，但无法再找到该定义。从官网的解释中我们可以看到如果编译了一个类B，在类A中调用，编译完成之后，你又删除B的class文件，运行A的时候那么就会出现这个错误。 总结这里总结一下ClassNotFoundException和NoClassDefFoundError的区别：（1）一个是Exception，受检的异常；而一个是Error。（2）ClassNotFoundException是在动态加载Class的时候调用Class.forName等方法抛出的异常；而NoClassDefFoundError是在编译通过后执行过程中Class找不到导出的错误。（3）ClassNotFoundException是发生在加载内存阶段，加载时从classpath中找不到需要的class就会出现ClassNotFoundException，出现这种错误可能是调用上述的三个方法加载类，也有可能是已经被一个类加载器加载过的类又被另一个类加载器加载；而NoClassDefFoundError是链接阶段从内存找不到需要的class才会出现，比如maven项目有的时候打包问题会引起这个error报错，这个时候要把仓库中的包删掉重新去拉一下包。]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java平台</tag>
        <tag>解释执行、编译执行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java平台与java是解释执行的正确性]]></title>
    <url>%2Fblog%2F2018%2F11%2F30%2Fjava%E5%B9%B3%E5%8F%B0%E4%B8%8Ejava%E6%98%AF%E8%A7%A3%E9%87%8A%E6%89%A7%E8%A1%8C%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%80%A7%2F</url>
    <content type="text"><![CDATA[背景最近看了一点点极客时间上的《Java核心技术36讲》，打算把一些自己感兴趣或者不知道的点总结到博客中，方便对一些知识有一些整理和拾遗。 Java平台性的理解java本身是一种面向对象的语言，有两个特征，一是“write once, run anywhere”，能够非常容易的获取跨平台的能力；另外就是垃圾手机机制，Java通过垃圾收集器(Garbage Collector)回收分配内存。 我们会接触到JRE和JDK。其中JRE是Java Runtime Environment，JDK是Java Development Kit。JRE是java运行环境，包含了JVM和java类库，以及一些模块等。而JDK可以看做是JRE一个超集，提供了更多工具，比如编译器、各种诊断工具、安全工具等。 对于Java平台的理解，可以从两个方面去谈一下。第一个方面是Java语言的特性，包括泛型、集合类、java8中的lambda特性、基础类库、IO/NIO、网络、并发、安全等，这个系统化的去总结一下，肯定能对java平台有更深的理解。另一方面可以去理解JVM中的一些概念和机制，比如Java的类加载机制，JDK中的Class-Loader，例如Bootstrap、Application和Extension Class-loader；类加载的过程：加载、验证、链接和初始化；自定义Class-Loader等；还有java的内存模型，堆、栈、方法区等内存在程序运行时的分配；垃圾回收的基本原理，常见的垃圾回收器，如SerialGC、Parallel GC、CMS、G1等，对于适用于什么样的工作负载也要去有深入的理解。 对于java是解释执行的理解，这个说法是不太准确的。我们写的java代码，首先是通过javac编译成为字节码（byteCode），然后在运行时，通过JVM内嵌的解释器将字节码转换成为最终的机器码。但是存在常见的JIT(Just-In-Time)编译器，也就是说的动态（及时）编译器，JIT能够在运行时将热点代码编译成机器码，这种情况部分热点代码就是属于编译执行，而不是解释执行了。这里说的Java中的编译不同于C中的编译，javac的编译，是把Java代码生成.class字节码，而不是机器直接运行的机器码，Java通过字节码和内嵌虚拟机的这种机制，屏蔽了操作系统和硬件的细节，促使了跨平台性。 在运行时，JVM会通过类加载器（Class-loader）加载字节码，解释或者编译运行。比如JDK8版本中，实际上是采用的解释和编译混合的模式，即混合模型（-Xmixed）。在运行在server模式的JVM，会进行上万次调用以收集足够的信息进行高效的编译，而client模式这个门限是1500次。Oracle HotSpot JVM内置了两个不同的JIT compliers，C1对应之前说的Client模式，使用于对于启动速度敏感的应用，比如普通Java应用；C2对应server模式，这种JIT编译优化适用于长时间运行的服务器端应用设计的。默认是采用所谓的分层编译。另外，在JDK9中引入了AOT特性，所谓AOT（Ahead-of-Time Compilation）就是直接将字节码编译成机器码，避免了JIT预热的一些开销，jdk9中也加入了新的jaotc工具，可以运用jaotc命令使某个类或者模块编译成为AOT库。 JVM启动时，可以指定不同的参数对运行模式进行选择。比如指定”-Xint”，就是告诉JVM只进行解释执行，不对代码进行编译，这种模式相当于直接抛弃了JIT带来的性能优化。与其相对应的，还有一个“-Xcomp”参数，这是告诉JVM关闭解释器，不再进行解释执行。那么这种是不是就会很高效呢？其实不然，这种模式会导致JVM的启动变得很慢。]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java平台</tag>
        <tag>解释执行、编译执行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——并发基础：守护线程和非守护线程]]></title>
    <url>%2Fblog%2F2018%2F11%2F26%2F%E5%AE%88%E6%8A%A4%E7%BA%BF%E7%A8%8B%E5%92%8C%E9%9D%9E%E5%AE%88%E6%8A%A4%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[前言最近在复习的时候，发现一个运行线程池拒绝策略demo中的main方法在运行了之后，进程并没有关闭。看了jconsole线程池中的线程都处于waiting状态。这里是跟我设置线程池的线程工厂中的设置线程是否为后台线程有关。 后台线程和非后台线程后台线程，也叫守护线程，指的是在程序运行的时候后台提供一种通用服务的线程，比如jvm里垃圾回收线程，这种线程并不属于程序中不可或缺的部分。因此，当所有的非守护线程结束时，程序也就终止了，同时会杀死进程中的所有守护线程。反过来说，只要有任何非守护线程在运行，程序就不会终止。 守护线程和非守护线程的区别：在于jvm的离开：如果用户线程（非守护线程）已经全部退出运行了，只剩下守护线程存在，那么虚拟机也就退出了。因为没了被守护者，守护线程也就没有工作可做了。 设置守护线程是通过调用Thread对象的setDaemon(true)方法来实现的。在使用守护线程的时候需要注意以下几点： （1）thread.setDeaemon(true)必须在thread.start()之前设置，否则会报错IllegalThreadStateException （2）在Daemon线程中产生的线程也是Daemon的。 （3）守护线程应该永远不去访问固有资源，如文件、数据库，因为它会在任何时候甚至在一个操作的中间发生终端。 不退出程序的代码这里在main方法结束之后不能正常退出的代码是测试了线程池两种抛出拒绝策略的场景： （1）线程池处于SHUTDOWN状态时，再提交新的任务到线程池 （2）线程池中所有的线程都处于运行状态，并且阻塞队列已满，这时再去提交新的线程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121public class ThreadPoolExecutorRejectNewTaskDemo &#123; /** * 线程池的最大容量 */ private static final int MAX_POOL_SIZE = 3; /** * 阻塞队列的容量 */ private static final int QUEUE_CAPACITY = 2; /** * 非核心线程处于空闲状态的最长时间 */ private static final int KEEP_ALIVE_TIME = 1; /** * 线程池对象 注意这里传入的是默认的拒绝策略 也就是AbortPolicy */ private static final ThreadPoolExecutor threadPool = new ThreadPoolExecutor(MAX_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(QUEUE_CAPACITY), new MyThreadFactory("task-reject")); public static void main(String[] args) &#123;// shutdownThreadPoolToRejectNewTask(); fullQueueThreadPoolRejectNewTask(); &#125; /** * 模拟线程池被关闭之后， 继续向其中提交新任务被拒绝的场景 */ private static void shutdownThreadPoolToRejectNewTask() &#123; MyRunnable runnable = new MyRunnable(); // 先提交 MAX_POOL_SIZE - 1个任务，此时线程池未满 for (int i=0; i &lt; MAX_POOL_SIZE - 1; i++) &#123; System.out.println("提交任务 " + i); threadPool.submit(runnable); &#125; // 在线程池未满的情况下关闭线程池 threadPool.shutdown(); if (threadPool.isShutdown()) &#123; System.out.println("提交任务" + MAX_POOL_SIZE); // 在线程池未满 但却关闭的情况下去提交任务 此时会拒绝 threadPool.submit(runnable); &#125; &#125; /** * 模拟线程池中线程数都在运行并且阻塞队列已满 */ private static void fullQueueThreadPoolRejectNewTask() &#123; MyRunnable myRunnable = new MyRunnable(); // 提交 MAX_POOL_SIZE + QUEUE_SIZE 个任务 使得线程池线程都在执行任务并且阻塞队列已满 for (int i = 0; i &lt; MAX_POOL_SIZE + QUEUE_CAPACITY; i++) &#123; System.out.println("提交任务 " + i); threadPool.submit(myRunnable); &#125; // 此时再去往其中添加 任务 if (threadPool.getActiveCount() == MAX_POOL_SIZE &amp;&amp; threadPool.getQueue().size() == QUEUE_CAPACITY) &#123; threadPool.submit(myRunnable); &#125; &#125; /** * 自定义线程工厂类 */ private static class MyThreadFactory implements ThreadFactory &#123; /** * namePrefix --&gt; 线程名字中的计数 */ private static Map&lt;String, AtomicInteger&gt; THREAD_ID_TABLE = new ConcurrentHashMap&lt;&gt;(); /** * 线程名称前缀 */ private String namePrefix; /** * 是否后台线程 */ private boolean isDamon; public MyThreadFactory(String namePrefix) &#123; this(namePrefix, false); &#125; public MyThreadFactory(String namePrefix, boolean isDamon) &#123; this.namePrefix = namePrefix; this.isDamon = isDamon; &#125; @Override public Thread newThread(Runnable r) &#123; String threadName = namePrefix + "-" + generateThreadId(this.namePrefix); Thread thread = new Thread(r, threadName); thread.setDaemon(this.isDamon); System.out.println("创建线程" + threadName); return thread; &#125; private static int generateThreadId(String namePrefix) &#123; if (!THREAD_ID_TABLE.containsKey(namePrefix)) &#123; THREAD_ID_TABLE.putIfAbsent(namePrefix, new AtomicInteger(0)); &#125; return THREAD_ID_TABLE.get(namePrefix).getAndIncrement(); &#125; &#125; /** * 向线程池提交的任务 */ private static class MyRunnable implements Runnable &#123; @Override public void run() &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 这段代码中创建新线程的线程工厂中设置了线程池中的线程是否为非后台线程，这里设置的是非后台线程，在我们运行main方法之后，即可观察到main方法一直没退出，而线程池中的线程都处于waiting状态。（这里需要设置下不达到拒绝策略的条件，比如去掉达到拒绝策略的条件之后不再提交任务），这里只是复现这个程序不退出的场景。这里只要把新创建的线程设置为后台线程，当main方法结束之后，线程池中的线程都是守护线程，会随着用户线程的结束也被结束掉。 同样的，我们可以看下Executors提供的一些工具线程池，比如Executors.newFixedThreadPool(int nThreads) 它的构造是使用的默认的线程工厂，我们可以看看默认线程工厂中线程的设置是否为后台线程： 1234567891011121314151617181920212223242526272829/** * The default thread factory */static class DefaultThreadFactory implements ThreadFactory &#123; private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() &#123; SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = "pool-" + poolNumber.getAndIncrement() + "-thread-"; &#125; public Thread newThread(Runnable r) &#123; Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; &#125;&#125; 可以看到默认的线程工厂在创建新的线程的时候也是设置daemon为false，这使得定长线程池中都是用户线程，这些个在主线程结束之后，也会使程序不能退出。]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>守护线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[url拼接参数问题]]></title>
    <url>%2Fblog%2F2018%2F11%2F23%2Furl%E6%8B%BC%E6%8E%A5%E5%8F%82%E6%95%B0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题现象在最近的开发过程中要根据一堆id值去删除ES中的数据，就写了一个脚本接口，传入了idList。这里选择的是GET方式的接口，将idList以逗号分隔当做字符串传入当做参数，然后在接口中转换成List类型再对ES进行操作。 脚本代码这个接口中的process是为了控制是否真正执行刷数据的逻辑，在一些刷数据的接口中加入这个参数，可以去在真正去刷数据之前，去看看捞出来的数据是否正确，然后再进行刷数据的逻辑。 1234567891011121314151617181920212223242526272829303132333435363738@GetMapping(value = "/es/fix/removeNotConsumerData")@ResponseBodypublic boolean removeNotConsumerData(@RequestParam(value = "kdtIdList") String kdtIdListString, @RequestParam(value = "process", defaultValue = "false") boolean process) &#123; List&lt;Long&gt; kdtIdList = Arrays.stream(kdtIdListString.split(",")).map(Long::parseLong).collect(Collectors.toList()); log.info("要刷的数据是：&#123;&#125;", JSON.toJSONString(kdtIdList)); if (!process) &#123; return true; &#125; Set&lt;Long&gt; successSet = Sets.newHashSet(); Set&lt;Long&gt; failSet = Sets.newHashSet(); for (Long kdtId : kdtIdList) &#123; try &#123; ESResult esResult = esSyncHelper.deleteDoc(EsConstant.ESV5_TEAM_INDEX, EsConstant.ESV5_TEAM_TYPE, kdtId.toString(), ESResult.class); Thread.sleep(200); if (AppConstant.SUCCESS != esResult.getCode()) &#123; log.warn("刷数据失败,kdtId=&#123;&#125;, msg:&#123;&#125;", kdtId, esResult.getMessage()); failSet.add(kdtId); &#125; else &#123; successSet.add(kdtId); &#125; &#125; catch (Throwable e) &#123; log.warn("刷数据异常, kdtId=&#123;&#125;", kdtId, e); failSet.add(kdtId); &#125; &#125; if (kdtIdList.size() != successSet.size()) &#123; log.warn("有数据删除失败, failSet=&#123;&#125;", JSON.toJSONString(failSet)); return false; &#125; else &#123; return true; &#125;&#125; 这里在机器上进行跑的时候，先去跑了单个id数据，发现是没有问题的。之后就想一把梭去将数据跑完，就把整个id集合数据放入了url要传入的参数之中，这个时候发现出现了问题。 在数据的最后一条id值被截断了，只是去截止到了42122，所以这个会被转为42122存入要刷的id集合中。这个就可能去刷错了数据（所以之前的process参数还是很有用的 = =）。 这里有个比较稳的解决方案是要刷的数据可以把文件放在resource目录下，然后通过读取这个文件的内容直接去去刷数据，这样就不会存在这个问题了。这里去简单记录下这次参数被截断的过程。]]></content>
      <categories>
        <category>bug记录</category>
      </categories>
      <tags>
        <tag>url拼接参数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastJson中的一些用法]]></title>
    <url>%2Fblog%2F2018%2F11%2F21%2FFastJson%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[FastJson中的一些用法总结fastJson在工作过程中经常用到的一个工具类，之前用到的最多的是在输出日志的时候的对java对象输出序列化之后的json字符串，最近在消费消息端也用到了JsonObject这个类的一些功能，做个简单的FastJson功能类的查漏补缺。 JSONObject通过观察fastJson的源码我们可以发现JSONObject是实现了Map&lt;String, Object&gt;接口的，而且其中也定义了一个map字段，在初始化的时候也是根据是否需要有序来初始化为LinkedHashMap或者HashMap。可以说JSONObject就相当于一个Map&lt;String,Object&gt;。 JSONObject.parseObject方法这个方法其实继承的JSON类中的方法，有多个参数的重载，返回JsonObject对象。那么其实就是将json转换成Map接口类型，然后将JSONObject中的map设置值返回的对象。 用法示例： 1JSONObject jsonObject = JSONObject.parseObject(msg); 当然和Json.parseObject一样，也可以在参数中加入Class参数，直接转成对应的类型对象 1TeamES teamES1 = JSONObject.parseObject(msg, TeamES.class); JSONObject.getXxx()方法JSONObject对象既然存在，那么就会提供访问其中一些键值对的访问，其中的键值对就是json字符串对应的键值对。 Object get(Object key) 这个是根据key值获取对应的value，看方法签名就知道这个方法返回的是Object对象。 JSONObject getJSONObject(String key) 这个方法是根据String的key（在使用的时候就是json字符串中的变量名称），再将value转换成对应的JsonObject对象。因为有时候我们json字符串反序列化出来的是一个嵌套对象，所以嵌套内部的json字符串也可以转换成为一个JSONObject对象。当然如果内部就是一个值的对象的时候会调用JSON的toJSON方法返回一个Object。 JSONArray getJSONArray(String key) 有时候key对应的对象是一个数组，那么可以直接转换成一个JSONArray对象，JsonArray对象和JSONObject对象的设计是一样的，实现了List接口并且把list作为一个字段进行使用。 T getObject(String key, Class clazz) 这个方法支持将JSONObject中的值根据传入的key和对应的Class直接转换为对应的T对象，这里T是泛型。同样的这个方法也支持传入Type和TypeReference getXxx(String key) 类似于getBoolean、getDouble、getLong、getBigDecimal等方法就非常明确了，就是根据String的key（变量名称）转换成对应的包装类型或者基本类型的值。这在我们去解析json字符串中的部分键值对比较有用。 在代码中的应用这里根据传入到底是{“KDT_ID”:xxx}还是传入的{“TEAM_ES”:”{…}”}，拿到对应的值，这里就可以应用我们上面提到的JSONObject类对应的方法。 123456789101112131415161718private TeamES convertTeamESFromMap(JSONObject map) &#123; String key = map.keySet().iterator().next(); TeamES teamES = null; switch (key) &#123; case KDT_ID : Long kdtId = map.getLong(KDT_ID); teamES = constructFromDB(kdtId); break; case TEAM_ES : teamES = map.getObject(TEAM_ES, TeamES.class); break; default:break; &#125; return teamES;&#125;]]></content>
      <categories>
        <category>小知识</category>
      </categories>
      <tags>
        <tag>FastJson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8中的LocalDateTime]]></title>
    <url>%2Fblog%2F2018%2F11%2F11%2Fjava8%E4%B8%AD%E7%9A%84LocalDateTime%2F</url>
    <content type="text"><![CDATA[背景最近在项目中遇到了一些时间进行转化的小需求，比如一个时间添加多少天之后，两个时间的比较之类的。这里要去了解一下java8中的新增的时间API–LocalDateTime。 参考博客： 一些用法系统时间12345678910111213141516171819// now方法获取系统时间LocalDate date = LocalDate.now();// getMonth：英文 getMonthValue : 数字System.out.println(date.getYear() + "/" + date.getMonthValue() + "/" + date.getDayOfMonth());LocalTime time = LocalTime.now();System.out.println(time.getHour() + ":" + time.getMinute() + ":" + time.getSecond());// 没有提供 getMiles()方法 可以这样获取mileSystem.out.println(time.get(ChronoField.MILLI_OF_SECOND));// 日期和时间LocalDateTime dateTime = LocalDateTime.now();System.out.println(dateTime.getYear() + "/" + dateTime.getMonthValue() + "/" + dateTime.getDayOfMonth() + " " + dateTime.getHour() + ":" + dateTime.getMinute() + ":" + dateTime.getSecond());// 时区 获取时间戳Clock clock = Clock.systemDefaultZone();System.out.println(clock.millis()); 特定日期12345678910111213141516171819202122// of方法获取特定日期LocalDate myDate = LocalDate.of(2018, 11, 6);System.out.println(myDate.getYear() + "/" + myDate.getMonthValue() + "/" + myDate.getDayOfMonth());// 获取特定日期对应的属性LocalDate independenceDay = LocalDate.of(2018, Month.JUNE, 4);// 获取周几System.out.println(independenceDay.getDayOfWeek());// 构造LocalTimeLocalTime myTime = LocalTime.of(10, 30, 45);System.out.println(myTime.getHour() + ":" + myTime.getMinute() + ":" + myTime.getSecond());// 同样，LocalDateTime也是可以通过of方法创建特定日期LocalDateTime myDateTime = LocalDateTime.of(2018, Month.JUNE, 4, 10, 30, 45);System.out.println(myDateTime.getYear() + "/" + myDateTime.getMonthValue() + "/" + myDateTime.getDayOfMonth()+ " " + myDateTime.getHour() + ":" + myDateTime.getMinute() + ":" + myDateTime.getSecond());// 也提供了LocalDate 和 LocalTime组合而成的LocalDateTimeLocalDateTime myDateTime2 = LocalDateTime.of(myDate, myTime);System.out.println(myDateTime2.getYear() + "/" + myDateTime2.getMonthValue() + "/" + myDateTime2.getDayOfMonth() + " " + myDateTime2.getHour() + ":" + myDateTime2.getMinute() + ":" + myDateTime2.getSecond()); 格式化123456789 */// date --&gt; StringLocalDate formatDate1 = LocalDate.of(2014, 3, 3);String dateString = formatDate1.format(DateTimeFormatter.ofPattern("yyyy/MM/dd"));System.out.println(dateString);// String --&gt; dateLocalDate formatDate2 = LocalDate.parse(dateString, DateTimeFormatter.ofPattern("yyyy/MM/dd"));System.out.println(formatDate2); 日期转换1234567891011// LocalDate --&gt; LocalDateTimeLocalDate changeDate = LocalDate.of(2018, 12, 4);LocalDateTime changeDateTime = changeDate.atTime(10, 20, 30);System.out.println(changeDateTime);// LocalTime --&gt; LocalDateTimeLocalTime changeTime = LocalTime.of(10, 20, 30);LocalDateTime changeDateTime2 = changeTime.atDate(LocalDate.of(2018, 12, 4));System.out.println(changeDateTime2);// LocalDateTime --&gt; LocalDate,LocalTime 有这样的api toLocalDate toLocalTimeSystem.out.println(changeDateTime.toLocalDate());System.out.println(changeDateTime2.toLocalTime()); 日期加减123456789101112LocalDate now = LocalDate.now();// 2天后System.out.println(now.plusDays(2L));// 3天前System.out.println(now.minusDays(3L));// 一年后System.out.println(now.plusYears(1));// 2周前System.out.println(now.minus(2L, ChronoUnit.WEEKS));// 3年2月1天后System.out.println(now.plus(Period.of(3, 2, 1))); 计算间隔123456789LocalDateTime before = LocalDateTime.of(2011, 2, 11, 11, 11, 11);LocalDateTime after = LocalDateTime.of(2014, 2, 11, 11, 11, 11);// Duration 来表示间隔Duration between = Duration.between(before, after);// 间隔的天System.out.println("间隔：" + between.toDays());// 间隔的分钟System.out.println("间隔：" + between.toMinutes()); 日期比较12345LocalDate compareDate1 = LocalDate.of(2011, 1, 1);LocalDate compareDate2 = LocalDate.of(2012, 1, 1);System.out.println(compareDate1.isBefore(compareDate2));int i = compareDate1.compareTo(compareDate2);System.out.println("c1 compareTo c2 is " + i); 和java.util.Date的转换1234567891011121314151617// LocalDateTime --&gt; Instant --&gt; DateLocalDateTime currentLocalDateTime = LocalDateTime.now();Instant instant = currentLocalDateTime.atZone(ZoneId.systemDefault()).toInstant();Date date1 = Date.from(instant);System.out.println(date1);// Date --&gt; Instant --&gt; LocalDateTimeDate date2 = new Date();Instant instant1 = date2.toInstant();LocalDateTime dateTime1 = LocalDateTime.ofInstant(instant1, ZoneId.systemDefault());System.out.println(dateTime1);// Calendar --&gt; Instant --&gt; LocalDateTimeCalendar calendar = Calendar.getInstance();Instant instant2 = calendar.toInstant();LocalDateTime dateTime2 = LocalDateTime.ofInstant(instant2, ZoneId.systemDefault());System.out.println(dateTime2);]]></content>
      <categories>
        <category>Java语法</category>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>Java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot配置jsp]]></title>
    <url>%2Fblog%2F2018%2F10%2F23%2Fspring-boot%E9%85%8D%E7%BD%AEjsp%2F</url>
    <content type="text"><![CDATA[spring-boot中jsp的使用jsp是之前在学习java开发中会学习到的知识，虽然现在公司中虽然使用jsp越来越少，但是spring-boot配置jsp的使用还是应该去记录一下。 相关依赖增加这里要加入一些依赖： 123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- jsper渲染引擎 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 内置tomact --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- jstl 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;/dependency&gt; 还要需要注意的是spring-boot默认打包方式jar包的形式，这里要换成war包的方式。 激活传统Servlet web部署springboot1.4版本之后通过实现org.springframework.boot.web.support.SpringBootServletInitializer抽象类中的抽象方法来将启动类添加到souce中 12345678public class JspConfig extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; // source启动类 告知一些静态资源 builder.sources(SpringBootDemoApplication.class); return builder; &#125;&#125; 加入资源目录位置在项目的src/main目录下建立一个webapp文件夹，这个webapp目录下建立WEB-INFO和jsp文件夹，写一个index.jsp文件作为之后的测试页面。 目录： 12345&lt;html&gt;&lt;body&gt;hello, $&#123;message&#125;&lt;/body&gt;&lt;/html&gt; 设置访问资源文件的前缀和后缀在application.properties配置文件中配置访问jsp文件中的prefix和suffix，注意这里这prefix中的开头和结尾的/是不能省略的，否则会访问不到你的资源。 123# 访问jsp资源的前缀和后缀spring.mvc.view.prefix = /WEB-INFO/jsp/spring.mvc.view.suffix = .jsp 写一个test的controller在配置好了之后，写一个controller作为入口去访问这个jsp文件 1234567891011@Controllerpublic class JspController &#123; @RequestMapping(value = "/index") public String index(Model model) &#123; model.addAttribute("message", "zlj"); return "index"; &#125;&#125; 这时候在浏览器中输入localhost:7001/index即可访问到我们返回给index.jsp中message占位符的字符串值。]]></content>
      <categories>
        <category>spring</category>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
        <tag>actuator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——ThreadPoolExecutor源码分析（二）]]></title>
    <url>%2Fblog%2F2018%2F10%2F11%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E2%80%94%E2%80%94ThreadPoolExecutor%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言在上一篇中，我们分析了ThreadPoolExecutor中关键变量ctl，这篇我们继续来看ThreadPoolExecutor中的构造函数及其参数。其中参数的相关解释来源于源码中的相关注释。 构造函数我们可以看到ThreadPoolExecutor有四个构造函数： 他们其实都是调用其中的全参数的构造函数，只不过有一些参数是使用了默认提供的参数。我们可以看一下构造函数： 123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 可以看到构造函数现对于参数去进行了校验：（1）corePoolSize必须大于等于0，maximumPoolSize必须大于0，maximumPoolSize必须大于等于corePoolSize，keepAliveTime如果传入了必须大于0。 （2）workQueue、threadFactory、handler不能为空 （3）其中有acc字段的设置是为了设置安全管理器，我们可以自定义我们的安全管理器，否则为从上下文中去拿。可以参考博客：jvm中的安全管理器 构造函数中的参数1.corePoolSizecorePoolSize参数表示线程池中一直存活的线程的最小数量，这些一直存活的线程被称为核心线程，默认情况下，核心线程的最小数量都是整数，除非是调用了allowCoreThreadTimeout()方法并且传入了true，即允许核心线程数在空闲状态下超时而停止（terminated状态），此时如果所有的核心线程先后都因为超时停止，那么线程池中核心线程数会变为0。默认情况下，核心线程是按照需要创建并启动的，也就是说只有当线程池接收到我们提交的任务后，它才会去创建并启动一定的核心线程去执行这些任务。如果没有接收到相关任务，就不会去主动创建核心线程，这种默认的核心线程创建启动方式变主动为被动，类似于观察者模式，有利于降低系统资源的消耗。当然，也可以通过设置preStartCoreThread()或者preStartAllCoreThreads()方法来改变这一机制，使得在新任务还未提交到线程池的时候，线程池就已经创建并启动了一个或所有线程，并让这些核心线程在池中等待任务的到来。 2.maximumPoolSizemaximumPoolSize表示线程池中能容量线程的最大数量，这个值不能超过常量CAPACITY的数值大小，上一篇中也提到了常量CAPACITY的计算方式，这里不去赘述。但是注意一点，当我们提供的工作队列是一个无界的队列，那么这里提供的maximumPoolSize将毫无意义。 当我们通过execute方法提交一个任务的时候： （1）如果线程池处于运行状态（RUNNING）的线程数量小于核心线程数（corePoolSize），那么即使有一些非核心线程处于空闲状态，系统也倾向于新建一个线程来处理这个任务。 （2）如果线程池处于运行状态（RUNNING）的线程数量大于核心线程数（corePoolSize），但又小于maximumPoolSize，那么系统会去判断线程池内部的阻塞队列是否有空位子，如果有空位子，系统会将该任务先存入阻塞队列，如果发现队列中已没有空位子（即队列已满），系统会创建一个新的线程来执行任务。 如果将线程池中的corePoolSize和maximumPoolSize设置为相同的数（也就是说线程池中所有线程都是核心线程），那么该线程池就是一个固定容量的池子。如果将线程池的maximumPoolSize设置为一个非常大的数值（例如Integer.MAX_VALUE），那么相当于允许线程池自己在不同时段调整参与并发的总任务数。通常情况下，都是通过构造函数去初始化corePoolSize和maximumPoolSize，也可以通过set方法调整这两个参数的大小。 3.keepAliveTime &amp; unitkeepAliveTime表示空闲线程处于等待的超时时间，超过该时间后该线程会停止工作。当线程池中总线程数量大于corePoolSize并且allowCoreThreadTimeOut为false时，这些多出来的非核心线程一旦进入空闲等待的状态，就开始计算各自的等待时间，并且这里设定的keepAliveTime的数值作为他们的超时时间，一旦某个非核心线程的等待时间到达了超时时间，该线程就会停止工作（terminated）。而如果不去设置allowCoreThreadTimeout为true，核心线程及时处于空闲状态等待了keepAliveTime，也依然可以继续处于空闲状态等待。 比较好的应用实践： 如果要执行的任务相对较多，并且每个任务执行的时间都比较短，那么可以为keepAliveTime参数设置一个相对较大的值，以提高线程的利用率；如果要执行的任务比较少，线程池使用率比较低，那么可以先将该参数设置为一个较小的参数值，通过超时停机的机制来降低系统资源的开销。 注意一点：构造函数中的参数keepAliveTime和unit这个参数和ThreadPoolExecutor中的keepAliveTime字段的值不一定相等，字段被设置为long型的值，且定义为纳秒的单位，构造函数中的参数还有unit单位，应该是keepAliveTime和unit计算的结果换算为纳秒才和类中的字段是一样的值。 keepAliveTime在构造函数中的类型是long型，这样保证了这个值不会太短。 4.workQueue构造函数中的workQueue是一个BlockIngQueue（阻塞队列）的实例。传入的泛型参数是Runnable，也就是说，workQueue是一个内部元素为Runnable（各种任务，通常是异步的任务）的阻塞队列。阻塞队列是一种类似于”生产者-消费者“模式的队列，当队列已满时如果继续向队列中插入元素，该插入操作将被阻塞一直处于等待状态，直到队列中有元素被移除，才能进行插入操作；当队列为空时如果继续执行元素的删除或者获取操作，也会被阻塞进入等待队列中有新的元素之后才能执行。 workQueue是一个用于保存等待执行的任务阻塞队列，当提交一个新的任务到线程池后，线程池会根据当前池子正在运行的线程数量来判断对这个任务的处理方式： （1）如果线程池中正在运行的线程数少于核心线程数，那么线程池总是倾向于新建一个线程来执行该任务。 （2）如果线程池中正在运行的线程数不少于核心线程数，那么线程池把该任务提交到workQueue中让其先等待 （3）如果线程池中正在运行的线程数不少于核心线程数，并且线程池中的阻塞队列也满了使得该任务入队失败，那么线程池会去判断当前池子中运行的线程数是否已经等于了该线程池允许运行的最大线程数。如果发现已经等于，说明池子已满，那么就会执行拒绝策略；如果发现运行的线程数小于池子允许的最大线程数，那么会创建一个线程（这个线程是非核心线程）来执行该任务。 这其中，队列对于提交的任务一般有三种策略：（1）直接切换 常用的队列是SynchronousQueue（同步队列）,这个队列内部不会存储元素，每一次插入操作都会先进入阻塞状态，一直等到另一个线程执行了队列的删除操作，然后该插入操作才会执行。当提交一个任务到包含这种SynchronousQueue队列的线程池后，线程池会去检测是否有可用的线程来执行任务，如果没有则创建一个新的线程来执行任务而不是将任务存储在任务队列中。”直接切换“的意思是：处理方式由”将该任务暂时存储在阻塞队列中“直接切换为”新建一个线程来处理任务“。这种执行策略适合处理多个有相互依赖关系的任务，因为该策略可以避免这些任务因一个没有及时处理而导致依赖于该任务的其他任务也不能及时处理而造成的锁定结果。因为这种策略的目的是要让几乎每一个新提交的任务都能立即得到处理，所以这种策略通常配合maximumPoolSize是无边际（Integer.MAX_VALUE）的。我们知道的静态工厂方法Executors.newCachedThreadPool()就是使用了这种直接切换的队列。 （2）使用无界队列 不预设队列的容量，队列将使用Integer.MAX_VALUE作为默认容量，例如：基于链表的阻塞队列 LinkedBlockingQueue。使用无界队列使得线程池中能创建的最大线程数等于核心线程数，这样的线程池的maxmumPoolSize的数值将不起任何作用。如果向线程池中提交一个新任务时发现所有的核心线程都处于运行状态，那么该任务将被放入无界队列中等待处理。当要处理的多个任务之间没有相互依赖关系的时候，就适合用这种队列策略来处理这些任务。静态工厂方法Executors.newFixedThreadPool()就使用了这个队列。 （3）使用有界队列 例如使用基于数组的阻塞队列 ArrayBlockingQueue。当要求线程池的最大线程数maximumPoolSize要限定在某个值以内的时候，线程池使用有界队列能降低资源的消耗，但这也使得线程池对线程的调控变得更加困难。因为队列容量和线程池容量都是有限的值，要想使线程处理任务的吞吐量在一个相对合理的范围内，同时又能使线程调度的难度相对较低，并且又尽可能节省系统资源的消耗，那么需要合理的调配这两个值。通常来说，设置较大的队列容量和较小的线程池容量，能够降低系统的资源的消耗（包括CPU的使用率，操作系统的消耗，上下文环境的切换的开销等），但是会降低系统吞吐率。如果发现提交的任务经常频繁的发生阻塞的情况，那么你可以考虑增大线程池的容量，可以通过setMaximumPoolSize()方法来重新设定线程池的容量。而设置较小的队列量时，通常需要将线程池的容量设置大一点，这种情况下，cpu的使用率会比较高，但是如果设置线程池的容量过大的时候，线程调度成了问题，反而使得吞吐率比较低。 5.threadFactory线程工厂，用于创建线程。默认使用Executors.defaultThreadFactory()方法创建线程工厂： 当然我们也可以自己实现ThreadFactory接口去实现我们自己的线程工厂。下边就是可以根据不同的namePrefix去获取单例线程的线程工厂： 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 自定义线程工厂类 */private static class MyThreadFactory implements ThreadFactory &#123; /** * namePrefix --&gt; 线程名字中的计数 */ private static Map&lt;String, AtomicInteger&gt; THREAD_ID_TABLE = new ConcurrentHashMap&lt;&gt;(); /** * 线程名称前缀 */ private String namePrefix; /** * 是否后台线程 */ private boolean isDamon; public MyThreadFactory(String namePrefix) &#123; this(namePrefix, true); &#125; public MyThreadFactory(String namePrefix, boolean isDamon) &#123; this.namePrefix = namePrefix; this.isDamon = isDamon; &#125; @Override public Thread newThread(Runnable r) &#123; String threadName = namePrefix + "-" + generateThreadId(this.namePrefix); Thread thread = new Thread(r, threadName); thread.setDaemon(this.isDamon); System.out.println("创建线程" + threadName); return thread; &#125; private static int generateThreadId(String namePrefix) &#123; // 判断后执行 concurrentHashMap不能保证完全线程安全 用了putIfAbsent if (!THREAD_ID_TABLE.containsKey(namePrefix)) &#123; THREAD_ID_TABLE.putIfAbsent(namePrefix, new AtomicInteger(0)); &#125; return THREAD_ID_TABLE.get(namePrefix).getAndIncrement(); &#125;&#125; 6.handler当满足以下两个条件其中一个的时候，如果继续向线程池中提交新的任务，那么线程池会调用内部的RejectedExecutionHandler对象的rejectedExecution()方法，表示拒绝执行这些新提交的任务： （1）当线程池处于SHUTDOWN状态时（不论线程池和阻塞队列是否已满） （2）当线程池中所有的线程都处于运行状态并且线程池中的阻塞队列已满。 一个demo去演示这两个情况：执行handler的两种情况 当采用默认的拒绝策略，线程池会使用抛出异常的方式来拒绝新任务的提交，这种拒绝方式在线程池中被称为AbortPolicy，我们可以来看下有哪些拒绝策略：（1）AbortPolicy 这中处理方式是直接抛出RejectedExecutionException异常，如果在ThreadPoolExecutor的构造函数中未指定RejectedExecutionHandler参数，那么线程池将使用defaultHandler参数，而这个就是采用的AbortPolicy。 （2）CallerRunsPolicy 将提交的任务放在ThreadPoolExecutor.execute()方法所在的那个线程执行。 （3）DiscardPolicy 直接不执行新提交的任务 （4）DiscardOldestPolicy 这个可以看源码中的解释： 由源码就可以知道，这种处理方式有两种情况：一，当线程池处于SHUTDOWN状态时，就默认不执行这个任务，即DiscardPolicy；二，当线程池处于运行状态时，会将队列中处于队首（head）的那个任务从队列中移除，然后将这个新提交的任务加入到阻塞队列中的队尾（tail）等待执行。 当然，RejectedExecutionHandler其实是个接口，我们可以自定义类去实现这个接口，重写rejectedExecution方法使用自己想要的拒绝策略即可。 下一篇这篇把线程池中的核心参数进行了一些解释，在下一篇中我们将介绍线程池进行任务调度的原理。]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
        <tag>ThreadPoolExecutor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8增加的接口中默认方法]]></title>
    <url>%2Fblog%2F2018%2F10%2F10%2Fjava8%E5%A2%9E%E5%8A%A0%E7%9A%84%E6%8E%A5%E5%8F%A3%E4%B8%AD%E9%BB%98%E8%AE%A4%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[前言最近在工作中的一次小修改让自己应用到了java8中的新特性：接口默认方法，这里去简单记录下。在java8之后可以在接口定义方法的实现，成为default方法，类似于Scala中的trait。比如在Iterable接口中新增了foreach默认方法： 12345678910111213141516171819202122232425/** * Performs the given action for each element of the &#123;@code Iterable&#125; * until all elements have been processed or the action throws an * exception. Unless otherwise specified by the implementing class, * actions are performed in the order of iteration (if an iteration order * is specified). Exceptions thrown by the action are relayed to the * caller. * * @implSpec * &lt;p&gt;The default implementation behaves as if: * &lt;pre&gt;&#123;@code * for (T t : this) * action.accept(t); * &#125;&lt;/pre&gt; * * @param action The action to be performed for each element * @throws NullPointerException if the specified action is null * @since 1.8 */default void forEach(Consumer&lt;? super T&gt; action) &#123; Objects.requireNonNull(action); for (T t : this) &#123; action.accept(t); &#125;&#125; 这个default方法的主要目的是为java8的Lambda表达式提供支持，如果将这个方法定义为普通接口方法，则会对现有的JDK的其他使用Iterable接口的类造成影响，因此提供了default方法的功能。 工作中的例子有一个小需求是对一个表的插入的实体的name字段做一个处理，这里name字段如果为空则用手机号（默认手机号不为空）进行插入。因为调用这个insert方法的业务代码比较多，每个都去做这个逻辑会显得很麻烦并且很重复。所以就想到了直接在mapper的xml中去进行修改： 12345insert into table_name ( &lt;if test=&quot;name ==null or name == &apos;&apos;&quot;&gt;NAME,&lt;/if&gt; ) values( &lt;if test=&quot;name == null or name == &apos;&apos;&quot;&gt;#&#123;mobile&#125;&lt;/if&gt;) 这样确实能够实现我们这个小需求，但是这个insert的mapper.xml是通过代码工具自动生成的标准insert方法，并且这样写可读性也不好，给人一种很奇怪的感觉。 这时候就用到了默认方法。这里我们可以在接口中定义一个默认方法insert，然后将之前的insert方法更换名称，在默认方法中去调用更换之后插入方法，而在默认方法中去做如果name为空则用手机号去代替这个逻辑。 12345678910111213int insertDefault(Clues entity);/** * * @param entity * @return */default int insert(Clues entity) &#123; if (StringUtils.isEmpty(entity.getName()) &amp;&amp; StringUtils.isNotEmpty(entity.getMobile())) &#123; entity.setName(entity.getMobile()); &#125; return insertDefault(entity);&#125; 这样原来调用的insert方法也不需要去做更改，并且也不用在xml中进行改动就实现了这个小逻辑。注意要将xml中方insert方法改为insertDefault，这个更改比上边那种修改要显得合理的多。 默认方法的一个总结java是面向对象的语言，那么就会有实现接口和继承父类，那么这些会对接口的默认方法有什么影响呢？下边参考博客：默认方法 存在一个父接口，定义了一个default方法： 12345public interface Parent &#123; default String doit() &#123; return &quot;Parent&quot;; &#125;&#125; 有一个类实现该接口，使用了默认的default方法： 123public class ParentImpl implements Parent&#123;&#125; 有一个ParentImpl2继承了ParentImpl，里面重写了接口中的默认方法： 1234567public class ParentImpl2 extends ParentImpl &#123; @Override public String doit() &#123; return &quot;ParentImpl2&quot;; &#125;&#125; 有一个Child接口继承了Parent接口，并且重写了Parent接口中的默认方法： 12345678910public interface Child extends Parent &#123; /** * 重写父接口中的default方法 * @return */ @Override default String doit() &#123; return &quot;Child&quot;; &#125;&#125; 有一个ChildImpl实现了Child： 12public class ChildImpl implements Child &#123;&#125; 又有一个ChildImpl2继承了ParentImpl2也实现了Child接口，为了测试当子类实现的接口和继承的父类中都有默认方法的场景: 12public class ChildImpl2 extends ParentImpl2 implements Child &#123;&#125; 测试类： 123456789101112131415161718192021222324252627282930313233public class Main &#123; public static void main(String[] args) &#123; /** * 测试实现类可以直接调用接口中的default方法 */ Parent parentImpl = new ParentImpl(); // 将输出Parent System.out.println(parentImpl.doit()); /** * 测试Child接口重写了Parent接口的default方法 */ Child child = new ChildImpl(); // 将输出Child System.out.println(child.doit()); /** * 测试ParentImpl2重写了Parent接口中default方法 */ Parent parentImpl2 = new ParentImpl2(); // 将输出ParentImpl2 System.out.println(parentImpl2.doit()); /** * 测试ChildImpl2父类和实现的接口都有default方法，优先使用父类中定义的方法 */ Child childImpl2 = new ChildImpl2(); // 将输出ParentImpl2 System.out.println(childImpl2.doit()); &#125;&#125; 从上述测试结果可以看出： 实现类可以直接使用父接口中定义的default方法。 接口可以重写父接口中定义的default方法。 实现类可以重写父接口中定义的方法、 当父类和父接口都存在default方法时，使用父类中重写的default方法 特别的，如果一个类实现了两个接口，这两个接口中有同名的default方法签名时，此时会编译不通过，必须在子类中重写这个default方法。]]></content>
      <categories>
        <category>Java语法</category>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>Java8</tag>
        <tag>接口默认方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——ThreadPoolExecutor源码分析（一）]]></title>
    <url>%2Fblog%2F2018%2F10%2F04%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E2%80%94%E2%80%94ThreadPoolExecutor%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言线程池是并发编程中最重要的应用之一，使用线程池可以防止大量的创建和销毁线程的过程，可以节省很多的内存空间，提高程序的响应率和cpu的利用率，并且也可以对线程进行统一管理和监控。这里将分几篇文章介绍一下线程池的源码分析。本篇是分析ThreadPoolExecutor中的ctl变量。并且去写了线程中的 ctl变量源码中的解释ThreadPoolExecutor中有个字段是ctl，具体来说是对线程池的运行状态和池子中的有效线程数量的控制的一个字段变量，我们可以看看源码中的解释： 123456789101112/** * The main pool control state, ctl, is an atomic integer packing * two conceptual fields * workerCount, indicating the effective number of threads * runState, indicating whether running, shutting down etc * * In order to pack them into one int, we limit workerCount to * (2^29)-1 (about 500 million) threads rather than (2^31)-1 (2 * billion) otherwise representable. If this is ever an issue in * the future, the variable can be changed to be an AtomicLong, * and the shift/mask constants below adjusted. But until the need * arises, this code is a bit faster and simpler using an int. 可以看到ctl是AtomicInteger对象，里面的操作都是基于CAS的原子操作。一个ctl变量包含两部分信息，因为int类型的变量是32位的，所以高3位表示线程池的运行状态（runState），低29位表示线程池中有效线程数（workerCount）。 所以，当我们知道了线程池中的运行状态和有效线程数，就可以通过ctlOf方法计算出ctl的值： 1private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 反过来，我们也可以通过ctl计算出runState和workerCount的值： 12private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125; 其中CAPACITY是(2^29)-1， 也就是高三位是0，低29位是1的一个int类型的数字常量。这个CAPACITY表示线程池中有效线程的上限值。这个值的计算过程： 12private static final int COUNT_BITS = Integer.SIZE - 3; // 29private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1; 线程池的状态线程池中有五种状态： 123456// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; 其中COUNT_BITS的值是29，在后边的源码分析过程中，我们其实只需要知道这几个状态是逐渐递增的即可。比如说在源码中看到 rs &lt; SHUTDOWN 其实就是表示此时线程池的运行状态是RUNNING。 五种状态的解释： （1）RUNNING：运行状态，能接受提交新的任务，并且也能处理阻塞队列中的任务。 （2）SHUTDOWN：关闭状态，不再接受新提交的任务，但是可以处理阻塞在任务队列中已保存的任务。在线程池处于RUNNING状态的时候，调用shutdown方法线程池即变为此状态。 （3）STOP：停止状态，不再接受新提交的任务，也不会处理阻塞队列中已保存的任务，并且会中断正在处理的任务，在线程池处于RUNNING状态或者SHUTDOWN状态的时候，调用shutdownNow方法线程池即进入该状态。 （4）TIDYING：清理状态，所有的任务都已经终止，workerCount有效线程数量为0，线程池进入该状态后调用terminated方法可以使线程池进入Terminated状态。当线程池处于SHUTDOWN状态时，如果此后线程池中没有线程并且阻塞队列中没有要执行的任务，就会进入到这个状态；当线程池处于STOP状态时，如果此时线程池中没有线程了，线程池会进入该状态。（5）TERMINATED：terminated()方法执行之后会进入该状态。 其他字段参数12345678910111213141516171819202122232425262728293031323334//线程池控制器private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));//任务队列private final BlockingQueue&lt;Runnable&gt; workQueue;//全局锁private final ReentrantLock mainLock = new ReentrantLock();//工作线程集合private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;();//终止条件 - 用于等待任务完成后才终止线程池private final Condition termination = mainLock.newCondition();//曾创建过的最大线程数private int largestPoolSize;//线程池已完成总任务数private long completedTaskCount;//工作线程创建工厂private volatile ThreadFactory threadFactory;//饱和拒绝策略执行器private volatile RejectedExecutionHandler handler;//工作线程活动保持时间(超时后会被回收) - 纳秒private volatile long keepAliveTime;/** * 允许核心工作线程响应超时回收 * false：核心工作线程即使空闲超时依旧存活 * true：核心工作线程一旦超过keepAliveTime仍然空闲就被回收 */private volatile boolean allowCoreThreadTimeOut;//核心工作线程数private volatile int corePoolSize;//最大工作线程数private volatile int maximumPoolSize;//默认饱和策略执行器 - AbortPolicy -&gt; 直接抛出异常private static final RejectedExecutionHandler defaultHandler = new AbortPolicy(); 下一章下一章将会去写一下ThreadPoolExecutor的构造函数和核心参数。]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
        <tag>ThreadPoolExecutor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi命令小总结（二）]]></title>
    <url>%2Fblog%2F2018%2F09%2F30%2Fvi%E5%91%BD%E4%BB%A4%E5%B0%8F%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[在vi编辑模式下显示行数在vi编辑模式下可以显示下行数，比如在php调试模式下可以根据相应的行数的代码去打印值调试代码。 方法：在vi模式下输入:set nu即可。也可以直接:line number跳转到对应的行数。 在vi编辑模式中撤回一个操作在INSERT模式下如果写了一些操作，然后想撤回这个操作，按一下esc之后输入u即可。如果是想撤回刚才那个撤回操作，可以按了esc之后点击Ctrl + r。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>vi命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot配置swagger]]></title>
    <url>%2Fblog%2F2018%2F09%2F24%2Fspring-boot%E9%85%8D%E7%BD%AEswagger%2F</url>
    <content type="text"><![CDATA[前言swagger是一个很好的restful形式的api文档，可以通过比较小的侵入来提供很好的restful的文档。因为swagger是依赖服务生成的，所以其实是依赖服务的，这也算是它的一个小缺点吧。但是其实如果一个项目习惯去手写文档之后，也是可以的，但是新的项目还是建议去用一些自动生成的文档，省去了很多麻烦。 spring boot配置swagger引入swagger依赖12345678910&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt; 编写swagger对应的配置1234567891011121314151617181920212223242526272829303132333435363738import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import springfox.documentation.builders.ApiInfoBuilder;import springfox.documentation.builders.PathSelectors;import springfox.documentation.builders.RequestHandlerSelectors;import springfox.documentation.service.ApiInfo;import springfox.documentation.spi.DocumentationType;import springfox.documentation.spring.web.plugins.Docket;import springfox.documentation.swagger2.annotations.EnableSwagger2;@Configuration@EnableSwagger2public class SwaggerConfig &#123; @Bean public Docket createRestApi() &#123; // 文档类型 return new Docket(DocumentationType.SWAGGER_2) // 创建api的基本信息 .apiInfo(apiInfo()) // 选择哪些接口去暴露 .select() // 扫描的包 .apis(RequestHandlerSelectors.basePackage(&quot;com.demo.web.controller&quot;)) .paths(PathSelectors.any()) .build(); &#125; private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() .title(&quot;groundhog-web swagger文档&quot;) .contact(&quot;name&quot;) .version(&quot;1.0&quot;) .build(); &#125;&#125; 在api和请求参数中使用注解接口中使用swagger注解 123456789101112@RestController@Api(value = &quot;测试swagger&quot;, description = &quot;测试swagger api&quot;)public class TestSwaggerController &#123; @ApiOperation(value = &quot;返回url中的参数&quot;, notes = &quot;返回url中的参数&quot;) @ApiImplicitParam(name = &quot;id&quot;, value = &quot;id值&quot;, paramType = &quot;path&quot;, required = true, dataType = &quot;Integer&quot;) @GetMapping(path = &quot;/getUrlParam/&#123;id&#125;&quot;) public Integer getUrlParam(@PathVariable(value = &quot;id&quot;) Integer id) &#123; return id; &#125;&#125; 可以访问localhost:port/swagger-ui.html看到生成的swagger文档。可以看到请求结果： 也可以看到之前post方法的接口也可以生成对于的参数文档，这里也可以对表单参数bean使用@ApiModel和@ApiProperty注解进行标识。 swagger相关注解和官方文档swagger常用注解： @Api：修饰整个类，描述controller的作用 @ApiOperation：描述一个类的一个方法，或者说一个接口 @ApiParam：单个参数描述 @ApiModel：用对象来接收参数 @ApiProperty：用对象接收参数时，描述对象的一个字段 @ApiImplicitParam：一个请求参数 @ApiImplicitParams：多个请求参数 这里推荐下官方文档，感兴趣可以看一下其他注解和相关配置： 注解官方文档]]></content>
      <categories>
        <category>spring</category>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>swagger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[两个切面配置的记录]]></title>
    <url>%2Fblog%2F2018%2F09%2F21%2F%E4%B8%A4%E4%B8%AA%E5%88%87%E9%9D%A2%E9%85%8D%E7%BD%AE%E7%9A%84%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[前言之前也学习过validate注解去校验一些参数的必要条件，这回工作中用的时候遇到了一些问题。这里都是使用spring boot框架去做的切面，省去了很多不需要的配置。这里去记录一下。 两个切面validate注解这里主要是去使用了hibernate中的注解而去做的切面，之前在博客中写到的整合了两个标准的注解的切面（注意区分下）。看一下切面的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182@Aspect@Configurationpublic class ValidateAspect &#123; /** * BEAN对象校验器 配置快速失败模式 */ private final Validator BEAN_VALIDATOR = Validation.byProvider(HibernateValidator.class) .configure() //快速失败模式开启，当检测到有一项失败立即停止 .failFast(true) .buildValidatorFactory().getValidator(); /** * 方法参数对象校验器 */ private final ExecutableValidator METHOD_VALIDATOR = BEAN_VALIDATOR.forExecutables(); /** * point配置 */ @Pointcut(&quot;execution(* com.zhanglijun.springbootdemo.web.controller..*.*(..))&quot;) public void pointcut() &#123; &#125; @Before(&quot;pointcut()&quot;) public void before(JoinPoint point) &#123; // 获得切入目标对象 Object target = point.getThis(); // 获得切入方法参数 Object [] args = point.getArgs(); // 获得切入的方法 Method method = ((MethodSignature)point.getSignature()).getMethod(); // 校验以基本数据类型 为方法参数的 checkWithResult(validMethodParams(target, method, args)); // 校验以java bean对象 for (Object bean : args) &#123; if (null != bean) &#123; checkWithResult(validBeanParams(bean)); &#125; &#125; &#125; /** * 校验bean对象中的参数 * @param bean * @param &lt;T&gt; * @return */ private &lt;T&gt; Set&lt;ConstraintViolation&lt;T&gt;&gt; validBeanParams(T bean) &#123; return BEAN_VALIDATOR.validate(bean); &#125; /** * 校验方法中的参数 * @param obj * @param method * @param params * @param &lt;T&gt; * @return */ private &lt;T&gt; Set&lt;ConstraintViolation&lt;T&gt;&gt; validMethodParams(T obj, Method method, Object [] params)&#123; return METHOD_VALIDATOR.validateParameters(obj, method, params); &#125; /** * 校验参数校验结果 * @param set */ private void checkWithResult(Set&lt;ConstraintViolation&lt;Object&gt;&gt; set) &#123; if (CollectionUtils.isEmpty(set)) &#123; return; &#125; Iterator&lt;ConstraintViolation&lt;Object&gt;&gt; methodIterator = set.iterator(); if (methodIterator.hasNext()) &#123; throw new IllegalArgumentException(methodIterator.next().getMessage()); &#125; &#125;&#125; 这个可以去实现validate注解的切面配置。 访问接口信息注解还有就是参考翟DD博客中的spring boot配置的一个记录访问接口信息的切面，这里在配置了之后遇到了两个问题，去记录一下。 切面配置12345678910111213141516171819202122232425262728293031323334353637@Aspect@Configuration@Slf4jpublic class WebRequestAspect &#123; // TODO 可以加上sessionUser的获取 @Pointcut(&quot;execution(* com.zhanglijun.springbootdemo.web.controller..*.*(..))&quot;) public void webLog()&#123;&#125; @Before(&quot;webLog()&quot;) public void doBefore(JoinPoint joinPoint) &#123; // 获取request对象 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); if (attributes == null) &#123; return; &#125; HttpServletRequest request = attributes.getRequest(); log.info(&quot;REQUEST_URL : &#123;&#125;, HTTP_METHOD : &#123;&#125;, ARGS : &#123;&#125;&quot;, request.getRequestURL().toString(), request.getMethod(), JSON.toJSONString(joinPoint.getArgs())); &#125; @AfterReturning(returning = &quot;ret&quot;, pointcut = &quot;webLog()&quot;) public void doAfterReturning(Object ret) &#123; // 打印返回内容 ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); if (attributes == null) &#123; // 容器启动时也去初始化了切面（因为是execution）这里去判断一下 return; &#125; HttpServletRequest request = attributes.getRequest(); log.info(&quot;REQUEST_URL : &#123;&#125;, RESPONSE : &#123;&#125; &quot;, request.getRequestURL(), JSON.toJSONString(ret)); &#125;&#125; 问题1 fastJson去打印入参在doBefore方法中去做了对方法参数的打印，这里没有思索去用了fastJson去打印参数，但是当去访问一个参数中带有response对象的时，会发生报错。 接口： 123456//========================= 测试web log切面 =====================@GetMapping(path = &quot;/testAspect&quot;, produces = MediaType.APPLICATION_JSON_VALUE)public void testAspect(HttpServletResponse response) &#123; log.info(&quot;测试web log 切面 fastJson序列化response对象会报错&quot;);&#125; 报错： 这个错误在网上找了比较久，也跟进去源码看了，reponse的getOutputSteam在走到切面的这个输出的时候，已经被调用过了，会有一个标志位标志其被调用过，这里如果用fastJson去序列化这个response对象，这里会报错。这就提醒我们这里输出参数不能去使用fastJson去序列化。（有的mapping还是会去用到response对象）。修改方法就是可以把输出参数的部分改成Arrays.toString进行输出。 1Arrays.toString(joinPoint.getArgs())); 这里要注意参数内部的toString方法的实现。 request对象获取的问题这个切面中因为要记录访问的url，所以用到了HttpServletRequest对象，这里获取的方式是：RequestContextHolder.getRequestAttributes()；这个实现的原理就是请求到来的时候去使用ThreadLocal放入request对象，所以在@AfterReturning中也可以去使用这个对象（因为这次请求还没有结束）。但是在项目初始化时，这个切面就会被加载一次，此时并没有请求所以ThreadLocal中也不会有这个request对象。 解决办法：我这里去对request对象做了判断，如果是null的话，那么方法直接返回。 这里也可以去写一下@AfterThrowing之后的处理，可以直接throw出去交给统一异常处理。这里注意切面的order属性和数据层面的事务的order大小顺序。order越小，执行的越靠前。 当你的切面order&gt;数据层面的事务order时，执行顺序是: transaction -&gt; doBefore -&gt; Exception -&gt; @AfterThrowing -&gt; rollback。注意这个时候会造成你的@AfterThrowing内容不生效，一起rollback了。 而当你的切面order &lt; 数据层面事务order时，执行顺序是： dobefore -&gt; transaction -&gt; exception -&gt; rollback -&gt; @AfterThrowing 所以要配置你的webLog切面的order小一些。在spring boot框架中可以通过@Order(level)在类上加上注解进行order控制。]]></content>
      <categories>
        <category>spring</category>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>validate注解</tag>
        <tag>aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi命令小总结]]></title>
    <url>%2Fblog%2F2018%2F09%2F09%2Fvi%E5%91%BD%E4%BB%A4%E5%B0%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[前言这篇去写一些最近在工作中get到的关于vi/vim命令的点，简单去记录下。 技能点在文件中快速删除一行在vi编辑文件的时候，发现有时要删除很多的文件内容，这个时候去一点点删除很慢，这里get到了一个快速删除一行的技能。在打开的文件中所要删除的行连续按两次d就可以快速删除一行，然后在用:wq保存即可。 解决没有正确关闭vi打开的文件有的时候我们在查看一个文件之后，直接ctrl+z去退出了文件，当我们再用vim命令去编辑文件的时候，这时候会发现报一个没有正确关闭这个文件的冲突错误，并且你不去解决就会一直存在。 这里get到了去解决的方法，其实也是怪自己一看到一堆英文就不想看下去，这里其实已经写得很清楚了，这里提供了两种情况： （1）有可能另一个人也在编辑这个文件，它提醒你要注意两个人同时编辑不同的地方。 （2）上一次编辑的session还在，这时候提供了解决办法：可以用recover或者vim -r 文件名去修复这个changes。 可以看到当你没正确退出时，还保持着edit session，这时候会生成两个临时文件，我这里采用的是直接删除这两个文件即可。 运行 rm -rf .fileName.* 命令之后，再次打开之后就不会有冲突。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>vi命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot应用拦截器]]></title>
    <url>%2Fblog%2F2018%2F09%2F05%2Fspringboot%E5%BA%94%E7%94%A8%E6%8B%A6%E6%88%AA%E5%99%A8%2F</url>
    <content type="text"><![CDATA[背景在工作中看到了不少项目用到了拦截器，这里去总结一下spring-boot使用拦截器。拦截器是Spring提供的HandlerInterceptor（拦截器），其功能和过滤器类似，但是提供更精细的控制能力：在request被响应之前、request被响应之后、视图渲染之前以及request全部结束之后。我们不能通过拦截器修改request的内容，但可以通过抛出异常（或者返回false）来暂停request的执行。 使用步骤配置拦截器也很简单，spring给我们提供了WebMvcConfigurerAdapter，我们在addInterceptors方法中添加注册拦截器即可。总结起来就是三步： 1.创建我们自己的拦截器类并实现HandlerInterceptor接口。 2.创建一个Java类继承WebMvcConfigurerAdapter，并重写addInterceptors方法。 3.实例化我们自定义的拦截器，然后将对象手动添加到拦截器链中。 代码示例自定义Session信息写入ThreadLocal12345678910111213141516171819202122232425262728293031323334@Slf4j@Componentpublic class SessionInterceptor implements HandlerInterceptor &#123; @Resource private RequestHelper requestHelper; @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception &#123; log.info(&quot;SessionInterceptor preHandle方法，在请求方法之前调用，Controller方法调用之前&quot;); // MOCK一个SessionUser对象，放入ThreadLocal中 SessionUser sessionUser = new SessionUser(); sessionUser.setId(2L).setName(&quot;夸克&quot;); requestHelper.setSessionUser(sessionUser); // 只有这个方法返回true 请求才能继续下去 return true; &#125; @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception &#123; log.info(&quot;SessionInterceptor postHandle方法，请求处理之后调用，但是在视图被渲染之前（Controller方法调用之后）&quot;); // 这里可以去做sessionUser的清除 防止内存泄漏 requestHelper.clearSession(); &#125; @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; log.info(&quot;SessionInterceptor afterCompletion方法，在整个请求结束之后调用，也就是在Dispatcher渲染了整个视图之后进行（主要进行资源清理工作）&quot;); if (Objects.nonNull(requestHelper.getSessionUser())) &#123; requestHelper.clearSession(); &#125; &#125;&#125; 可以看到实现了HandlerInterceptor接口之后，要实现其中的三个方法。 preHandle方法：在请求controller方法之前调用，这里就可以做一些session对象的校验及写入ThreadLocal方便方法调用等。只有这个方法返回true，请求才能继续下去。 postHandle方法：这个方法是在请求了controller方法之后但在视图渲染之前调用的。这里可以去做ThreadLocal中资源的清除。 afterCompletion方法：这个方法是在整个请求结束之后调用的，也就是在Dispatcher渲染整个视图之后进行的，主要进行资源清理工作。（这里也是去补偿了ThreadLocal中资源的清除）。 注册拦截器在WebMvcConfigurerAdapter的子类中注册这个拦截器。WebMvcConfigurerAdapter看名字就提供了很多springmvc关于web访问的配置。 123456789101112131415161718192021222324252627282930@Componentpublic class WebMvcConfig extends WebMvcConfigurerAdapter &#123; /** * sessionInterceptor不需要拦截的请求 * 比如swagger的请求、比如一些静态资源的访问、比如错误统一处理的页面 */ private static final String[] EXCLUDE_SESSION_PATH= &#123;&#125;; @Resource private SessionInterceptor sessionInterceptor; @Resource private MyInterceptor myInterceptor; /** * 对所有的拦截器组成一个拦截器链 * addPathPatterns 用于添加拦截规则 * excludePathPatterns 用户排除拦截 * * @param registry 拦截器注册对象 */ @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 加入自定义拦截器到 // registry.addInterceptor(myInterceptor); registry.addInterceptor(sessionInterceptor).excludePathPatterns(EXCLUDE_SESSION_PATH); super.addInterceptors(registry); &#125;&#125; 通过spring管理把自定义的拦截器注册成bean对象，然后通过register的addInterceptor方法注册到拦截器执行链中，这里也可以设置包括/过滤的访问地址等相关子属性。 我们写了个controller去测试这个自定义拦截器。其中helloService是把ThreadLocal中的对象给直接返回 1234567891011121314151617@RestControllerpublic class ThreadLocalController &#123; @Resource private HelloService helloService; /** * 在拦截器中用ThreadLocal * * @return SessionUser */ @GetMapping(value = &quot;/threadLocal/getSessionUser&quot;) public SessionUser getSessionUser() &#123; return helloService.getThreadSessionUser(); &#125;&#125; 1234@Override public SessionUser getThreadSessionUser() &#123; return requestHelper.getSessionUser(); &#125; 在浏览器中访问http://localhost:7001/threadLocal/getSessionUser 可以看到结果返回： 这说明我们的拦截器是正确将sessionUser设置进入ThreadLocal对象的。看控制台的日志输入： 多个自定义拦截器的执行顺序如果再定义一个自定义拦截器，那么执行的顺序是什么呢？ 123456789101112131415161718192021222324252627@Slf4j@Componentpublic class MyInterceptor implements HandlerInterceptor &#123; @Override public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception &#123; log.info(&quot;MyInterceptor preHandle方法，在请求方法之前调用，Controller方法调用之前&quot;); // 返回true才能继续执行 return true; &#125; @Override public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception &#123; log.info(&quot;MyInterceptor postHandler方法，请求处理之后调用，但是在视图被渲染之前（Controller方法调用之后）&quot;); &#125; @Override public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception &#123; log.info(&quot;MyInterceptor afterCompletion方法，在整个请求结束之后调用，也就是在Dispatcher渲染了整个视图之后进行&quot; + &quot;主要进行资源清理工作&quot;); &#125;&#125; 在webMvcConfig中注册这两个拦截器，并且在浏览器中访问同样的url。看到控制台的输入。 在webMvcConfig中，我们注册的顺序是先注册了myInterceptor，然后注册了SessionInterceptor，看到的执行顺序为： preHandle方法：myInterceptor先执行 postHandle方法：sessionInterceptor先执行 afterCompletion方法：sessionInterceptor先执行。 可见多个自定义拦截器在执行链中的执行顺序是与注册顺序相关的，preHandle方法是先注册先执行，其他两个方法是后注册的先执行。具体执行的顺序的分析可以见下图。 拦截器的缺点它依赖于web框架，在SpringMVC中就是依赖于SpringMVC框架。在实现上,基于Java的反射机制，属于面向切面编程（AOP）的一种运用，就是在service或者一个方法前，调用一个方法，或者在方法后，调用一个方法，比如动态代理就是拦截器的简单实现，在调用方法前打印出字符串（或者做其它业务逻辑的操作），也可以在调用方法后打印出字符串，甚至在抛出异常的时候做业务逻辑的操作。由于拦截器是基于web框架的调用，因此可以使用Spring的依赖注入（DI）进行一些业务操作，同时一个拦截器实例在一个controller生命周期之内可以多次调用。但是缺点是只能对controller请求进行拦截，对其他的一些比如直接访问静态资源的请求则没办法进行拦截处理。 同时，反射也有可能对性能有些影响。 HandlerInterceptor接口分析1234567891011121314151617181920212223242526272829303132public interface HandlerInterceptor &#123; /** * preHandle方法是进行处理器拦截用的，顾名思义，该方法将在Controller处理之前进行调用，SpringMVC中的Interceptor拦截器是链式的，可以同时存在 * 多个Interceptor，然后SpringMVC会根据声明的前后顺序一个接一个的执行，而且所有的Interceptor中的preHandle方法都会在 * Controller方法调用之前调用。SpringMVC的这种Interceptor链式结构也是可以进行中断的，这种中断方式是令preHandle的返 * 回值为false，当preHandle的返回值为false的时候整个请求就结束了。 */ boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; /** * 这个方法只会在当前这个Interceptor的preHandle方法返回值为true的时候才会执行。postHandle是进行处理器拦截用的，它的执行时间是在处理器进行处理之 * 后，也就是在Controller的方法调用之后执行，但是它会在DispatcherServlet进行视图的渲染之前执行，也就是说在这个方法中你可以对ModelAndView进行操 * 作。这个方法的链式结构跟正常访问的方向是相反的，也就是说先声明的Interceptor拦截器该方法反而会后调用，这跟Struts2里面的拦截器的执行过程有点像， * 只是Struts2里面的intercept方法中要手动的调用ActionInvocation的invoke方法，Struts2中调用ActionInvocation的invoke方法就是调用下一个Interceptor * 或者是调用action，然后要在Interceptor之前调用的内容都写在调用invoke之前，要在Interceptor之后调用的内容都写在调用invoke方法之后。 */ void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception; /** * 该方法也是需要当前对应的Interceptor的preHandle方法的返回值为true时才会执行。该方法将在整个请求完成之后，也就是DispatcherServlet渲染了视图执行， * 这个方法的主要作用是用于清理资源的，当然这个方法也只能在当前这个Interceptor的preHandle方法的返回值为true时才会执行。 */ void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception; &#125; HandlerInterceptorAdapterspringMvc还提供了HandlerInterceptorAdapter这个抽象类，这个抽象类中实现了AsyncHandlerInterceptor接口，而AsyncHandlerInterceptor接口又继承了HandlerInterceptor接口，我们可以首先看下AsyncHandlerInterceptor接口： 12345678910111213141516171819public interface AsyncHandlerInterceptor extends HandlerInterceptor &#123; /** * Called instead of &#123;@code postHandle&#125; and &#123;@code afterCompletion&#125;, when * the a handler is being executed concurrently. * &lt;p&gt;Implementations may use the provided request and response but should * avoid modifying them in ways that would conflict with the concurrent * execution of the handler. A typical use of this method would be to * clean up thread-local variables. * @param request the current request * @param response the current response * @param handler the handler (or &#123;@link HandlerMethod&#125;) that started async * execution, for type and/or instance examination * @throws Exception in case of errors */ void afterConcurrentHandlingStarted(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception;&#125; 可以看到在这个接口中添加了一个afterConcurrentHandlingStarted方法。 该方法是用来处理异步请求。当Controller中有异步请求方法的时候会触发该方法。异步请求先支持preHandle、然后执行afterConcurrentHandlingStarted，之后才会执行postHandle的方法。 比如现在我们配置了一个拦截器是用来拦截异步请求的： 123456789101112131415161718192021@Component@Slf4jpublic class MyAsyncHandlerInterceptor extends HandlerInterceptorAdapter &#123; /** * 该方法是用来处理异步请求。当Controller中有异步请求方法的时候会触发该方法。 * 异步请求先支持preHandle、然后执行afterConcurrentHandlingStarted。 * * @param request * @param response * @param handler * @throws Exception */ @Override public void afterConcurrentHandlingStarted(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; super.afterConcurrentHandlingStarted(request, response, handler); log.info(&quot;MyAsyncHandlerInterceptor afterConcurrentHandlingStarted 方法执行&quot;); &#125;&#125; 这个时候我们有个异步请求处理：(对于springMvc的异步请求可以看看这篇博客：springmvc的异步请求) 123456789@RestController@Slf4jpublic class HelloController &#123; @GetMapping(value = &quot;/hello&quot;) public Callable&lt;String&gt; sayHello() &#123; return () -&gt; &quot;controller&quot;; &#125;&#125; 这时在浏览器中调用我们的异步请求，可以看到控制台中输出 可见这时先调用的是afterConcurrentHandlingStarted方法，而后调用的是postHandle方法。 我们再去看适配器HandlerInterceptorAdapter的代码： 12345678910111213141516171819202122232425262728293031323334353637383940public abstract class HandlerInterceptorAdapter implements AsyncHandlerInterceptor &#123; /** * This implementation always returns &#123;@code true&#125;. */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; return true; &#125; /** * This implementation is empty. */ @Override public void postHandle( HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; &#125; /** * This implementation is empty. */ @Override public void afterCompletion( HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; &#125; /** * This implementation is empty. */ @Override public void afterConcurrentHandlingStarted( HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; &#125;&#125; 可以看到preHandle方法默认实现返回了true，比如我们只想去定义一个拦截器去在方法执行完之后去释放掉一些资源，如果去实现HandlerInterceptor则显得有点麻烦。这里只要去继承这个抽象类，实现afterCompletion方法即可。 github上述代码都能在gitHub上看到：https://github.com/zhanglijun1217/spring-boot-demo]]></content>
      <categories>
        <category>spring</category>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>拦截器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Function函数式接口的优化代码应用]]></title>
    <url>%2Fblog%2F2018%2F09%2F02%2FFunction%E5%87%BD%E6%95%B0%E5%BC%8F%E6%8E%A5%E5%8F%A3%E7%9A%84%E4%BC%98%E5%8C%96%E4%BB%A3%E7%A0%81%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[前言函数式接口之前就一直在接触过，之前在github上写过关于几个函数式接口简单应用的代码，但一直没有记录在工作中的应用，这次就用Function接口优化了一次重复代码的警告。关于函数式接口不熟悉的同学，可以先看下我在github上的代码工程：java8 优化记录优化前的代码数据底层提供了查询报表四个不同纬度的接口，而接口中的方法其实都是一样的：比如通过部门去查、通过部门下的人去查找报表数据，而在应用层如果去写这些查询接口的话，就是每种报表都要去查子部门和人员的数据（真实的情况更多），那么很自然的就在每种报表的实现方法中去根据查询部门还是人或者其他的可能的去进行数据的查询。 这里可以看到，这只是销售报表这一种情况，然后比如成单量、业绩额、成单率等等也是这样写的。这时候在IDEA中就会报一个警告：Duplicated Code，因为除了调用底层的接口不一样之外，其余的逻辑都是一样，并且这四个service没有去继承一个抽象类去做封装，并且这四个的返回值也不能去做抽象，并且在最后是要根据返回值的类型去进行bean转换，这时候不能简单的去根据泛型去抽象出private方法，这里想到了用函数式接口去做。 优化过程Function接口定义注意到查询底层报表数据是有的要传入两个参数的，也有的是要传入三个参数的，所以我们就需要多参数的Function接口，Java中为我们提供了BiFunction，也就是两个参数的Function接口，但是三个参数的函数式接口要我们自己定义。这里去定义一个传入三个参数和一个返回值的函数式接口。 1234567891011121314@FunctionalInterfacepublic interface TripleFunction&lt;T, U, K, R&gt; &#123; /** * Applies this function to the given arguments * * @param t the first function argument * @param u the second function argument * @param k the third function argument * @return the function result */ R apply(T t, U u, K k);&#125; 这里可以看到这个接口中有@FunctionalInterface注解和apply方法。 查询报表逻辑处理这里去写一个查询报表的通用逻辑处理。首先看参数，condition是传入的筛选条件，clazz是返回值泛型R的class，queryUserFun是传入两个参数的计算出T的的函数，同样queryDeptByIdFunc是传入两个参数，计算出PlainResult的函数式参数。剩下两个函数参数是传入三个参数计算出ListResult参数。 这里为什么要去传R、T两个泛型？因为这里直接调用rpc接口返回的结果是他们封装的一个DTO对象，我们要自爱应用层自己运用orika工具进行DTO的转换（防污染和降低耦合）。那为什么要单独去传入一个最后返回值得class对象？这里是因为泛型在运行时会被擦除，要使用orika去转换DTO时要进行class的参数传入。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 查询报表的方法 * * @param condition 筛选条件 * @param clazz 结果class * @param queryUserFunc 查询user函数 * @param queryUsersFunc 查询users函数 * @param queryDeptFunc 查询dept函数 * @param &lt;T&gt; 泛型1 * @param &lt;R&gt; 泛型2 * @return */ private &lt;T, R&gt; Pagination&lt;R&gt; getReport(AchievementPkDTO condition, Class&lt;R&gt; clazz, BiFunction&lt;Long, DateRangeDTO, ListResult&lt;T&gt;&gt; queryUserFunc, BiFunction&lt;Long, DateRangeDTO, PlainResult&lt;T&gt;&gt; queryDeptByIdFunc, TripleFunction&lt;Long, DateRangeDTO, OrderAndPageDTO, ListResult&lt;T&gt;&gt; queryUsersFunc, TripleFunction&lt;Long, DateRangeDTO, OrderAndPageDTO, ListResult&lt;T&gt;&gt; queryDeptFunc) &#123; DateRangeDTO dateRangeDTO = convertDateRangeDTO(condition); OrderAndPageDTO orderAndPageDTO = convertOrderAndPageDTO(condition); // 初始化 ListResult&lt;T&gt; result = new ListResult&lt;&gt;(); // 如果是有赞部门节点 if (OrganizationType.YOUZAN_SUB_DEPARTMENTS.equals(condition.getOrganizationType())) &#123; result = queryDeptFunc.apply(condition.getOrganizationId(), dateRangeDTO, orderAndPageDTO); &#125; else if (OrganizationType.YOUZAN_USER.equals(condition.getOrganizationType())) &#123; // 人的节点 result = queryUserFunc.apply(condition.getOrganizationId(), dateRangeDTO); &#125; else if (OrganizationType.YOUZAN_DEPARTMENT_USERS.equals(condition.getOrganizationType())) &#123; // 部门人的节点 result = queryUsersFunc.apply(condition.getOrganizationId(), dateRangeDTO, orderAndPageDTO); &#125; else if (OrganizationType.SINGLE_PROVIDER.equals(condition.getOrganizationType())) &#123; // 单个渠道商节点 PlainResult&lt;T&gt; apply = queryDeptByIdFunc.apply(condition.getOrganizationId(), dateRangeDTO); // 设置result信息 result.setData(Collections.singletonList(CheckWrapper.checkWrap(apply))); result.setCount(1); result.setSuccess(true); &#125; List&lt;R&gt; rList = orikaBeanUtil.convertList(CheckWrapper.checkWrap(result), clazz); return new Pagination&lt;&gt;(rList, condition.getPage(), condition.getPageSize(), result.getCount()); &#125; 实现不同种报表的查询有了上述通过Function接口的改造，使得不同的业务场景（业绩、成单量等）都可以传入一个lambda表达式去调用上边封装的获取参数的方法。比如下面的这两个方法（这样并不会报Duplicated Code警告）： 12345678910111213141516171819202122232425262728293031@Override public Pagination&lt;DealAmountStatistics&gt; getDealReport(AchievementPkDTO condition) &#123; if (Objects.equals(OrganizationType.PROVIDER, condition.getOrganizationType())) &#123; return doWithDealProvider(condition); &#125; else &#123; return getReport(condition, DealAmountStatistics.class, (organizationId, date) -&gt; dealAmountStatisticsService.getUserById(organizationId, date), (organizationId, date) -&gt; dealAmountStatisticsService.getDepartmentById(organizationId, date), (organizationId, date, range) -&gt; dealAmountStatisticsService.getUsersByDepartmentId(organizationId, date, range), (organizationId, date, range) -&gt; dealAmountStatisticsService.getDepartmentsByParentId(organizationId, date, range)); &#125; &#125; @Override public Pagination&lt;OrderNumStatistics&gt; getOrderNumberReport(AchievementPkDTO condition) &#123; if (Objects.equals(OrganizationType.PROVIDER, condition.getOrganizationType())) &#123; return doWithOrderNumProvider(condition); &#125; else &#123; return getReport(condition, OrderNumStatistics.class, (organizationId, date) -&gt; orderNumStatisticsService.getUserById(organizationId, date), (organizationId, date) -&gt; orderNumStatisticsService.getDepartmentById(organizationId, date), (organizationId, date, range) -&gt; orderNumStatisticsService.getUsersByDepartmentId(organizationId, date, range), (organizationId, date, range) -&gt; orderNumStatisticsService.getDepartmentsByParentId(organizationId, date, range)); &#125; &#125; 这里去因为PROVIDER类型节点要对不同的返回值做特殊处理，否则这两个方法还可以抽象成为一个公用方法，这里不去对抽象过多要求，主要想记录学习的是用函数式接口去优化代码减少了代码重复行数，并且lambda表达式作为参数的一个使用也没有降低可读性。之后在写代码的过程中可以多应用这种设计，去让代码变得更加简洁，使用更多的新特性。]]></content>
      <categories>
        <category>Java语法</category>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>函数式接口</tag>
        <tag>代码优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一次freemarker错误排查]]></title>
    <url>%2Fblog%2F2018%2F08%2F29%2F%E8%AE%B0%E4%B8%80%E6%AC%A1freemarker%E9%94%99%E8%AF%AF%E6%8E%92%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[前言在最近的工作中遇到了一个做一个导出功能时遇到了一个很奇怪的事情，逻辑是先做一个export方法上传到文件服务器上，然后重定向到一个doExport方法中，这个doExport方法中是去判断这个文件是否生成（之前生成Excel文件是异步线程生成的），如果没有生成，则转到一个export.ftl的freemarker页面，这个页面中去不断reload去调用这个doExport方法，直到导出了文件。但是在本地测试的时候，总发现doExport方法会无限的将请求再转发到export方法中，然后就一直产生了无限重定向，在浏览器中会有报无限重定向而给拦截掉。 现象两个controller： 模板文件： 123456789101112131415161718&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;报表导出&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div style=&quot; border:1px solid #ccc; margin:200px auto; height:70px;background:#eee;padding:40px;color:green;text-align:center;&quot;&gt;正在生成报表，请耐心等待...&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt; setTimeout(&quot;location.reload()&quot;, 1000);&lt;/script&gt;、&lt;/body&gt;&lt;/html&gt; 本地开启debug模式进行调试： 可以看到现在是线程nio-7001-exec-1去发送到重定向到下边的doExport方法 此时在doExport中是新的exec-2线程去调用这个方法判断文件是否存在 同样的是这个线程去返回modelAndView，按理说这时候应该返回模板类freemarker中的export.ftl，但是它没有去返回freemarker文件，而是将这个请求转发到了export方法： 而且是同一个线程，是容器内跳转。之后因为每次export方法都会去生成一个新的随机的文件名称，所以又会去调用doExport方法去下载新的文件名称的文件，直到有一个个文件内容比较少，异步线程在判断文件是否存在之前，文件导出，否则浏览器会直接拦截这么多次的重定向： 排查过程是不是freemarker的文件路径配置有问题既然是容器内跳转，而没有访问到真正的导出页面的文件，那是不是freemarker的文件访问路径配置有问题。看到本地环境的properties文件中关于freemarker配置如下： 12345678# freemarker相关配置spring.freemarker.enabled=truespring.freemarker.suffix=.ftlspring.freemarker.content-type=text/htmlspring.freemarker.cache=falsespring.freemarker.charset=utf-8spring.freemarker.check-template-location=truespring.freemarker.template-loader-path=classpath:/templates/ 这里去查看了模板文件确实是放在了classpath下的templates文件夹中的。所以配置和访问文件的路径没有问题。 是不是因为mapping重名，优先映射到了mapping地址这里发现模板文件的名称和export的mapping地址是一样的，则在exprot方法上加了一个1,saleDetail/exprot1这样，然后再次debug发现，这时候不会再重新容器内跳转到这个方法，但是页面直接报了404错误，这说明还是没有去加载到模板文件。 进行一个简答的路由到模板文件的测试这时候去写了一个helloController，然后在templetes下新建了一个hello.ftl模板文件，在controller中去return modelAndView，这时视图也是hello，发现也是404的错误。这说明和sendRedirect方法也没有关系，本身项目是不支持freemarker的。 最后发现在整个项目中，都没有freemarker的依赖和jar包。。。加入freemarker的依赖之后问题解决。 因为mapping地址和freemarker的模板文件地址相同，在你返回modelAndView的视图名称在mapping中找的到时，springmvc会容器内跳转，也不会报freemarker模板文件找不到的错误。因为其他工程也是这样去写的，所以就想当然的以为这个工程中也会有freemarker依赖。其实这个时候应该跟一下spring mvc的源码，就可以看到这时候跳转的是dispatcherServlet路由到的export方法，并没有走freemarker的模板文件。这里去记录一下。]]></content>
      <categories>
        <category>bug记录</category>
      </categories>
      <tags>
        <tag>freemarker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring boot actuator]]></title>
    <url>%2Fblog%2F2018%2F08%2F25%2Fspring-boot-actutor%2F</url>
    <content type="text"><![CDATA[前言spring boot的一大特性就是自带的actuator。它是spring-boot框架提供的对应系统的自省和监控的集成功能，可以对系统进行配置查看、相关功能统计等。 actuator的使用引入依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件的配置 management.port:指定访问监控防范的端口，这个端口应该与逻辑端口分离。如果不想使actuator暴露在http中，可以设置这个端口为7002。 management.address：指定地址，比如只能通过本机监控，可以设置 management.address = 127.0.0.1启动项目，可以看到actuator启动在了配置的7002端口，并且提供了可以访问其中的一些endPoints。 一些主要的EndPointsspring-boot提供了一些常用的EndPoints 其中鉴权为true的表示访问这些endPoints是需要保护的不能随意进行访问的。如果要取消，可以设置关闭鉴权（低版本的spring-boot没有提供鉴权） 1management.security.enable=false 官方文档可以看到这个监控和自省的功能是十分有用的，可以看到bean信息、dump信息、mapping信息和访问链路信息等，所以这个功能在官方文档中也说的很清楚，我们也可以通过实现HealthIndicator接口，编写自己的health接口，也可以增加自己的监控接口。具体的还可以看一下官方文档 acautor文档]]></content>
      <categories>
        <category>spring</category>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
        <tag>actuator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[validate注解做校验]]></title>
    <url>%2Fblog%2F2018%2F08%2F18%2Fvalidate%E6%B3%A8%E8%A7%A3%E5%81%9A%E6%A0%A1%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[前言之前也用过hibernate的校验注解，但是没有去做一个总结，这里参考一篇博客去做一个总结。简述JSR303/JSR-349，hibernate validation，spring validation之间的关系。JSR303是一项标准,JSR-349是其的升级版本，添加了一些新特性，他们规定一些校验规范即校验注解，如@Null，@NotNull，@Pattern，他们位于javax.validation.constraints包下，只提供规范不提供实现。而hibernate validation是对这个规范的实践（不要将hibernate和数据库orm框架联系在一起），他提供了相应的实现，并增加了一些其他校验注解，如@Email，@Length，@Range等等，他们位于org.hibernate.validator.constraints包下。而万能的spring为了给开发者提供便捷，对hibernate validation进行了二次封装，显示校验validated bean时，你可以使用spring validation或者hibernate validation，而spring validation另一个特性，便是其在springmvc模块中添加了自动校验，并将校验信息封装进了特定的类中。这无疑便捷了我们的web开发。本文主要介绍在springmvc中自动校验的机制。 引入依赖因为我们构建的是spring boot项目，所以直接引入web的starter的依赖即可。123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 如果查看其子依赖，可以发现如下的依赖：12345678&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt; 进行校验校验的实体类这里用了lombok的@Data注解，也是非常推荐大家使用的一个插件。12345678910111213141516171819202122232425@Datapublic class ValidateBO &#123; @NotBlank(message = &quot;name不能为空&quot;) private String name; @Min(value = 18, message = &quot;年龄不能小于18岁&quot;) private Integer age; @Email(message = &quot;email格式错误&quot;) private String email; /** * 自定义注解 不能包含空格字符串 */ @CannotHaveBlank private String blank; /** * 正则校验 */ @Pattern(regexp = &quot;^1(3|4|5|7|8)\\d&#123;9&#125;$&quot;, message = &quot;手机号码格式错误&quot;) private String phone;&#125; 可以看到这里用到了一些常见的注解。（自定义校验注解在下边会提到） controller进行校验在controller中进行这个字段的校验，可以看到每个需要校验的对象，都需要一个BindingResult去承接校验的结果，并且也要对要校验的类去加上@Validated注解。123456789101112@GetMapping(value = &quot;/validate&quot;) public String validate(ValidateBO validateBO, BindingResult bindingResult) &#123; if (bindingResult.hasErrors()) &#123; StringBuilder stringBuilder = new StringBuilder(); for (String s : bindingResult.getFieldErrors().stream() .map(FieldError::getDefaultMessage).collect(Collectors.toList())) &#123; stringBuilder.append(s); &#125; return stringBuilder.toString(); &#125; return &quot;success&quot;; &#125; 启动项目，在url中输入对项目进行debug。可以看到，校验在碰到第一个字段不符合要求之后，并不是直接返回错误，而是会对所有的要校验字段去校验。当然这个也是可以配置的，下边会提到fast-fail的配置。最后返回的结果：这里是打印出了这个对象中所有的错误。 常见的校验注解JSR提供的校验注解:12345678910111213@Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Min(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value) 被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=) 被注释的元素的大小必须在指定的范围内 @Digits (integer, fraction) 被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past 被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 @Pattern(regex=,flag=) 被注释的元素必须符合指定的正则表达式 Hibernate Validator提供的校验注解：12345@NotBlank(message =) 验证字符串非null，且长度必须大于0 @Email 被注释的元素必须是电子邮箱地址 @Length(min=,max=) 被注释的字符串的大小必须在指定的范围内 @NotEmpty 被注释的字符串的必须非空 @Range(min=,max=,message=) 被注释的元素必须在合适的范围内 分组校验场景如果同一个类，在不同的使用场景下有不同的校验规则，那么可以使用分组校验。未成年人是不能喝酒的，而在其他场景下我们不做特殊的限制，这个需求如何体现同一个实体，不同的校验规则呢？ 校验对象12345678910111213@Datapublic class ValidateByGroupBO &#123; /** * 只有adult组内才进行 validate 校验 */ @Min(value = 18, groups = &#123;Adult.class&#125;) private Integer age; public interface Adult&#123;&#125; public interface Minor&#123;&#125;&#125; 这就定义了只有在在adult组内才会进行最小值18的校验。 进行验证1234567891011121314151617181920212223242526272829/** * 喝酒这个去校验了年龄值，因为只有adult这个组才去校验年龄 * @param validateByGroupBO * @param bindingResult * @return */ @GetMapping(value = &quot;/drink&quot;) public String drink(@Validated(&#123;ValidateByGroupBO.Adult.class&#125;) ValidateByGroupBO validateByGroupBO, BindingResult bindingResult) &#123; if (bindingResult.hasErrors()) &#123; // 处理错误 return &quot;false&quot;; &#125; return &quot;success&quot;; &#125; /** * 生活不需要去校验adult的分组 就不去校验对应的age的最小值 * @param validateByGroupBO * @param bindingResult * @return */ @GetMapping(value = &quot;live&quot;) public String live(@Validated ValidateByGroupBO validateByGroupBO, BindingResult bindingResult) &#123; if (bindingResult.hasErrors()) &#123; // 错误处理 return &quot;false&quot;; &#125; return &quot;success&quot;; &#125; 运行之后，在url中输入http://localhost:7001/drink?age=10，第一个返回false，因为指定了使用其中的Adult分组，会开启对age的校验。输入http://localhost:7001/live?age=10则会不去校验age的大小，返回success。喝酒要校验是否成年，而生活不用，类似的场景还是很容易碰到的。 自定义注解实现一个注解这里去实现一个字符串中不能含有blank空格。主要分为两步： 先去定义这个注解，其中validatedBy指定的是真正去做校验的实体类。而其中的groups和payload可以直接用默认。 123456789101112131415161718192021222324@Target(&#123;METHOD, FIELD, ANNOTATION_TYPE, ElementType.CONSTRUCTOR, PARAMETER&#125;)@Retention(RUNTIME)@Documented// 这个注解是引入真正的去做验证的类@Constraint(validatedBy = &#123;CannotHaveBlankValidator.class&#125;)public @interface CannotHaveBlank &#123; // 默认错误信息 String message() default &quot;不能包含空格&quot;; // 分组 Class&lt;?&gt;[] groups() default &#123;&#125;; //负载 Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;; //指定多个时使用 @Target(&#123;FIELD, METHOD, PARAMETER, ANNOTATION_TYPE&#125;) @Retention(RUNTIME) @Documented @interface List &#123; CannotHaveBlank[] value(); &#125;&#125; 第二步是去实现真正去做校验的实体类 12345678910111213141516171819202122232425public class CannotHaveBlankValidator implements ConstraintValidator&lt;CannotHaveBlank, String&gt; &#123; @Override public void initialize(CannotHaveBlank cannotHaveBlank) &#123; &#125; @Override public boolean isValid(String value, ConstraintValidatorContext context) &#123; //null时不进行校验 if (value != null &amp;&amp; value.contains(&quot; &quot;)) &#123; //获取默认提示信息 String defaultConstraintMessageTemplate = context.getDefaultConstraintMessageTemplate(); System.out.println(&quot;default message :&quot; + defaultConstraintMessageTemplate); //禁用默认提示信息 context.disableDefaultConstraintViolation(); //设置提示语 context.buildConstraintViolationWithTemplate(&quot;can not contains blank&quot;) .addConstraintViolation(); return false; &#125; return true; &#125;&#125; 这里去实现类去实现了ConstraintValidator接口，这个接口中包含一个初始化事件方法和一个判断是否合法的方法：123456789package javax.validation;import java.lang.annotation.Annotation;public interface ConstraintValidator&lt;A extends Annotation, T&gt; &#123; void initialize(A var1); boolean isValid(T var1, ConstraintValidatorContext var2);&#125; 其中的A泛型参数是上一步定义的注解类，泛型T是要去校验的字段类型。ConstraintValidatorContext 这个参数上下文包含了认证中所有的信息，我们可以利用这个上下文实现获取默认错误提示信息，禁用错误提示信息，改写错误提示信息等操作。 自定义注解进行校验还是用第一个controller去验证这个自定义注解。因为要校验的对象中加入了自定义注解的blank字段。启动项目，输入http://localhost:7001/validate?blank=19 209（这里加了空格），可以看到返回值是：说明自定义注解起到了作用。 @Valid 和 @Validated的区别https://blog.csdn.net/qq_27680317/article/details/79970590这篇讲的很清晰了。 aop很显然，如果我们每个controller中的方法都去写BindingResult就显得很麻烦了，其实我们就是要对参数进行校验并且输出到log中，这就很自然的想到了aop。 注解标识定义一个注解去标识使用了hibernate validate注解123456789101112131415161718192021package com.zhanglijun.springbootdemo.domain.anno;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;import javax.validation.groups.Default;/** * 用来表示开启hibernate校验的注解 * @author 夸克 * @create 2018/8/19 22:28 */@Target(&#123;ElementType.METHOD,ElementType.TYPE,ElementType.PARAMETER&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface EnableValidate &#123; Class&lt;?&gt; [] groups() default &#123; Default.class &#125;;//校验分组信息&#125; 定义切面123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162package com.zhanglijun.springbootdemo.aspect;import com.zhanglijun.springbootdemo.domain.anno.EnableValidate;import java.lang.annotation.Annotation;import java.lang.reflect.Method;import java.util.HashSet;import java.util.Set;import javax.validation.ConstraintViolation;import javax.validation.ConstraintViolationException;import javax.validation.Validation;import javax.validation.Validator;import javax.validation.executable.ExecutableValidator;import lombok.extern.slf4j.Slf4j;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.hibernate.validator.HibernateValidator;import org.springframework.core.annotation.AnnotationUtils;import org.springframework.stereotype.Component;import org.springframework.util.ClassUtils;/** * @author 夸克 * @create 2018/8/19 18:45 */@Slf4j@Aspect@Componentpublic class ValidatorAspect &#123; /** * 获取校验的工厂的信息 */ private static final Validator validator = Validation.byProvider(HibernateValidator.class) .configure() //快速失败模式开启，当检测到有一项失败立即停止 .failFast(true) .buildValidatorFactory().getValidator(); /** * point配置 */ @Pointcut(&quot;execution(* com.zhanglijun.springbootdemo.web.controller..*.*(..))&quot;) public void pointcut() &#123; &#125; /** * @desction: 校验步骤 1.首先校验是否含有基本的Hibernate validator 注解，有异常抛出 2.校验方法参数中是否含有EgValidate注解，获取分组信息，进行Bean级别的校验，有异常抛出 * 3.查看当前的方法中（优先级高）(或者父类、父接口)是否含有EgValidate注解，没有获取当前类的中是否是否含有EgValidate注解，获取分组信息，针对每一个非基本类型Bean进行校验，有异常掏出 * @author: wangji * @date: 2018/3/13 10:16 */ @Before(&quot;pointcut()&quot;) public void before(JoinPoint point) &#123; // 获得切入目标对象 Object target = point.getThis(); // 获得切入方法参数 Object[] args = point.getArgs(); // 获得切入的方法 Method method = ((MethodSignature) point.getSignature()).getMethod(); Annotation[] classAnnotations = target.getClass().getAnnotations(); Annotation[] methodAnnotations = method.getAnnotations(); Annotation[][] parameterAnnotations = method.getParameterAnnotations(); //如果方法参数有基本的注解，就进行Hibernate validator 基本的参数校验 if (parameterAnnotations != null) &#123; validMethodParams(target, method, args); &#125; // 判断参数中是否含有EgValidate注解，进行特殊分组，Bean级别的参数校验 int i = 0; //排查掉已经在参数中校验过的参数不适用类或者方法上的校验参数在次进行校验 Set&lt;Integer&gt; idSet = new HashSet&lt;&gt;(3); for (Object arg : args) &#123; if (arg != null) &#123; if (parameterAnnotations != null) &#123; for (Annotation parameterAnnotation : parameterAnnotations[i]) &#123; if (parameterAnnotation instanceof EnableValidate) &#123; if (!ClassUtils.isPrimitiveOrWrapper(arg.getClass())) &#123; validBeanParam(arg, ((EnableValidate) parameterAnnotation).groups()); idSet.add(i); &#125; &#125; &#125; &#125; i++; &#125; &#125; // 如果没有异常继续校验当前的每一个非基本类型的参数 EnableValidate egValidate = null; //方法上是否有校验参数 if (methodAnnotations != null) &#123; egValidate = AnnotationUtils.findAnnotation(method, EnableValidate.class); &#125; // 类上是否含有 if (egValidate == null &amp;&amp; classAnnotations != null) &#123; egValidate = AnnotationUtils.findAnnotation(target.getClass(), EnableValidate.class); &#125; // 如果在类或者方法上加了验证注解 ，则对所有非基本类型的参数对象进行验证,不管参数对象有没有加注解，使用方法上的分组 if (egValidate != null &amp;&amp; args != null &amp;&amp; args.length &gt; 0) &#123; i = 0; for (Object arg : args) &#123; if (arg != null &amp;&amp; !ClassUtils.isPrimitiveOrWrapper(arg.getClass()) &amp;&amp; !idSet .contains(i)) &#123; validBeanParam(arg, egValidate.groups()); &#125; i++; &#125; &#125; &#125; /** * @param obj 参数中的Bean类型参数 * @param groups 分组信息 * @desction: 进行参数中的Bean校验 * @author: wangji * @date: 2018/3/13 10:10 */ private void validBeanParam(Object obj, Class&lt;?&gt;... groups) &#123; Set&lt;ConstraintViolation&lt;Object&gt;&gt; validResult = validator.validate(obj, groups); throwConstraintViolationException(validResult); &#125; /** * @param obj 当前的实例 * @param method 实例的方法 * @param params 参数 * @desction: 对于Hibernate 基本校验Bean放在参数中的情况的校验 【例如 User getUserInfoById(@NotNull(message = * &quot;不能为空&quot;) Integer id);】 * @author: wangji * @date: 2018/3/13 10:11 */ private void validMethodParams(Object obj, Method method, Object[] params) &#123; ExecutableValidator validatorParam = validator.forExecutables(); Set&lt;ConstraintViolation&lt;Object&gt;&gt; validResult = validatorParam .validateParameters(obj, method, params); throwConstraintViolationException(validResult); &#125; /** * @desction: 判断校验的结果是否存在异常 * @author: wangji * @date: 2018/3/13 10:09 */ private void throwConstraintViolationException(Set&lt;ConstraintViolation&lt;Object&gt;&gt; validResult) &#123; if (!validResult.isEmpty()) &#123; throw new ConstraintViolationException(validResult.iterator().next().getMessage(), validResult); &#125; &#125;&#125; 这个切面说的也很清除，对多处使用这个的地方都去做了一个校验。 github上述代码都在我的github，可以在review一下代码。validated注解相关 引用参考的博客： https://www.cnkirito.moe/spring-validation/ https://blog.csdn.net/oKuZuoZhou/article/details/81024795 https://blog.csdn.net/u012881904/article/details/79538895]]></content>
      <categories>
        <category>spring</category>
        <category>注解</category>
      </categories>
      <tags>
        <tag>validate注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——ThreadLocal总结]]></title>
    <url>%2Fblog%2F2018%2F08%2F16%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E2%80%94%E2%80%94ThreadLocal%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[概念介绍ThreadLocal是早期jdk版本中就有的一个工具，基本原理是同一个ThreadLocal所包含的对象（对ThreadLocal而言即为String类型变量），在不同的Thread中有不同的副本（实际是不同的实例）。这里有几点需要注意：​ 因为每个Thread内有自己的实例副本，且该副本只能由当前的Thread使用。这也是ThreadLocal命名的由来。 既然每个Thread都有自己的实例副本，且其他的Thread不可访问，那么就不存在多线程共享的问题（其实ThreadLocal也不是去解决多线程共享的问题）。 那么ThreadLocal解决了什么问题呢？ThreadLocal提供了线程本地的实例。它与普通变量的区别在于，每个使用该变量的线程都会初始化一个完全独立的实例副本。ThreadLocal变量通常被private static修饰。当一个线程结束时，它所使用的ThreadLocal相对的实例副本都可被回收。 ThreadLocal的适用场景：ThreadLocal适用于每个线程需要自己独立的实例且该实例需要在多个方法中使用，也即变量在线程间隔离而在方法或类间共享的场景。其实这种场景下并不只是可以用ThreadLocal去解决，只不过ThreadLocal更简洁。 ThreadLocal原理实现可能的猜想ThreadLocal维护线程与实例的映射 既然每个访问ThreadLocal变量的线程都有自己的一个“本地”实例副本，那么可能的方案是ThreadLocal维护着一个Map，键是Thread，值是它在这个Thread中的实例。线程通过该ThreadLocal的get()方法获取实例时，只需要以线程为键，从map中获取实例即可。 这个方案却又有问题： 增加线程和减少线程都需要去put、remove操作map,这个时候如果在一个ThreadLocal对该线程存入两个实例，就会有线程安全问题、 线程结束时，需要保证它所访问的所有的ThreadLocal中的对应的映射均删除，否则可能会引起内存泄漏。 第一个问题是jdk不去采取这种做法的原因。 ThreadLocal维护ThreadLocal与实例的映射 如果这个Map是每个线程去访问自己的一个Map，就不会产生多线程写的问题。map中维护着key为ThreadLocal实例，设计如下图所示。 这个方案中解决了map的线程安全问题，相当于第一种方法的倒转想法，map中key设置为ThreadLocal实例在不同线程中访问。 这种方案还是没有去解决内存泄漏问题。由于每个线程访问到ThreadLocal变量之后，都会在自己的Map内维护该ThreadLocal变量与具体实例的映射，如果不删除这些引用（映射），则这些ThreadLocal不能被回收，可能会造成内存泄漏。 JDK中的解决ThreadLocalMap 上边提到的维护的map是由ThreadLocal中的静态内部类ThreadLcoalMap去提供的，该类的实例维护着某个ThreadLocal与具体实例的映射。与HashMap不同的是，每个ThreadLocalMap的每一个Entry都是一个对键的弱引用，这一点可以从super(k)可以看出。每一个Entry对key的引用是强引用。使用ThreadLocal弱引用的原因是可以被及时回收。但是这里不能解决Entry引用内存泄漏的问题。当ThreadLocal变量被回收之后，该映射的键值变为null，该Entry无法被移除。从而也有可能造成内存泄漏。（下面会提到JDK的解决）ThreadLocalMap中的Entry代码如下：12345678static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; 读取实例12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; 读取实例时，线程首先通过getMap(t)方法获取自身的ThreadLocalMap。获取到ThreadLocalMap后，通过map.getEntity(this)方法获取该ThreadLocal在当前线程的ThreadLocalMap中的Entry。该方法中的this即当前访问的ThreadLocal方法。如果获取到的Entry不为null，从Entry中取出值即为所需访问的本线程对应的实例。如果获取到的Entry为null，则通过setInitialValue()方法设置该ThreadLocal变量在该线程中对应的具体实例的初始值。设置初始值的方法如下： 12345678910private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; 注意此方法为private方法，无法被重载。 首先，通过initialValue()方法能生成一个初始值，这个方法是一个public方法，且默认值为null。所以典型用法中常常去重载该方法去给一个默认值。然后，通过当前线程对象拿到ThreadLocalMap对象，若该对象不为null，则直接塞入map中set进去线程内实例的值。如果map为null，则去创建该ThreadLcoalMap对象。 设置实例。 设置实例的方法也是采用了上述方法中的原理，不多做解释了。12345678public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125; 防止内存泄漏 对于已经不再使用且已被回收的ThreadLocal对象，它在每个线程内对应的实例由于被线程的TheradLcoalMap的Entry强引用，无法被回收，可能会造成内存泄漏。 针对该问题，ThreadLocal的set方法中去做了处理。replaceStaleEntry方法将所有键为null 的Entry的值设置为null，从而使得该值可被回收。另外，会在rehash方法中通过 expungeStaleEntry 方法将键和值为null的Entry设置为null从而使得该 Entry可被回收。通过这种方式，ThreadLocal可防止内存泄漏。 1234567891011121314151617181920private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash();&#125; ThreadLocal的适用场景 每个线程需要自己有单独的实例 实例需要在对个方法中共享，但不希望被多线程共享。 Threadlocal一个工具类总结123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * threadLocal工具类 * @author 夸克 * @create 2018/8/15 16:47 */public class ThreadLocalUtil &#123; /** * 不同的业务区分ThreadLocal中map的key * （这里的map不是threadLocal中对应线程的threadLocalMap，而是要塞入线程中的map的值， * 这里可能在一个业务域中一个线程存在多次使用ThreadLocal，所以在threadLocal中塞入的是个map。而 * 当前线程中存放的是&lt;threadLocal对象,&lt;业务key, 真正要使用的变量&gt;&gt;） * threadLocal内存泄漏问题（（1）ThreadLocalMap中Entry的引用没有释放）在jdk8中得到了解决， * 对ThreadLocalMap中的键值threadLocal实例的引用改为弱引用 * 所以建议使用ThreadLocal */ /** * 业务前缀key值的维护 */ public enum Key &#123; /** * 测试使用 */ COMMON_TEST(&quot;COMMON_TEST&quot;); private String key; Key(String key) &#123; this.key = key; &#125; &#125; private static final ThreadLocal&lt;Map&lt;String, Object&gt;&gt; THREAD_LOCAL = new ThreadLocal&lt;&gt;(); /** * set方法 * @param key * @param value */ public static void set(String key, Object value) &#123; if(THREAD_LOCAL.get() == null) &#123; // 初始化 init(); &#125; if (StringUtils.isEmpty(key) || Objects.isNull(value)) &#123; return; &#125; THREAD_LOCAL.get().put(key, value); &#125; /** * get方法 * @param key * @return */ public static Object get(String key) &#123; if (StringUtils.isEmpty(key)) &#123; return null; &#125; return THREAD_LOCAL.get().get(key); &#125; /** * 刷新方法 */ public static void refresh() &#123; if (THREAD_LOCAL.get() == null) &#123; return; &#125; // map清除 key value THREAD_LOCAL.get().clear(); // 清除map THREAD_LOCAL.set(null); // 线程中ThreadLocalMap remove THREAD_LOCAL.remove(); &#125; private static void init() &#123; THREAD_LOCAL.set(Maps.newHashMap()); &#125;]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发工具</category>
      </categories>
      <tags>
        <tag>ThreadLcoal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring中根据Application获取BEAN的工具类]]></title>
    <url>%2Fblog%2F2018%2F08%2F12%2Fspring%E4%B8%AD%E6%A0%B9%E6%8D%AEApplication%E8%8E%B7%E5%8F%96BEAN%E7%9A%84%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[背景 在最近的开发工作中，用到了策略模式（之前也写过关于策略模式这个设计模式的学习，但是之前那个不是在spring框架中），这时候策略中的context或者factory就要去动态的根据调用的策略类型不同去拿到对应的bean对象，这里去了解了一个通过application context拿取bean的工具类，这里记录一下。 话不多说，直接上代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Component@Slf4jpublic class ApplicationContextBeanUtil implements ApplicationContextAware &#123; private static ApplicationContext applicationContext; /** * 利用aware注入application * @param applicationContext * @throws BeansException */ @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; // 注入application ApplicationContextBeanUtil.applicationContext = applicationContext; &#125; private static ApplicationContext getApplicationContext() &#123; return applicationContext; &#125; /** * 通过name获取bean * @param name * @return */ public static Object getBean(String name) &#123; return getApplicationContext().getBean(name); &#125; /** * 通过class获取bean * @param clazz * @param &lt;T&gt; * @return */ public static &lt;T&gt; T getBean(Class&lt;T&gt; clazz) &#123; return getApplicationContext().getBean(clazz); &#125; /** * 通过name和class获取bean * @param name * @param clazz * @param &lt;T&gt; * @return */ public static &lt;T&gt; T getBean(String name, Class&lt;T&gt; clazz) &#123; return getApplicationContext().getBean(name, clazz); &#125; /** * 根据clazz类型获取spring容器中的对象 * @param clazz * @param &lt;T&gt; * @return */ public static &lt;T&gt; Map&lt;String, T&gt; getBeansOfType(Class&lt;T&gt; clazz) &#123; return getApplicationContext().getBeansOfType(clazz); &#125; /** * 根据注解类从容器中获取对象 * @param clazz * @param &lt;T&gt; * @return */ public static &lt;T&gt; Map&lt;String, Object&gt; getBeansOfAnnotation(Class&lt;? extends Annotation&gt; clazz) &#123; return getApplicationContext().getBeansWithAnnotation(clazz); &#125;&#125; 这是通过实现ApplicationContextAware接口去实现注入application的，这里应该注意几点： application应该是静态的。这个Util类应该是在别的类中直接调用获取bean的静态方法，所以注入的applicationContext应该都是该类的静态变量。 要用注解或者在xml文件中将这个Util配置成bean。（这里用的spring boot，就直接配置的扫描）。 在其中提供了一些获取bean的方法。 这里去记录下，方便在之后的工作中遇到了之后去直接使用]]></content>
      <categories>
        <category>spring</category>
        <category>框架应用</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8中list转map方法总结]]></title>
    <url>%2Fblog%2F2018%2F08%2F12%2FJava8%E4%B8%ADlist%E8%BD%ACmap%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景 在最近的工作开发之中，慢慢习惯了很多Java8中的Stream的用法，很方便而且也可以并行的去执行这个流，这边去写一下昨天遇到的一个list转map的场景。 list转map在Java8中stream的应用常用方式1.利用Collectors.toMap方法进行转换123public Map&lt;Long, String&gt; getIdNameMap(List&lt;Account&gt; accounts) &#123; return accounts.stream().collect(Collectors.toMap(Account::getId, Account::getUsername));&#125; 其中第一个参数就是可以，第二个参数就是value的值。 2.收集对象实体本身 在开发过程中我们也需要有时候对自己的list中的实体按照其中的一个字段进行分组（比如 id -&gt;List），这时候要设置map的value值是实体本身。123public Map&lt;Long, Account&gt; getIdAccountMap(List&lt;Account&gt; accounts) &#123; return accounts.stream().collect(Collectors.toMap(Account::getId, account -&gt; account));&#125; account -&gt; account是一个返回本身的lambda表达式，其实还可以使用Function接口中的一个默认方法 Function.identity()，这个方法返回自身对象，更加简洁 重复key的情况。 在list转为map时，作为key的值有可能重复，这时候流的处理会抛出个异常：Java.lang.IllegalStateException:Duplicate key。这时候就要在toMap方法中指定当key冲突时key的选择。(这里是选择第二个key覆盖第一个key)123public Map&lt;String, Account&gt; getNameAccountMap(List&lt;Account&gt; accounts) &#123; return accounts.stream().collect(Collectors.toMap(Account::getUsername, Function.identity(), (key1, key2) -&gt; key2));&#125; 用groupingBy 或者 partitioningBy进行分组 根据一个字段或者属性分组也可以直接用groupingBy方法，很方便。 12345678Map&lt;Integer, List&lt;Person&gt;&gt; personGroups = Stream.generate(new PersonSupplier()). limit(100). collect(Collectors.groupingBy(Person::getAge));Iterator it = personGroups.entrySet().iterator();while (it.hasNext()) &#123; Map.Entry&lt;Integer, List&lt;Person&gt;&gt; persons = (Map.Entry) it.next(); System.out.println(&quot;Age &quot; + persons.getKey() + &quot; = &quot; + persons.getValue().size());&#125; partitioningBy可以理解为特殊的groupingBy，key值为true和false，当然此时方法中的参数为一个判断语句（用于判断的函数式接口） 12345Map&lt;Boolean, List&lt;Person&gt;&gt; children = Stream.generate(new PersonSupplier()). limit(100). collect(Collectors.partitioningBy(p -&gt; p.getAge() &lt; 18));System.out.println(&quot;Children number: &quot; + children.get(true).size());System.out.println(&quot;Adult number: &quot; + children.get(false).size()); 关于stream使用的好文推荐： 这里去看了ibm的一篇关于stream的文章，get到了不少stream还没遇到过的用法。老铁们可以去学习一下。[https://www.ibm.com/developerworks/cn/java/j-lo-java8streamapi/ ]]]></content>
      <categories>
        <category>Java语法</category>
        <category>Java8</category>
      </categories>
      <tags>
        <tag>Java8</tag>
        <tag>Stream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——CopyOnWrite容器]]></title>
    <url>%2Fblog%2F2018%2F08%2F04%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E2%80%94%E2%80%94CopyOnWrite%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[前言 Copy-On-Write简称COW，是一种用于程序设计中的优化策略。其基本思路是，从一开始大家都在共享同一个内容，当某个人想要修改这个内容的时候，才会真正把内容Copy出去形成一个新的内容然后再改，这是一种延时懒惰策略。从JDK1.5开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器,它们是CopyOnWriteArrayList和CopyOnWriteArraySet。CopyOnWrite容器非常有用，可以在非常多的并发场景中使用到。 什么是CopyOnWrite容器从字面意思上看是写时复制的容器。通俗理解就是我们往一个容器中添加元素时，不直接往容器中添加，而是先将容器copy，复制出一个新的容器，然后新的容器里添加元素，添加完新的元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，但不需要加锁，因为当前在读的容器中不会添加新的元素，运用一种读写分离容器的思想。 copyOnWriteArrayList实现原理来看几个方法（1）add (E e)这里是要加锁的，因为在add的时候是要Arrays.copyOf出一个容器副本的，如果多线程访问会造成copy多个容器副本出来。可以看到是在copy完成添加元素之后去将引用指向这个新的数组。1234567891011121314151617181920/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125; （2）get(int index)读的时候并没有加锁，如果读的时候有线程同时去添加元素，还是会读到之前的旧的容器，所以并不用加速，但要求对读的数据一致性没那么高。123456789/** * &#123;@inheritDoc&#125; * * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E get(int index) &#123; return get(getArray(), index);&#125; 应用场景 copyOnWrite并发容器用于读多写少的并发场景。比如白名单、黑名单、商品类目等。比如有一个搜索网站，用户在这个网站的搜索框中，输入关键字搜索，但是某些关键字不允许被搜索。这些不能被搜索的关键字会被放在一个黑名单中，黑名单每晚会更新一次。当用户搜索时，会检查关键字在不在黑名单中，如果在，则提示不能搜索。 另外迭代操作远远大于修改操作时，才应该使用“写入时复制”容器。这个准则很好的描述了许多事件通知系统：在分发通知时需要迭代已注册监听器链表，并调用每个监听器，在大多数情况下，注册和注销事件监听器的操作远少于接受事件通知的操作。 copyOnWriteArrayList用于替代同步list，在某些情况下它提供了更好的并发性能，并且能在迭代期间不需要对容器进行加锁或者复制。 实现一个copyOnWriteMap容器 简单根据“写入时复制”的思想实现一个map容器，并做简单测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package collection;import java.util.AbstractMap;import java.util.HashMap;import java.util.Map;import java.util.Set;/** * 简单根据CopyOnWrite容器的思想去实现一个map 只实现了get put putAll方法 且一些临界异常条件没有去处理 * * @author 夸克 * @create 2018/7/8 15:58 */public class CopyOnWriteMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements Cloneable &#123; private volatile Map&lt;K, V&gt; internalMap; public CopyOnWriteMap() &#123; internalMap = new HashMap&lt;&gt;(); &#125; /** * 此方法未实现 */ @Override public Set&lt;Entry&lt;K, V&gt;&gt; entrySet() &#123; return null; &#125; @Override public V get(Object key) &#123; // 读时不加锁 return internalMap.get(key); &#125; @Override public V put(K key, V value) &#123; // 写时复制加锁 synchronized (this) &#123; Map&lt;K, V&gt; newMap = new HashMap&lt;K, V&gt;(internalMap); V val = newMap.put(key, value); internalMap = newMap; return val; &#125; &#125; @Override public void putAll(Map&lt;? extends K, ? extends V&gt; data) &#123; synchronized (this) &#123; Map&lt;K, V&gt; newMap = new HashMap&lt;&gt;(internalMap); newMap.putAll(data); internalMap = newMap; &#125; &#125; public static void main(String[] args) &#123; CopyOnWriteMap&lt;Integer, Integer&gt; copyOnWriteMap = new CopyOnWriteMap(); // 初始化数据 Map&lt;Integer, Integer&gt; map = new HashMap(); for (int i = 0; i &lt; 10; i++) &#123; map.put(i, i); &#125; // 读五次线程 for (int i = 0; i &lt; 5; i++) &#123; Thread read = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 方便写线程写入数据 如果不加sleep 是读不到数据的，因为是在新复制的容器中写。 // 测试copyOnWrite思想 Thread.sleep(500); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(copyOnWriteMap.get(5)); &#125; &#125;); read.start(); &#125; // 写线程 Thread write = new Thread(new Runnable() &#123; @Override public void run() &#123; map.forEach((k, v) -&gt; &#123; copyOnWriteMap.put(k, v); &#125;); &#125; &#125;); write.start(); &#125;&#125; 代码地址https://github.com/zhanglijun1217/juc 缺点 显然，每次修改容器的时候都会复制底层数组，这回造成一定的内存开销，特别是当容器的规模很大的时候，可能有将内存撑爆的可能性存在。这时候可能要考虑别的容器。另外上边也提到了这种复制一份新的容器延迟的做法会有数据一致性的问题，如果你对写入的数据读出来实时性很高，那么久不要去选择copyOnWrite容器。 其他文章 copyOnWriteArrayList于同步集合工具容器性能比较：性能比较 简单使用：简单使用 引用说明 1.https://www.cnblogs.com/dolphin0520/p/3938914.html2.《java并发编程实战》]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发工具</category>
      </categories>
      <tags>
        <tag>copyOnWrite</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程——线程基础（一）]]></title>
    <url>%2Fblog%2F2018%2F08%2F02%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E2%80%94%E2%80%94%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[线程是Java学习过程中比较难理解的一part，所以要好好打下基础，之后也会对juc包等其他并发编知识去做一个具体的原理性的学习。 一些概念一、并发与并行 并发：同一个时间间隔内做很多件事情；并行：同一个时刻同时做多件事情。 其实对于这句话可以这样理解：并发是两个任务可以在重叠的时间段内启动、运行和完成。并行是任务在同一时间运行，例如，在多核处理器上，并发是独立执行过程的组合，而并行是同时执行的。并发更像是操作系统用线程模型抽象之后站在线程的角度上看到的任务的”同时“执行。 二、临界区 表示一种公有区域或者公有数据，但是每一次只有一个线程使用，其他线程想使用必须等待。进程在访问资源的时候必须经过这些步骤：【进程】–&gt;【进入区（申请资源）】–&gt;【临界区】–&gt;【退出区（释放资源）】 在进入区中资源如果被占用访问，其他进入阻塞队列等待。 阻塞：一个线程占用了临界区资源，其他需要这个资源的线程在临界区中等待，导致这些线程挂起。 非阻塞：其他线程可以同时进入临界区，但保证公有数据不被改坏。 三、锁 死锁（DeadLock）:线程之间互相等待释放资源 饥饿锁（strarvation）:某一个线程或多个线程无法获取资源，导致一直无法执行 活锁（liveLock）:可以想象为电梯遇到人，同时都往一个方向去给对方让出资源，是个动态的问题 四、并发级别 阻塞状态级别 非阻塞状态级别（这里面还分为三种）(1) 无障碍。一种最弱的非阻塞调度，自由进出临界区。无竞争时要求有限步骤内完成操作；无竞争时直接进行回滚数据。(2) 无锁。保证只有一个线程可以胜出访问临界资源。比如乐观锁（CAS）(3) 无等待的，并发中最高级别，是无锁的，要求所有的线程都在有限步内完成，并且无饥饿的。比如：读线程和写线程，所有线程都是无等待的。比如CopyOnWriteArrayList写时写副本数据，读时共享读，线程之间是无等待的。 五、并行的两个定律 加速比：优化前系统耗时/优化后系统耗时。 说明增加CPU个数不一定增加加速比。 古斯塔夫森定律：只要有足够的并行化，那么加速比和CPU个数成正比 六、线程相关知识线程和进程 进程是分配资源的基本单位，线程是CPU调度的基本单位。进程之间的资源是互相独立不可共享的，但是线程之间是可以共享父线程或者进程的资源的。进程之间切换要比线程之间切换消耗资源代价多很多。 线程的状态 新建状态（NEW）：新创建了一个线程对象 可运行状态（RUNNABLE）：线程对象创建之后，其他线程（比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取cpu的使用权。 运行（running）：可运行状态的线程获得了cpu时间片，执行程序代码。 阻塞（block）：阻塞状态是指线程因为某种原因放弃了cpu使用权，也即让出了cpu timeslice，暂时停止运行。直到线程进入可运行（runnable）状态，才有机会再次获得cpu timeslice转到运行（running）状态。其中阻塞的状态分三种：（1）等待阻塞：运行（running）的线程执行o.wait()方法，JVM会把线程放入到等待队列（waitting queue）中。（2）同步阻塞：运行（running）的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池（lock pool）中。（3）其他阻塞：运行（running）的线程执行Thread.Sleep(long ms)或者join方法，或者发出了IO请求，JVM会把该线程置为阻塞状态。当sleep状态超时、join()等待线程终止或者超时、或者IO处理完毕之后，线程重新转入可运行（runnable）状态。 死亡（dead）：线程run()、main()方法执行结束，或者因一场退出了run()方法，则该线程结束生命周期，死亡的线程不可再次复生。 线程状态扭转的图： 可以看到：（1）当调用new Threa()方法之后，线程就会处于新建状态。（2）调用start()方法之后，线程会进入runnable状态，当操作系统选中之后给当前线程分配了时间片线程进入running状态。（3）当run()方法、main()方法结束或者发生异常，线程会进入dead状态。（4）当因为synchronize或者lock同步方法，线程没有获取到锁标识就会进入到锁池（lock pool）中等待；同样当调用o.wait()方法之后线程会进入等待队列中，这时会释放锁或者monitor，直到被其他线程的notify()方法或者notifyAll()方法唤醒。（5）当调用Thread.yield()方法之后，会使线程从running状态转换到runnable状态再去和其他线程一起去竞争时间片资源，所以会出现调用yield()方法之后又重新竞争到了资源变成running状态。（6）当调用了sleep()/join()方法之后，线程并不会释放锁或者monitor，而当sleep时间到了或者调用join()方法的线程执行完毕之后会继续进入running状态。 注意其中线程的一些方法经常在面试中问到的问题： 1.sleep方法和wait方法的区别 （1）sleep是Thread类的方法，wait是Object类的方法（2）调用sleep方法不会释放锁，wait方法会使线程释放当前的锁。（3）wait方法必须别的线程执行notify/notifyAll()方法才能重新获取CPU执行时间。 2.join()方法的本质 join方法是把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行的线程。比如main线程中调用t.join()时候，main线程会获得线程对象t的锁，调用对象的wait方法(等待时间)，直到该对象唤醒main线程，所以意味着main线程调用t.join()时，必须能够拿到线程t对象的锁。注意join()方法也是要捕捉异常的，关于join()方法的比较好的一篇文章：http://uule.iteye.com/blog/1101994 3.yield()方法 yield()方法与sleep()方法类似，只是不能由用户指定暂停多长时间，并且yield()方法只能让同优先级的线程有执行的机会，yield()方法不会释放对象上的锁。 4.wait()和notify()、notifyAll() 这三个方法用于协调多个线程对共享数据的存取，所以必须在synchronized中执行使用。synchronized关键字用于保护共享数据，阻止其他线程对共享数据的存取，可以用这三个方法去灵活控制。wait()方法使当前的线程暂停执行并释放对象锁标识，让其他线程可以进入synchronized数据块，当前线程被放入对象等待池中，只有锁标志等待池中线程能获取锁标志；如果锁标志等待池中没有线程，则notify()不起作用。notifyAll()方法则从对象等待池中移走所有等待那个对象的线程并放入锁标志等待池中。 wait()、notify()方法是Object类的方法，因为他们必须要标识它们操作线程的锁，而锁对象可能是任何对象，所有这里这两个方法是Object类的方法。 线程创建几种方式继承Thread类 可以继承Thread实现其中的run()方法12345678910111213141516171819202122232425262728293031323334353637public class NewThread2 extends Thread &#123; public NewThread2(String name) &#123; super(name); &#125; @Override public void run() &#123; while (!interrupted()) &#123;// 这里的循环是当不被中断的时候 才执行 System.out.println(getName() + &quot; 线程运行&quot;); &#125; &#125; public static void main(String[] args) &#123; NewThread2 t1 = new NewThread2(&quot;first thread&quot;); NewThread2 t2 = new NewThread2(&quot;second thread&quot;); t1.setDaemon(true);// 后台（守护）线程会随着主线程结束也结束 t2.setDaemon(true); t1.start(); t2.start(); // 中断不用stop()方法 已经过时 t1.interrupt(); t2.interrupt();// try &#123; // 让主线程sleep两秒 Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 实现Runnable接口其实看到Thread类也是实现了Runnable接口的。1234567891011121314151617181920212223242526272829303132333435363738394041424344package newthread;/** * created by zlj on 2018/5/31 * Runnable接口 创建线程 */public class NewThread implements Runnable &#123; @Override public synchronized void run() &#123; while (true) &#123; try &#123; // Thread.sleep(1000);// 调用超时等待使得线程进入阻塞状态 到达时间后线程到达就绪状态 wait();// 线程通讯必须在同步代码块中 否则会报错IllegalMonitorStateException &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;自定义线程执行...&quot;); &#125; &#125; public static void main(String[] args) &#123; NewThread newThread = new NewThread(); // 线程初始化 Thread thread = new Thread(newThread);// 构造函数是runnable接口参数 thread.start();// 调用start方法使得线程进入就绪状态 while (true) &#123; synchronized (newThread) &#123;// 这里同步代码块中监视的是同步的对象 对应上边wait方法获取的是this对象 System.out.println(&quot;主线程执行...&quot;);// try &#123;// Thread.sleep(100);// &#125; catch (InterruptedException e) &#123;// e.printStackTrace();// &#125; newThread.notifyAll();// notify方法必须在同步监视器中 否则会报错 &#125; &#125; &#125;&#125; 实现Callable接口（线程可以有返回值和抛出异常）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package newthread;import java.util.concurrent.Callable;import java.util.concurrent.FutureTask;/** * created by zlj on 2018/5/31 * 带有返回值 和 抛出异常 的线程创建 */public class NewThread4 implements Callable&lt;Integer&gt; &#123; // 实现call方法 // callable接口 和 runnable接口 比较：（1）callable接口可以传入泛型有返回值 （2）可以抛出异常 @Override public Integer call() throws Exception &#123; System.out.println(&quot;正在紧张的计算&quot;); // sleep 模拟计算过程 Thread.sleep(3000); return 1; &#125; public static void main(String[] args) &#123; NewThread4 t = new NewThread4(); /* * 对线程任务的封装 */ // 主线程可以先去做点别的 System.out.println(&quot;先去做点别的&quot;); // 首先用FutureTask&lt;T&gt; 对 call 封装成任务 FutureTask&lt;Integer&gt; task = new FutureTask&lt;Integer&gt;(t); // 再根据构造函数去封装成Thread对象 传入FutureTask类型的task Thread thread = new Thread(task); thread.start(); // 拿到线程的结果 try &#123; System.out.println(&quot;计算的结果是&quot; + task.get()); // 这里注意TaskFuture的get()方法是等上边线程结束之后去进行计算结果的。也是一个闭锁操作 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 线程池实现1234567891011121314151617181920212223242526272829303132package newthread;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * created by zlj on 2018/5/31 * 线程池创建线程 */public class NewThread6 &#123; public static void main(String[] args) &#123; // 创建十个定长的线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); // 提交一百个 线程执行的任务 for (int i=0; i&lt;100; i++) &#123; threadPool.execute(new Runnable() &#123; @Override public void run() &#123; System.out.println(Thread.currentThread().getName()); &#125; &#125;); &#125; // 线程池的停止销毁 否则程序一直会运行 threadPool.shutdown(); // 注意shutdownNow方法和shutdown方法的区别 threadPool.shutdownNow(); &#125;&#125; Executors中提供了一系列工厂方法用于创建线程池，返回的线程池都实现了ExecutorService接口。（1）public static ExecutorService newFixedThreadpPool(int nThreads) // 创建国定数目线程的线程池。（2）public static ExecutorService newCachedThreadPool() // 创建一个可缓存的线程池，调用execute将重用以前构造的线程（如果线程可用）。如果线程没有可用的，则创建一个新线程并添加到池中。终止并从缓存中移除那些已有60秒钟未被使用的线程。（3）public static ExecutorService newSingleThreadExecutor（） // 创建一个单线程化的Executor（4）public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) // 创建一个支持定时及周期性的任务执行线程池，多数情况可以用来替代Timer类。但是其实在阿里的代码检查工具中，是不建议去使用这个工具类去调用线程池的，建议去手动的写自定义的线程池。 线程中的其他常见方法 方法 说明 setPriority(int priority) 设置线程的优先级 setDaemon(boolean on) 设置是否为后台线程 interrupt() 中断线程 isAlive() 测试线程是否处于活动状态 守护线程和用户线程的区别：守护线程依赖于创建它的线程，而用户线程则不依赖。举个简单的例子：如果在main线程中创建了一个守护线程，当main方法允许完毕之后，守护线程也会随着消亡，而用户线程则不依赖会一直运行到完毕为止，在JVM中像垃圾回收线程就是守护线程。但是要注意，设置守护线程要在thread.start()方法之前，否则会报IllegalThreadStateException异常。不应该所有的线程都可以分配给Deamon线程来进行服务，比如读写操作或计算逻辑，因为在Deamon Thread没来得及进行操作时，虚拟机可能已经退出了。 停止线程的方法 使用退出标志，使线程正常退出。 使用stop方法终止线程（已过时不推荐） while判断 + interrupt方法终止线程。其中interrupt方法不会终止正在运行的线程，所以要加入一个判断去完成线程的优雅退出。 一些面试题 线程和进程有什么区别？答：一个进程是一个独立(self contained)的运行环境，它可以被看作一个程序或者一个应用。而线程是在进程中执行的一个任务。线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据。 Thread类的sleep()方法和对象的wait()方法都可以让线程暂停执行，它们有什么区别?答：sleep()方法（休眠）是线程类（Thread）的静态方法，调用此方法会让当前线程暂停执行指定的时间，将执行机会（CPU）让给其他线程，但是对象的锁依然保持，因此休眠时间结束后会自动恢复（线程回到就绪状态）。wait()是Object类的方法，调用对象的wait()方法导致当前线程放弃对象的锁（线程暂停执行），进入对象的等待池（wait pool），只有调用对象的notify()方法（或notifyAll()方法）时才能唤醒等待池中的线程进入等锁池（lock pool），如果线程重新获得对象的锁就可以进入就绪状态。 线程的sleep()方法和yield()方法有什么区别？答：① sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会；② 线程执行sleep()方法后转入阻塞（blocked）状态，而执行yield()方法后转入就绪（ready）状态；③ sleep()方法声明抛出InterruptedException，而yield()方法没有声明任何异常；④ sleep()方法比yield()方法（跟操作系统CPU调度相关）具有更好的可移植性。 请说出与线程同步以及线程调度相关的方法。wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁；sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理InterruptedException异常；notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由JVM确定唤醒哪个线程，而且与优先级无关；notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； 先写到这关于线程的知识总结还有很多，后边关于并发编程还要更深入的理解，这里先上一张知识总结图吧。]]></content>
      <categories>
        <category>并发编程</category>
        <category>并发基础</category>
      </categories>
      <tags>
        <tag>并发编程</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式——策略模式]]></title>
    <url>%2Fblog%2F2018%2F08%2F02%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E2%80%94%E2%80%94%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[策略模式 定义一系列的算法，把每一个算法封装起来，并且使它们可以相互替换。这个模式中使得各个算法可以独立于使用它的客户而变化。 策略模式的构成：1.抽象策略角色：策略类，通常由一个接口或者抽象类实现。2.具体策略角色：包装了相关的算法和行为，实现策略接口或继承抽象类。3.环境角色：context，运行时持有一个策略类的引用，最终给客户端调用。对应的uml图片为： 策略模式让算法独立于使用它的客户而独立变化。策略模式重点是封装不同的算法和行为，不同的场景下可以相互替换。策略模式是开闭原则的体现，开闭原则讲的是一个软件实体应该对拓展开放对修改关闭。因为策略模式在加入新的策略时，不会影响其他类的修改，增加了拓展性，也就是对拓展是开放的；对于调用场景来说，只依赖于抽象，而不依赖于具体实现，所以对修改是关闭的。 策略模式的优点和缺点优点：（1）调用策略中的方法在context中，没有和各个策略的实现耦合在一起，各个实现策略的不同子类可以去拓展、修改和切换。（2）避免写很多if else代码，提高了可观性。同时可以结合抽象类（策略类）去使用，Java支持很好。缺点：（1）客户端调用时必须知道所有的策略类，并且感知到要调用哪一种策略实现。（2）一旦抽象，必然会对一些特殊场景难以处理。并且这里去加入了很多的策略实现类，也有Context类的加入，增加了开销。 代码示例比如现在支付方式有四种，这其中每一种方式都对应着不同的最后费用。这样可以应用策略模式。 Strategy.java12345678910111213package design_pattern.strategy_pattern.intf;import design_pattern.strategy_pattern.constant.ReChargeTypeEnum;/** * 抽象策略角色Strategy接口 * @author 夸克 * @create 2018/7/23 17:56 */public interface Strategy &#123; // 定义计算recharge的方法 Double calRecharge(Double charge);&#125; 2.StrategyContext.java123456789101112131415161718192021222324package design_pattern.strategy_pattern.context;import design_pattern.strategy_pattern.factory.StrategyFactory;import design_pattern.strategy_pattern.constant.ReChargeTypeEnum;import design_pattern.strategy_pattern.intf.Strategy;/** * 策略模式中的环境角色 context * @author 夸克 * @create 2018/7/24 14:57 */public class Context &#123; private Strategy strategy; public Double calRecharge(Double charge, Integer type) &#123; // 利用一个工厂去生成对应的策略 strategy = StrategyFactory.getInstance().creator(ReChargeTypeEnum.from(type)); if (strategy == null) &#123; throw new RuntimeException(&quot;策略生成错误&quot;); &#125; return strategy.calRecharge(charge); &#125;&#125; 3.策略工厂，返回对应的策略12345678910111213141516171819202122232425262728293031323334353637383940package design_pattern.strategy_pattern.factory;import design_pattern.strategy_pattern.constant.ReChargeTypeEnum;import design_pattern.strategy_pattern.intf.Strategy;import design_pattern.strategy_pattern.strategy.BusiAcctStrategy;import design_pattern.strategy_pattern.strategy.CardStrategy;import design_pattern.strategy_pattern.strategy.EBankStrategy;import design_pattern.strategy_pattern.strategy.MobileStrategy;import java.util.HashMap;import java.util.Map;/** * 策略工厂 负责Strategy实例的创建 根据传入的type实现创建不同的策略 * @author 夸克 * @create 2018/7/24 15:01 */public class StrategyFactory &#123; private static StrategyFactory factory = new StrategyFactory(); private static Map&lt;ReChargeTypeEnum, Strategy&gt; map = new HashMap&lt;&gt;(); static &#123; map.put(ReChargeTypeEnum.E_BANK, new EBankStrategy()); map.put(ReChargeTypeEnum.BUSI_ACCOUNTS, new BusiAcctStrategy()); map.put(ReChargeTypeEnum.MOBILE, new MobileStrategy()); map.put(ReChargeTypeEnum.CARD_RECHARGE, new CardStrategy()); &#125; /** * getInstance方法进行初始化 * @return */ public static StrategyFactory getInstance() &#123; return factory; &#125; public Strategy creator(ReChargeTypeEnum type) &#123; return map.get(type); &#125;&#125; 四种策略的实现：12345678910111213141516package design_pattern.strategy_pattern.strategy;import design_pattern.strategy_pattern.constant.ReChargeTypeEnum;import design_pattern.strategy_pattern.intf.Strategy;/** * @author 夸克 * @create 2018/7/24 14:53 */public class BusiAcctStrategy implements Strategy &#123; @Override public Double calRecharge(Double charge) &#123; return charge * 0.9; &#125;&#125; 12345678910111213141516package design_pattern.strategy_pattern.strategy;import design_pattern.strategy_pattern.constant.ReChargeTypeEnum;import design_pattern.strategy_pattern.intf.Strategy;/** * @author 夸克 * @create 2018/7/24 14:57 */public class CardStrategy implements Strategy &#123; @Override public Double calRecharge(Double charge) &#123; return charge + charge * 0.01; &#125;&#125; 12345678910111213141516package design_pattern.strategy_pattern.strategy;import design_pattern.strategy_pattern.constant.ReChargeTypeEnum;import design_pattern.strategy_pattern.intf.Strategy;/** * @author 夸克 * @create 2018/7/24 14:52 */public class EBankStrategy implements Strategy &#123; @Override public Double calRecharge(Double charge) &#123; return charge * 0.85; &#125;&#125; 12345678910111213141516package design_pattern.strategy_pattern.strategy;import design_pattern.strategy_pattern.constant.ReChargeTypeEnum;import design_pattern.strategy_pattern.intf.Strategy;/** * @author 夸克 * @create 2018/7/24 14:54 */public class MobileStrategy implements Strategy &#123; @Override public Double calRecharge(Double charge) &#123; return charge; &#125;&#125; Main.java12345678910111213141516171819202122232425package design_pattern.strategy_pattern.main;import design_pattern.strategy_pattern.constant.ReChargeTypeEnum;import design_pattern.strategy_pattern.context.Context;/** * @author 夸克 * @create 2018/7/26 23:30 */public class StrategyMain &#123; public static void main(String[] args) &#123; Context context = new Context(); /** * 计算四种计算方式 */ Double aDouble = context.calRecharge(100D, ReChargeTypeEnum.E_BANK.getValue()); Double bDouble = context.calRecharge(100D, ReChargeTypeEnum.BUSI_ACCOUNTS.getValue()); Double cDouble = context.calRecharge(100D, ReChargeTypeEnum.MOBILE.getValue()); Double dDouble = context.calRecharge(100D, ReChargeTypeEnum.CARD_RECHARGE.getValue()); System.out.println(aDouble + &quot;\t&quot; + bDouble + &quot;\t&quot; + cDouble + &quot;\t&quot; + dDouble); &#125;&#125; github代码已经上传至我的github：https://github.com/zhanglijun1217/java8/tree/master/src/design_pattern/strategy_pattern 引用https://www.jianshu.com/p/71feb016ac05]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>策略模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orika转换bean的字段丢失]]></title>
    <url>%2Fblog%2F2018%2F08%2F02%2Forika%E8%BD%AC%E6%8D%A2bean%E7%9A%84%E5%AD%97%E6%AE%B5%E4%B8%A2%E5%A4%B1%2F</url>
    <content type="text"><![CDATA[背景 用orika对象转换工具去转换list的时候，发现只去完整转了list的第一条数据，但是后边的数据都没有将字段全部映射上去。 描述： 1.debug时发现的，源数据list是数据都存在的 转完之后的list数据，发现userName、realName等字段是丢失的。 解决经过排查发现是因为在转换注册的字段中，有个type字段没有对应的注册上去。这里就造成了orika这个转换工具丢失了list中记录字段的数据转换。 这里去记录一下这次碰到的小bug，其实也是粗心导致的。]]></content>
      <categories>
        <category>bug记录</category>
      </categories>
      <tags>
        <tag>orika</tag>
        <tag>bug</tag>
      </tags>
  </entry>
</search>
